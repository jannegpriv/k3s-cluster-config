apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    prometheus-operator-validated: "true"
  creationTimestamp: "2025-04-03T20:42:27Z"
  generation: 15
  labels:
    app: kube-prometheus-stack
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/part-of: kube-prometheus-stack
    kustomize.toolkit.fluxcd.io/name: apps
    kustomize.toolkit.fluxcd.io/namespace: flux-system
    prometheus: kube-prometheus-stack-prometheus
    release: kube-prometheus-stack
  name: k3s-alert-rules
  namespace: monitoring
  resourceVersion: "11117696"
  uid: 94bf1e79-6a86-4a30-b9c9-2aeb9b852722
spec:
  groups:
  - name: kubernetes-system-controller-manager
    rules:
    - alert: KubeControllerManagerDown
      annotations:
        description: This alert is disabled for k3s as the controller manager is embedded
        summary: Expected state - KubeControllerManager is embedded in k3s
      expr: absent(up{job="kube-controller-manager"} == 1)
      for: 15m
      labels:
        severity: none
        silent: "true"
  - name: kubernetes-system-scheduler
    rules:
    - alert: KubeSchedulerDown
      annotations:
        description: This alert is disabled for k3s as the scheduler is embedded
        summary: Expected state - KubeScheduler is embedded in k3s
      expr: absent(up{job="kube-scheduler"} == 1)
      for: 15m
      labels:
        severity: none
        silent: "true"
  - name: kubernetes-system-kube-proxy
    rules:
    - alert: KubeProxyDown
      annotations:
        description: This alert is disabled for k3s as kube-proxy is managed differently
        summary: Expected state - KubeProxy is embedded in k3s
      expr: absent(up{job="kube-proxy"} == 1)
      for: 15m
      labels:
        severity: none
        silent: "true"
  - name: K3sAlerts
    rules:
    - alert: K3sServiceDown
      annotations:
        description: K3s service has been down for more than 5 minutes.
        summary: K3s service is down
      expr: up{job="kubelet"} == 0 or absent(up{job="kubelet"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: ApiServerHighLatency
      annotations:
        description: 99th percentile of request latency is above 5 seconds for {{
          $labels.verb }} requests. This may indicate k3s CPU pressure.
        summary: API server high latency
      expr: histogram_quantile(0.99, sum by(le, verb) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|WATCH"}[5m])))
        > 5
      for: 30m
      labels:
        severity: warning
    - alert: ApiServerCriticalLatency
      annotations:
        description: 99th percentile of request latency is above 15 seconds for {{
          $labels.verb }} requests. Consider restarting k3s.service.
        summary: API server critical latency
      expr: histogram_quantile(0.99, sum by(le, verb) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|WATCH"}[5m])))
        > 15
      for: 15m
      labels:
        severity: critical
    - alert: K3sHighCPU
      annotations:
        description: K3s is using more than 80% CPU for over 15 minutes.
        summary: K3s high CPU usage
      expr: rate(process_cpu_seconds_total{job="k3s"}[5m]) * 100 > 80
      for: 15m
      labels:
        severity: warning
    - alert: K3sHighMemory
      annotations:
        description: K3s is using more than 2GB of memory for over 15 minutes.
        summary: K3s high memory usage
      expr: process_resident_memory_bytes{job="k3s"} / (1024 * 1024 * 1024) > 2
      for: 15m
      labels:
        severity: warning
    - alert: K3sEtcdHighLatency
      annotations:
        description: 99th percentile of etcd request latency is above 1 second. This
          may cause API server performance issues.
        summary: K3s etcd high latency
      expr: histogram_quantile(0.99, sum(rate(etcd_request_duration_seconds_bucket[5m]))
        by (le)) > 1
      for: 15m
      labels:
        severity: warning
