apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-26T19:24:36Z"
    generateName: bitwarden-
    labels:
      app: bitwarden
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: bitwarden-7fd6fd9558
      statefulset.kubernetes.io/pod-name: bitwarden-0
    name: bitwarden-0
    namespace: bitwarden
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: bitwarden
      uid: f6300354-5f9b-4cae-8cfb-abc1f678f8db
    resourceVersion: "9322488"
    uid: f50c6bda-ce54-40e7-9f5e-c93d0553dec9
  spec:
    containers:
    - env:
      - name: ROCKET_PORT
        value: "80"
      - name: ADMIN_TOKEN
        valueFrom:
          secretKeyRef:
            key: token
            name: bitwarden-admin-token
      - name: SMTP_USERNAME
        valueFrom:
          secretKeyRef:
            key: emailUser
            name: bitwarden-smtp
      - name: SMTP_PASSWORD
        valueFrom:
          secretKeyRef:
            key: emailPassword
            name: bitwarden-smtp
      envFrom:
      - configMapRef:
          name: bitwarden
      image: vaultwarden/server:1.33.0
      imagePullPolicy: IfNotPresent
      name: bitwarden
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 3012
        name: websocket
        protocol: TCP
      resources:
        limits:
          cpu: 300m
          memory: 1Gi
        requests:
          cpu: 50m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: bitwarden-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ds75q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: bitwarden-0
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: bitwarden
    serviceAccountName: bitwarden
    subdomain: bitwarden
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: bitwarden-data
      persistentVolumeClaim:
        claimName: bitwarden-data-claim
    - name: kube-api-access-ds75q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:25:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:25:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:25:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a453612aa55c6c804fd608ccb22b08971dbd133c52e9c6cb8e1b4c99a2dea333
      image: docker.io/vaultwarden/server:1.33.0
      imageID: docker.io/vaultwarden/server@sha256:e81ca01351ecf40083366202b163e7a31abca04d96e2194e9e1f78a57052f65c
      lastState: {}
      name: bitwarden
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:25:38Z"
      volumeMounts:
      - mountPath: /data
        name: bitwarden-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ds75q
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 10.42.3.181
    podIPs:
    - ip: 10.42.3.181
    qosClass: Burstable
    startTime: "2025-03-26T19:24:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-03T01:00:00Z"
    generateName: bitwarden-backup-29060700-
    labels:
      batch.kubernetes.io/controller-uid: 44873d7a-6514-4522-9008-5eceeaad1b57
      batch.kubernetes.io/job-name: bitwarden-backup-29060700
      controller-uid: 44873d7a-6514-4522-9008-5eceeaad1b57
      job-name: bitwarden-backup-29060700
    name: bitwarden-backup-29060700-whr4k
    namespace: bitwarden
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: bitwarden-backup-29060700
      uid: 44873d7a-6514-4522-9008-5eceeaad1b57
    resourceVersion: "11099987"
    uid: 01601ece-2ceb-4fe3-93ba-3d82a820ba49
  spec:
    containers:
    - args:
      - |
        # Install required packages
        apk add --no-cache curl openssh-client sshpass && \
        # Install kubectl
        curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
        chmod +x ./kubectl && \
        mv ./kubectl /usr/local/bin/kubectl && \
        /backup-script.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: NAS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: nas-credentials
      image: alpine:3.18
      imagePullPolicy: IfNotPresent
      name: backup
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
        subPath: backup-script.sh
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9t9qc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: bitwarden-backup
    serviceAccountName: bitwarden-backup
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 511
        name: bitwarden-backup-script
      name: backup-script
    - name: kube-api-access-9t9qc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T01:00:12Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T01:00:00Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T01:00:10Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T01:00:10Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T01:00:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4b92d787da5be646fa2eb551a76742a072e68b67e7b774822d4deaf7fc38b99f
      image: docker.io/library/alpine:3.18
      imageID: docker.io/library/alpine@sha256:de0eb0b3f2a47ba1eb89389859a9bd88b28e82f5826b6969ad604979713c2d4f
      lastState: {}
      name: backup
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4b92d787da5be646fa2eb551a76742a072e68b67e7b774822d4deaf7fc38b99f
          exitCode: 0
          finishedAt: "2025-04-03T01:00:09Z"
          reason: Completed
          startedAt: "2025-04-03T01:00:01Z"
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9t9qc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-04-03T01:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-04T01:00:00Z"
    generateName: bitwarden-backup-29062140-
    labels:
      batch.kubernetes.io/controller-uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
      batch.kubernetes.io/job-name: bitwarden-backup-29062140
      controller-uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
      job-name: bitwarden-backup-29062140
    name: bitwarden-backup-29062140-x9v6t
    namespace: bitwarden
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: bitwarden-backup-29062140
      uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
    resourceVersion: "11084773"
    uid: ff513506-7856-45a1-8eeb-d0ba43544856
  spec:
    containers:
    - args:
      - |
        # Install required packages
        apk add --no-cache curl openssh-client sshpass && \
        # Install kubectl
        curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
        chmod +x ./kubectl && \
        mv ./kubectl /usr/local/bin/kubectl && \
        /backup-script.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: NAS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: nas-credentials
      image: alpine:3.18
      imagePullPolicy: IfNotPresent
      name: backup
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
        subPath: backup-script.sh
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jcxcz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: bitwarden-backup
    serviceAccountName: bitwarden-backup
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 511
        name: bitwarden-backup-script
      name: backup-script
    - name: kube-api-access-jcxcz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T01:00:07Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T01:00:00Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T01:00:06Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T01:00:06Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T01:00:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8bbf30fbdcafc55755c401348e9cd822783b3745d89f09b258ce5743bea08018
      image: docker.io/library/alpine:3.18
      imageID: docker.io/library/alpine@sha256:de0eb0b3f2a47ba1eb89389859a9bd88b28e82f5826b6969ad604979713c2d4f
      lastState: {}
      name: backup
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://8bbf30fbdcafc55755c401348e9cd822783b3745d89f09b258ce5743bea08018
          exitCode: 0
          finishedAt: "2025-04-04T01:00:05Z"
          reason: Completed
          startedAt: "2025-04-04T01:00:00Z"
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jcxcz
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-04-04T01:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-05T01:00:00Z"
    generateName: bitwarden-backup-29063580-
    labels:
      batch.kubernetes.io/controller-uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
      batch.kubernetes.io/job-name: bitwarden-backup-29063580
      controller-uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
      job-name: bitwarden-backup-29063580
    name: bitwarden-backup-29063580-x8829
    namespace: bitwarden
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: bitwarden-backup-29063580
      uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
    resourceVersion: "11084775"
    uid: 1938255d-92ee-4406-aa69-cecf6fd346ba
  spec:
    containers:
    - args:
      - |
        # Install required packages
        apk add --no-cache curl openssh-client sshpass && \
        # Install kubectl
        curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
        chmod +x ./kubectl && \
        mv ./kubectl /usr/local/bin/kubectl && \
        /backup-script.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: NAS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: nas-credentials
      image: alpine:3.18
      imagePullPolicy: IfNotPresent
      name: backup
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
        subPath: backup-script.sh
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zzrh6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: bitwarden-backup
    serviceAccountName: bitwarden-backup
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 511
        name: bitwarden-backup-script
      name: backup-script
    - name: kube-api-access-zzrh6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T01:00:07Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T01:00:00Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T01:00:06Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T01:00:06Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T01:00:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://edbad681a62e8688ee74ac4b5dbcce7c1237c9ee9f3ee7a74548e69741f22239
      image: docker.io/library/alpine:3.18
      imageID: docker.io/library/alpine@sha256:de0eb0b3f2a47ba1eb89389859a9bd88b28e82f5826b6969ad604979713c2d4f
      lastState: {}
      name: backup
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://edbad681a62e8688ee74ac4b5dbcce7c1237c9ee9f3ee7a74548e69741f22239
          exitCode: 0
          finishedAt: "2025-04-05T01:00:05Z"
          reason: Completed
          startedAt: "2025-04-05T01:00:00Z"
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zzrh6
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-04-05T01:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-27T03:42:11Z"
    generateName: cert-manager-b6fd485d9-
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: b6fd485d9
    name: cert-manager-b6fd485d9-lnr59
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-b6fd485d9
      uid: 79dcdd4d-9251-4ef6-8af3-9e252dd1dbb6
    resourceVersion: "9409217"
    uid: b246aead-a02a-4377-a8c1-7ebb92d4d3a6
  spec:
    containers:
    - args:
      - --v=2
      - --cluster-resource-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=kube-system
      - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.16.2
      - --max-concurrent-challenges=60
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-controller:v1.16.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          path: /livez
          port: http-healthz
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: cert-manager-controller
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      - containerPort: 9403
        name: http-healthz
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-59bzn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: k3s-w-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager
    serviceAccountName: cert-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-59bzn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-27T03:42:11Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-27T03:42:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-27T03:42:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-27T03:42:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-27T03:42:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9d051090f6ebd2e428c6977af1b435c5ba478539f2a2eb0597083c22baf6e171
      image: quay.io/jetstack/cert-manager-controller:v1.16.2
      imageID: quay.io/jetstack/cert-manager-controller@sha256:0eec9c89554011c9aa16eea88ab89a91b32dc8d6eb96307b7fc2dcd6bbbc7fcf
      lastState: {}
      name: cert-manager-controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-27T03:42:11Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-59bzn
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 10.42.3.127
    podIPs:
    - ip: 10.42.3.127
    qosClass: BestEffort
    startTime: "2025-03-27T03:42:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-28T17:05:08Z"
    generateName: cert-manager-cainjector-dcc5966bc-
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: dcc5966bc
    name: cert-manager-cainjector-dcc5966bc-57jw2
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-cainjector-dcc5966bc
      uid: 4fc0038f-bc31-4fc8-be51-dbf57252ab23
    resourceVersion: "6013960"
    uid: 2cc06cee-cba9-48b4-9f05-78ac3ea5d1a6
  spec:
    containers:
    - args:
      - --v=2
      - --leader-election-namespace=kube-system
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-cainjector:v1.16.2
      imagePullPolicy: IfNotPresent
      name: cert-manager-cainjector
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wshsn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-cainjector
    serviceAccountName: cert-manager-cainjector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-wshsn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e71ee953efaab52a0a2572d015fdc2e81a91e44b4bbe53f49b60fc43e877e3a7
      image: quay.io/jetstack/cert-manager-cainjector:v1.16.2
      imageID: quay.io/jetstack/cert-manager-cainjector@sha256:593abc7bf8b926d6fcb9c63b91504ad85e1379b25056aa5dee90b35505ab3211
      lastState:
        terminated:
          containerID: containerd://adc9ef7bb4fc4c5349b49f302d7479aace977a22ec0876b477b479db4e7a7ad2
          exitCode: 255
          finishedAt: "2025-03-09T20:21:08Z"
          reason: Unknown
          startedAt: "2025-03-09T18:51:27Z"
      name: cert-manager-cainjector
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:11Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wshsn
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.219
    podIPs:
    - ip: 10.42.0.219
    qosClass: BestEffort
    startTime: "2025-02-28T17:05:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-12T20:36:46Z"
    generateName: cert-manager-webhook-dfb76c7bd-
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: dfb76c7bd
    name: cert-manager-webhook-dfb76c7bd-gdcdh
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-webhook-dfb76c7bd
      uid: b1c06a3b-85ac-4e62-807a-aee5fb822bbd
    resourceVersion: "10640696"
    uid: 22101998-7dcd-4eb8-b339-97ca67aee65c
  spec:
    containers:
    - args:
      - --v=2
      - --secure-port=10250
      - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
      - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
      - --dynamic-serving-dns-names=cert-manager-webhook
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-webhook:v1.16.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: cert-manager-webhook
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      - containerPort: 6080
        name: healthcheck
        protocol: TCP
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-krqpg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-webhook
    serviceAccountName: cert-manager-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-krqpg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-12T20:36:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-12T20:36:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b45438c0cf0c491487ed7621837786ddeb23c6ba697b03c240d0cc7ec78e388f
      image: quay.io/jetstack/cert-manager-webhook:v1.16.2
      imageID: quay.io/jetstack/cert-manager-webhook@sha256:64c4d5dd55d7d00e5b162e21887f9bffbf1e531ee0668eaf801fae9bd54e536d
      lastState:
        terminated:
          containerID: containerd://a5ec1e6236cc481d9355b47b3af312cbd7dcf37dd8f1ad427d8ed115f7edeaae
          exitCode: 0
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Completed
          startedAt: "2025-03-09T01:33:11Z"
      name: cert-manager-webhook
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:12Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-krqpg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.220
    podIPs:
    - ip: 10.42.0.220
    qosClass: BestEffort
    startTime: "2025-02-12T20:36:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-26T19:21:46Z"
    generateName: cloudflared-c8b747df5-
    labels:
      app: cloudflared
      pod-template-hash: c8b747df5
    name: cloudflared-c8b747df5-68hj5
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cloudflared-c8b747df5
      uid: 51f1d663-a1ca-42ca-92d6-4772ecd64901
    resourceVersion: "11099886"
    uid: 281ed8ad-6fd0-4292-a506-c6137750ae62
  spec:
    containers:
    - args:
      - tunnel
      - --config
      - /etc/cloudflared/config.yaml
      - run
      image: cloudflare/cloudflared:latest
      imagePullPolicy: Always
      name: cloudflared
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cloudflared/config.yaml
        name: config
        subPath: config.yaml
      - mountPath: /etc/cloudflared/credentials.json
        name: creds
        subPath: credentials.json
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kpkb9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: cloudflared-config
      name: config
    - name: creds
      secret:
        defaultMode: 420
        secretName: tunnel-credentials
    - name: kube-api-access-kpkb9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:22:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:22:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://20f290f678eb63f1b13692134247a4ebb87f67f1301443a286767609d826627a
      image: docker.io/cloudflare/cloudflared:latest
      imageID: docker.io/cloudflare/cloudflared@sha256:03737f27c38ecfb257a55664953cac510727cf27052c51ddb7c8ff1a2b9969e1
      lastState: {}
      name: cloudflared
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:22:07Z"
      volumeMounts:
      - mountPath: /etc/cloudflared/config.yaml
        name: config
      - mountPath: /etc/cloudflared/credentials.json
        name: creds
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kpkb9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.95
    podIPs:
    - ip: 10.42.1.95
    qosClass: BestEffort
    startTime: "2025-03-26T19:21:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-05T12:29:52Z"
    labels:
      run: curl
    name: curl
    namespace: default
    resourceVersion: "11104843"
    uid: 0412e103-edd5-4933-a99e-dd557b9ba76c
  spec:
    containers:
    - args:
      - sleep
      - "600"
      image: curlimages/curl
      imagePullPolicy: Always
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6zxg4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-6zxg4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:39:55Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:29:52Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:39:54Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:39:54Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:29:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://746756a4fc2510e9bb85b22889eb060ef1211442a44b50784afc10382acad590
      image: docker.io/curlimages/curl:latest
      imageID: docker.io/curlimages/curl@sha256:94e9e444bcba979c2ea12e27ae39bee4cd10bc7041a472c4727a558e213744e6
      lastState: {}
      name: curl
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://746756a4fc2510e9bb85b22889eb060ef1211442a44b50784afc10382acad590
          exitCode: 0
          finishedAt: "2025-04-05T12:39:53Z"
          reason: Completed
          startedAt: "2025-04-05T12:29:53Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6zxg4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Succeeded
    podIP: 10.42.3.88
    podIPs:
    - ip: 10.42.3.88
    qosClass: BestEffort
    startTime: "2025-04-05T12:29:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"debug-pvc","namespace":"default"},"spec":{"containers":[{"command":["sleep","3600"],"image":"busybox","name":"debug","volumeMounts":[{"mountPath":"/userdata","name":"userdata"},{"mountPath":"/conf","name":"conf"},{"mountPath":"/addons","name":"addons"},{"mountPath":"/karaf","name":"karaf"}]}],"volumes":[{"name":"userdata","persistentVolumeClaim":{"claimName":"openhab-production-userdata-claim"}},{"name":"conf","persistentVolumeClaim":{"claimName":"openhab-production-conf-claim"}},{"name":"addons","persistentVolumeClaim":{"claimName":"openhab-production-addons-claim"}},{"name":"karaf","persistentVolumeClaim":{"claimName":"openhab-production-karaf-claim"}}]}}
    creationTimestamp: "2025-02-15T10:57:49Z"
    name: debug-pvc
    namespace: default
    resourceVersion: "11111155"
    uid: 5da3a368-ac08-4fd7-89ac-e29ba6a499d2
  spec:
    containers:
    - command:
      - sleep
      - "3600"
      image: busybox
      imagePullPolicy: Always
      name: debug
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /userdata
        name: userdata
      - mountPath: /conf
        name: conf
      - mountPath: /addons
        name: addons
      - mountPath: /karaf
        name: karaf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tr2lp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: userdata
      persistentVolumeClaim:
        claimName: openhab-production-userdata-claim
    - name: conf
      persistentVolumeClaim:
        claimName: openhab-production-conf-claim
    - name: addons
      persistentVolumeClaim:
        claimName: openhab-production-addons-claim
    - name: karaf
      persistentVolumeClaim:
        claimName: openhab-production-karaf-claim
    - name: kube-api-access-tr2lp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:44:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-15T10:57:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T13:29:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T13:29:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-15T10:57:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ce8645d53fa4a537d2d8bbfd0598451be0c567a66f9ea54ba49bca58f631a3fa
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:29c2339e8727db1f58366f70f63063154c8230beb9ce1833380b71fa92bd0b89
      lastState:
        terminated:
          containerID: containerd://1f8a34511da3b1362d7aa579ec5c1bc3dd7301c8ed45085684c953256a3ec955
          exitCode: 0
          finishedAt: "2025-04-05T13:29:20Z"
          reason: Completed
          startedAt: "2025-04-05T12:29:20Z"
      name: debug
      ready: true
      restartCount: 1176
      started: true
      state:
        running:
          startedAt: "2025-04-05T13:29:22Z"
      volumeMounts:
      - mountPath: /userdata
        name: userdata
      - mountPath: /conf
        name: conf
      - mountPath: /addons
        name: addons
      - mountPath: /karaf
        name: karaf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tr2lp
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.231
    podIPs:
    - ip: 10.42.0.231
    qosClass: BestEffort
    startTime: "2025-02-15T10:57:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-21T22:07:46Z"
    name: node-debugger-k3s-w-1-4h88c
    namespace: default
    resourceVersion: "11099916"
    uid: d0178dd1-f8f5-4ede-b937-e4d835c98a0c
  spec:
    containers:
    - image: busybox
      imagePullPolicy: Always
      name: debugger
      resources: {}
      stdin: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /host
        name: host-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bvgsb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostIPC: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-root
    - name: kube-api-access-bvgsb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T22:07:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T22:07:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T22:07:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T22:07:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4b1b3462dcc0045934bb40c96a95712d8669fe1679f79d0d0f3e1612a909198a
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:37f7b378a29ceb4c551b1b5582e27747b855bbfaa73fa11914fe0df028dc581f
      lastState: {}
      name: debugger
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-21T22:07:49Z"
      volumeMounts:
      - mountPath: /host
        name: host-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bvgsb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 192.168.50.237
    podIPs:
    - ip: 192.168.50.237
    qosClass: BestEffort
    startTime: "2025-03-21T22:07:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-22T16:29:04Z"
    name: node-debugger-k3s-w-1-bpccz
    namespace: default
    resourceVersion: "11099973"
    uid: 0f75aacb-1167-4e60-b066-af57a937ec39
  spec:
    containers:
    - image: busybox
      imagePullPolicy: Always
      name: debugger
      resources: {}
      stdin: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /host
        name: host-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5pmlk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostIPC: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-root
    - name: kube-api-access-5pmlk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T16:29:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T16:29:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T16:29:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T16:29:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://54fe0b0d8e67410f82070b5522ee34611b25c367d0dfc3fb2bd97c242442e859
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:37f7b378a29ceb4c551b1b5582e27747b855bbfaa73fa11914fe0df028dc581f
      lastState: {}
      name: debugger
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-22T16:29:06Z"
      volumeMounts:
      - mountPath: /host
        name: host-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5pmlk
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 192.168.50.237
    podIPs:
    - ip: 192.168.50.237
    qosClass: BestEffort
    startTime: "2025-03-22T16:29:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-17T20:09:51Z"
    name: node-debugger-k3s-w-3-5x58k
    namespace: default
    resourceVersion: "11084777"
    uid: a7cef8b8-2e20-4d85-acb3-d4ccbe912320
  spec:
    containers:
    - command:
      - chroot
      - /host
      - lsblk
      image: ubuntu
      imagePullPolicy: Always
      name: debugger
      resources: {}
      stdin: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /host
        name: host-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pbv2b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostIPC: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-root
    - name: kube-api-access-pbv2b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:09:58Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:09:51Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:09:51Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:09:51Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:09:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c1d5bbd698e7cb4bb07fd7226f1262cffd8694292bb249d9d63ebf2cbae8ecaf
      image: docker.io/library/ubuntu:latest
      imageID: docker.io/library/ubuntu@sha256:72297848456d5d37d1262630108ab308d3e9ec7ed1c3286a32fe09856619a782
      lastState: {}
      name: debugger
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c1d5bbd698e7cb4bb07fd7226f1262cffd8694292bb249d9d63ebf2cbae8ecaf
          exitCode: 0
          finishedAt: "2025-03-17T20:09:56Z"
          reason: Completed
          startedAt: "2025-03-17T20:09:56Z"
      volumeMounts:
      - mountPath: /host
        name: host-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pbv2b
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Succeeded
    podIP: 192.168.50.108
    podIPs:
    - ip: 192.168.50.108
    qosClass: BestEffort
    startTime: "2025-03-17T20:09:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-17T20:17:28Z"
    name: node-debugger-k3s-w-3-9l9zp
    namespace: default
    resourceVersion: "11084779"
    uid: 48f4f8af-6d39-44ff-be9a-ad665078a070
  spec:
    containers:
    - command:
      - chroot
      - /host
      - lsblk
      - -f
      image: ubuntu
      imagePullPolicy: Always
      name: debugger
      resources: {}
      stdin: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
      - mountPath: /host
        name: host-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-txsrv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostIPC: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-root
    - name: kube-api-access-txsrv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:17:31Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:17:28Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:17:28Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:17:28Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:17:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2f6763a698b74e2a34177db421ffcc2d60a32c9f936187d5ce19cc1120952caf
      image: docker.io/library/ubuntu:latest
      imageID: docker.io/library/ubuntu@sha256:72297848456d5d37d1262630108ab308d3e9ec7ed1c3286a32fe09856619a782
      lastState: {}
      name: debugger
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://2f6763a698b74e2a34177db421ffcc2d60a32c9f936187d5ce19cc1120952caf
          exitCode: 0
          finishedAt: "2025-03-17T20:17:30Z"
          reason: Completed
          startedAt: "2025-03-17T20:17:30Z"
      volumeMounts:
      - mountPath: /host
        name: host-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-txsrv
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Succeeded
    podIP: 192.168.50.108
    podIPs:
    - ip: 192.168.50.108
    qosClass: BestEffort
    startTime: "2025-03-17T20:17:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-26T19:21:45Z"
    generateName: helm-controller-7f788c795c-
    labels:
      app: helm-controller
      pod-template-hash: 7f788c795c
    name: helm-controller-7f788c795c-b8gbh
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: helm-controller-7f788c795c
      uid: 0d8538e9-84ad-4df8-9789-6829cc8b5ea5
    resourceVersion: "10640656"
    uid: f4374079-c7e3-495b-83d7-38cfc4248b95
  spec:
    containers:
    - args:
      - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
      - --watch-all-namespaces=true
      - --log-level=info
      - --log-encoding=json
      - --enable-leader-election
      env:
      - name: RUNTIME_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: GOMAXPROCS
        valueFrom:
          resourceFieldRef:
            containerName: manager
            divisor: "0"
            resource: limits.cpu
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            containerName: manager
            divisor: "0"
            resource: limits.memory
      image: ghcr.io/fluxcd/helm-controller:v1.1.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 8080
        name: http-prom
        protocol: TCP
      - containerPort: 9440
        name: healthz
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: temp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pgksv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1337
    serviceAccount: helm-controller
    serviceAccountName: helm-controller
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: temp
    - name: kube-api-access-pgksv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:51Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fd3fdf7eceafaed292b59bd8e1a9c4251f8c1fa8139f28588291947ed573f4d4
      image: ghcr.io/fluxcd/helm-controller:v1.1.0
      imageID: ghcr.io/fluxcd/helm-controller@sha256:4c75ca6c24ceb1f1bd7e935d9287a93e4f925c512f206763ec5a47de3ef3ff48
      lastState: {}
      name: manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:21:50Z"
      volumeMounts:
      - mountPath: /tmp
        name: temp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pgksv
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.110
    podIPs:
    - ip: 10.42.0.110
    qosClass: Burstable
    startTime: "2025-03-26T19:21:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-28T17:05:08Z"
    generateName: kustomize-controller-b4f45fff6-
    labels:
      app: kustomize-controller
      pod-template-hash: b4f45fff6
    name: kustomize-controller-b4f45fff6-8rb5k
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kustomize-controller-b4f45fff6
      uid: 0b6928a5-2466-48dc-88ad-681d41736161
    resourceVersion: "10640664"
    uid: 9177e0ab-d25e-4794-a572-404cd2ed6819
  spec:
    containers:
    - args:
      - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
      - --watch-all-namespaces=true
      - --log-level=info
      - --log-encoding=json
      - --enable-leader-election
      env:
      - name: RUNTIME_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: GOMAXPROCS
        valueFrom:
          resourceFieldRef:
            containerName: manager
            divisor: "0"
            resource: limits.cpu
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            containerName: manager
            divisor: "0"
            resource: limits.memory
      image: ghcr.io/fluxcd/kustomize-controller:v1.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 8080
        name: http-prom
        protocol: TCP
      - containerPort: 9440
        name: healthz
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: temp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kfc95
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1337
    serviceAccount: kustomize-controller
    serviceAccountName: kustomize-controller
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: temp
    - name: kube-api-access-kfc95
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:15Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://03a328dc16eb770e455714d5f0af676fafcd45877ddcd800a752a0ede7b87851
      image: ghcr.io/fluxcd/kustomize-controller:v1.4.0
      imageID: ghcr.io/fluxcd/kustomize-controller@sha256:e3b0cf847e9cdf47b19af0fbcfe22786b80b598e0caeea8b6d2a5f9c26a48a24
      lastState:
        terminated:
          containerID: containerd://935dbedeff7e76cdec6736c1e65656fa48fe96dfc8df23d2ab7388d85efbc14b
          exitCode: 255
          finishedAt: "2025-03-09T20:21:08Z"
          reason: Unknown
          startedAt: "2025-03-09T18:51:25Z"
      name: manager
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:12Z"
      volumeMounts:
      - mountPath: /tmp
        name: temp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kfc95
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.226
    podIPs:
    - ip: 10.42.0.226
    qosClass: Burstable
    startTime: "2025-02-28T17:05:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-28T17:05:08Z"
    generateName: notification-controller-556b8867f8-
    labels:
      app: notification-controller
      pod-template-hash: 556b8867f8
    name: notification-controller-556b8867f8-vfjkk
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: notification-controller-556b8867f8
      uid: 1366df8e-b946-4e13-8789-72a6016552e4
    resourceVersion: "10640710"
    uid: 5943c6d0-7f26-47be-a1cc-c41f6a34d9d8
  spec:
    containers:
    - args:
      - --watch-all-namespaces=true
      - --log-level=info
      - --log-encoding=json
      - --enable-leader-election
      env:
      - name: RUNTIME_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: GOMAXPROCS
        valueFrom:
          resourceFieldRef:
            containerName: manager
            divisor: "0"
            resource: limits.cpu
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            containerName: manager
            divisor: "0"
            resource: limits.memory
      image: ghcr.io/fluxcd/notification-controller:v1.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 9090
        name: http
        protocol: TCP
      - containerPort: 9292
        name: http-webhook
        protocol: TCP
      - containerPort: 8080
        name: http-prom
        protocol: TCP
      - containerPort: 9440
        name: healthz
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: temp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8mfs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1337
    serviceAccount: notification-controller
    serviceAccountName: notification-controller
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: temp
    - name: kube-api-access-t8mfs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://514af1a0cecf842810d7f627c1db16166a7d8a9fb677dcb3b02d1a7f1464fc8b
      image: ghcr.io/fluxcd/notification-controller:v1.4.0
      imageID: ghcr.io/fluxcd/notification-controller@sha256:425309a159b15e07f7d97622effc79bc432a37ed55289dd465d37fa217a92a7d
      lastState:
        terminated:
          containerID: containerd://c017560c946e82da7e6788f2b263be0973163bda5efc145c4cc517feee9ee328
          exitCode: 1
          finishedAt: "2025-03-09T19:01:18Z"
          reason: Error
          startedAt: "2025-03-09T18:51:25Z"
      name: manager
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:19Z"
      volumeMounts:
      - mountPath: /tmp
        name: temp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t8mfs
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.228
    podIPs:
    - ip: 10.42.0.228
    qosClass: Burstable
    startTime: "2025-02-28T17:05:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-28T17:05:08Z"
    generateName: source-controller-77d6cd56c9-
    labels:
      app: source-controller
      pod-template-hash: 77d6cd56c9
    name: source-controller-77d6cd56c9-rs9zt
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: source-controller-77d6cd56c9
      uid: 56673d73-1911-471b-9826-b002a1862a9e
    resourceVersion: "10640738"
    uid: cba4c8dd-7db1-4525-8d03-96164a88e2c7
  spec:
    containers:
    - args:
      - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
      - --watch-all-namespaces=true
      - --log-level=info
      - --log-encoding=json
      - --enable-leader-election
      - --storage-path=/data
      - --storage-adv-addr=source-controller.$(RUNTIME_NAMESPACE).svc.cluster.local.
      env:
      - name: RUNTIME_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: TUF_ROOT
        value: /tmp/.sigstore
      - name: GOMAXPROCS
        valueFrom:
          resourceFieldRef:
            containerName: manager
            divisor: "0"
            resource: limits.cpu
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            containerName: manager
            divisor: "0"
            resource: limits.memory
      image: ghcr.io/fluxcd/source-controller:v1.4.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 9090
        name: http
        protocol: TCP
      - containerPort: 8080
        name: http-prom
        protocol: TCP
      - containerPort: 9440
        name: healthz
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x6c6x
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1337
    serviceAccount: source-controller
    serviceAccountName: source-controller
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-x6c6x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:13Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9ab392fc1746929d55d5c1901ce29fc742f5c21e567e10d6fa093a51da61cb4d
      image: ghcr.io/fluxcd/source-controller:v1.4.1
      imageID: ghcr.io/fluxcd/source-controller@sha256:3c5f0f022f990ffc0daf00e5b199548fc0fa6e7119e972318f0267081a332963
      lastState:
        terminated:
          containerID: containerd://52daffc4da8a45e5313db3363fccf623b42910216d3aca8ca0b855cea929e359
          exitCode: 255
          finishedAt: "2025-03-09T20:21:08Z"
          reason: Unknown
          startedAt: "2025-03-09T18:51:25Z"
      name: manager
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:11Z"
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x6c6x
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.213
    podIPs:
    - ip: 10.42.0.213
    qosClass: Burstable
    startTime: "2025-02-28T17:05:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-28T17:05:08Z"
    generateName: ww-gitops-weave-gitops-6bd7fbd8c4-
    labels:
      app.kubernetes.io/instance: ww-gitops
      app.kubernetes.io/name: weave-gitops
      app.kubernetes.io/part-of: weave-gitops
      pod-template-hash: 6bd7fbd8c4
      weave.works/app: weave-gitops-oss
    name: ww-gitops-weave-gitops-6bd7fbd8c4-6pc6l
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ww-gitops-weave-gitops-6bd7fbd8c4
      uid: 54165ed3-3a05-4346-8311-e09d067886b2
    resourceVersion: "10640647"
    uid: 91188fae-a2d7-4e8b-af83-cee9af6c2ba2
  spec:
    containers:
    - args:
      - --log-level
      - info
      - --insecure
      env:
      - name: WEAVE_GITOPS_FEATURE_TENANCY
        value: "true"
      - name: WEAVE_GITOPS_FEATURE_CLUSTER
        value: "false"
      image: ghcr.io/weaveworks/wego-app:v0.38.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: weave-gitops
      ports:
      - containerPort: 9001
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2vzf9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ww-gitops-weave-gitops
    serviceAccountName: ww-gitops-weave-gitops
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-2vzf9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-28T17:05:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a64c0e6977bdd657a1f5d818b4e6d04b1a5fc3c663abd7f62cfb246c59f1e80b
      image: ghcr.io/weaveworks/wego-app:v0.38.0
      imageID: ghcr.io/weaveworks/wego-app@sha256:dd2ff7daf1711b190b1b12ac965c6a43c11c13b229af3aa5ebf9035382bbcc59
      lastState:
        terminated:
          containerID: containerd://6275f9563495ff2596e97ec20f2dc7ff606b47ed6cd9d7e83cc29a75f9ceb31c
          exitCode: 0
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Completed
          startedAt: "2025-03-09T01:33:10Z"
      name: weave-gitops
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:12Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2vzf9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.222
    podIPs:
    - ip: 10.42.0.222
    qosClass: BestEffort
    startTime: "2025-02-28T17:05:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-26T19:21:45Z"
    generateName: headlamp-66557f8596-
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 66557f8596
    name: headlamp-66557f8596-jgdzl
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: headlamp-66557f8596
      uid: 89700d92-e9ae-4939-bfd9-72acdeb85008
    resourceVersion: "11100009"
    uid: 89a73ee5-ca68-4254-8b77-2d35f974f3c8
  spec:
    containers:
    - args:
      - -in-cluster
      - -plugins-dir=/headlamp/plugins
      - --plugins-dir=/headlamp/plugins
      - --kubeconfig=/home/headlamp/.config/Headlamp/kubeconfigs/config
      env:
      - name: KUBECONFIG
        value: /home/headlamp/.config/Headlamp/kubeconfigs/config
      image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: headlamp
      ports:
      - containerPort: 4466
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        privileged: false
        runAsGroup: 101
        runAsNonRoot: true
        runAsUser: 100
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/headlamp
        name: config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54mm9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - |
        mkdir -p /home/headlamp/.config/Headlamp/kubeconfigs/
        cat > /home/headlamp/.config/Headlamp/kubeconfigs/config << EOF
        apiVersion: v1
        kind: Config
        current-context: default
        clusters:
        - name: default
          cluster:
            certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            server: https://10.43.0.1:443
        contexts:
        - name: default
          context:
            cluster: default
            user: default
        users:
        - name: default
          user:
            tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
        EOF
        chown -R 100:101 /home/headlamp
      command:
      - /bin/sh
      - -c
      image: busybox
      imagePullPolicy: Always
      name: init-config
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/headlamp
        name: config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54mm9
        readOnly: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: headlamp
    serviceAccountName: headlamp
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: config-dir
    - name: kube-api-access-54mm9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b1a35d3deda70bbcd47c21ed3824c63d07cb584da5c05471f535c26a6f1de47d
      image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
      imageID: ghcr.io/headlamp-k8s/headlamp@sha256:06e9c1f22d4185b90980181ce7acc3045d236e221ee5f59becd9a24e655b0c59
      lastState: {}
      name: headlamp
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:21:53Z"
      volumeMounts:
      - mountPath: /home/headlamp
        name: config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54mm9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    initContainerStatuses:
    - containerID: containerd://a5871768691a537304b2922b9d90f88d1fe3eaa37ca6d866f0e90834ba222176
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:37f7b378a29ceb4c551b1b5582e27747b855bbfaa73fa11914fe0df028dc581f
      lastState: {}
      name: init-config
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a5871768691a537304b2922b9d90f88d1fe3eaa37ca6d866f0e90834ba222176
          exitCode: 0
          finishedAt: "2025-03-26T19:21:50Z"
          reason: Completed
          startedAt: "2025-03-26T19:21:50Z"
      volumeMounts:
      - mountPath: /home/headlamp
        name: config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-54mm9
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.1.89
    podIPs:
    - ip: 10.42.1.89
    qosClass: BestEffort
    startTime: "2025-03-26T19:21:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-26T21:00:08+01:00"
    creationTimestamp: "2025-03-26T19:21:45Z"
    generateName: coredns-554b67d6c4-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 554b67d6c4
    name: coredns-554b67d6c4-h4hcp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-554b67d6c4
      uid: 4cb8d08d-fe8c-42a7-a6d6-0806f66894dc
    resourceVersion: "11099997"
    uid: 92a3b803-a50c-4c55-9d7a-e096b60ba840
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.12.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z28l4
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: k3s-w-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          k8s-app: kube-dns
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        - key: NodeHosts
          path: NodeHosts
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-z28l4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:49Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2ac9a8e78692654fdbd9608052460f43d3c2ed3b41f26645e12839a690658945
      image: docker.io/rancher/mirrored-coredns-coredns:1.12.0
      imageID: docker.io/rancher/mirrored-coredns-coredns@sha256:82979ddf442c593027a57239ad90616deb874e90c365d1a96ad508c2104bdea5
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:21:49Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z28l4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.90
    podIPs:
    - ip: 10.42.1.90
    qosClass: Burstable
    startTime: "2025-03-26T19:21:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
    creationTimestamp: "2025-02-08T22:04:28Z"
    generateName: helm-install-traefik-crd-
    labels:
      batch.kubernetes.io/controller-uid: 1d05ec10-0395-4b33-9e6c-a8a5886f731a
      batch.kubernetes.io/job-name: helm-install-traefik-crd
      controller-uid: 1d05ec10-0395-4b33-9e6c-a8a5886f731a
      helmcharts.helm.cattle.io/chart: traefik-crd
      job-name: helm-install-traefik-crd
    name: helm-install-traefik-crd-f646j
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik-crd
      uid: 1d05ec10-0395-4b33-9e6c-a8a5886f731a
    resourceVersion: "2617872"
    uid: 3a9e8dfc-4d5b-45d3-8dc0-914c26eddc64
  spec:
    containers:
    - args:
      - install
      env:
      - name: NAME
        value: traefik-crd
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-27.0.201+up27.0.2.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.3-build20241008
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tmqlx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-traefik-crd
    serviceAccountName: helm-traefik-crd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik-crd
    - configMap:
        defaultMode: 420
        name: chart-content-traefik-crd
      name: content
    - name: kube-api-access-tmqlx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:41Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:30Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:40Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:40Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2223d264cf9977e60f0ab33245604a0d0fe7ef87eb1c912b72580940f3bb34af
      image: docker.io/rancher/klipper-helm:v0.9.3-build20241008
      imageID: docker.io/rancher/klipper-helm@sha256:73ff7ef399717ba8339559054557bd427bdafb47db112165a8c0c358d1ca0283
      lastState: {}
      name: helm
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://2223d264cf9977e60f0ab33245604a0d0fe7ef87eb1c912b72580940f3bb34af
          exitCode: 0
          finishedAt: "2025-02-08T22:04:40Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-02-08T22:04:39Z"
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tmqlx
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-02-08T22:04:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-08T22:04:28Z"
    generateName: local-path-provisioner-5cf85fd84d-
    labels:
      app: local-path-provisioner
      pod-template-hash: 5cf85fd84d
    name: local-path-provisioner-5cf85fd84d-g7tmt
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-5cf85fd84d
      uid: 2092e1f8-4754-43a4-aa6f-28d7daf6b021
    resourceVersion: "6014007"
    uid: 919d46e2-ec3f-4b96-9a86-9d79f6e659ce
  spec:
    containers:
    - command:
      - local-path-provisioner
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.30
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lfqhs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-lfqhs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4e226663e9e2de647c5ff1d3f4daad1bb8b82b4f03f40e7baba07035912cd5b5
      image: docker.io/rancher/local-path-provisioner:v0.0.30
      imageID: docker.io/rancher/local-path-provisioner@sha256:9b914881170048f80ae9302f36e5b99b4a6b18af73a38adc1c66d12f65d360be
      lastState:
        terminated:
          containerID: containerd://259a5c6ee453b888f3e134f98e1565d223eba77374fb5d893c85f87e808d1bb2
          exitCode: 255
          finishedAt: "2025-03-09T20:21:08Z"
          reason: Unknown
          startedAt: "2025-02-08T22:04:36Z"
      name: local-path-provisioner
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:12Z"
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lfqhs
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.221
    podIPs:
    - ip: 10.42.0.221
    qosClass: BestEffort
    startTime: "2025-02-08T22:04:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-08T22:04:28Z"
    generateName: metrics-server-5985cbc9d7-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5985cbc9d7
    name: metrics-server-5985cbc9d7-rpx4k
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-5985cbc9d7
      uid: 23e60b8d-b20d-4ef4-82c5-e054dcc5e60f
    resourceVersion: "10640649"
    uid: 52ba19b0-ea60-4ce7-ab7b-d4334a89b4f3
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=10250
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
      image: rancher/mirrored-metrics-server:v0.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-82tqr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-82tqr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6a656686140512fcfef5c6da2a1dbd8a5154dac19f95f221958c04a1dc9bcbc1
      image: docker.io/rancher/mirrored-metrics-server:v0.7.2
      imageID: docker.io/rancher/mirrored-metrics-server@sha256:dccf8474fb910fef261d31d9483d7e4c1df7b86cf4d638fb6a7d7c88bd51600a
      lastState:
        terminated:
          containerID: containerd://646f6b9a877c609fd4a75b30b862e635c4032fd16f9edf785be891ba3d5f6051
          exitCode: 2
          finishedAt: "2025-03-09T19:01:25Z"
          reason: Error
          startedAt: "2025-03-09T18:51:25Z"
      name: metrics-server
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:19Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-82tqr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.229
    podIPs:
    - ip: 10.42.0.229
    qosClass: Burstable
    startTime: "2025-02-08T22:04:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-05T11:39:15Z"
    generateName: pushprox-kube-proxy-client-
    labels:
      component: kube-proxy
      controller-revision-hash: 7d4ff9c5cd
      k8s-app: pushprox-kube-proxy-client
      pod-template-generation: "7"
      provider: kubernetes
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-client-jkhtx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: pushprox-kube-proxy-client
      uid: 0787b257-cf34-4cd7-ab68-49495a5d40dc
    resourceVersion: "11096612"
    uid: 732f7b5a-6066-4b1c-afab-bca5a233ffeb
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-m-1
    containers:
    - args:
      - --fqdn=$(HOST_IP)
      - --proxy-url=$(PROXY_URL)
      - --allow-port=10249
      command:
      - pushprox-client
      env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: PROXY_URL
        value: http://pushprox-kube-proxy-proxy.kube-system.svc:8080
      image: rancher/pushprox-client:v0.1.0-rancher2-client
      imagePullPolicy: IfNotPresent
      name: pushprox-client
      resources: {}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zxs7s
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: pushprox-kube-proxy-client
    serviceAccountName: pushprox-kube-proxy-client
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: kube-api-access-zxs7s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2811a968968a420cacecf087ac6b43455fa36cbe235277e41476ce5532f316fa
      image: docker.io/rancher/pushprox-client:v0.1.0-rancher2-client
      imageID: docker.io/rancher/pushprox-client@sha256:a41cd716c41208909102acfcb4b97d3c556f42c0b35f9123738d39759f3c2aec
      lastState: {}
      name: pushprox-client
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-05T11:39:15Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zxs7s
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 192.168.50.75
    podIPs:
    - ip: 192.168.50.75
    qosClass: BestEffort
    startTime: "2025-04-05T11:39:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-05T11:39:28Z"
    generateName: pushprox-kube-proxy-client-
    labels:
      component: kube-proxy
      controller-revision-hash: 7d4ff9c5cd
      k8s-app: pushprox-kube-proxy-client
      pod-template-generation: "7"
      provider: kubernetes
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-client-mkncb
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: pushprox-kube-proxy-client
      uid: 0787b257-cf34-4cd7-ab68-49495a5d40dc
    resourceVersion: "11096668"
    uid: 804a492a-c4c3-415a-ad53-2facb111f08e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-2
    containers:
    - args:
      - --fqdn=$(HOST_IP)
      - --proxy-url=$(PROXY_URL)
      - --allow-port=10249
      command:
      - pushprox-client
      env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: PROXY_URL
        value: http://pushprox-kube-proxy-proxy.kube-system.svc:8080
      image: rancher/pushprox-client:v0.1.0-rancher2-client
      imagePullPolicy: IfNotPresent
      name: pushprox-client
      resources: {}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kx6qs
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: pushprox-kube-proxy-client
    serviceAccountName: pushprox-kube-proxy-client
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: kube-api-access-kx6qs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9bc20b5b3128aaf57a75f0a2e567063c5227569d545b684aa06ab1c3a9f84716
      image: docker.io/rancher/pushprox-client:v0.1.0-rancher2-client
      imageID: docker.io/rancher/pushprox-client@sha256:a41cd716c41208909102acfcb4b97d3c556f42c0b35f9123738d39759f3c2aec
      lastState: {}
      name: pushprox-client
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-05T11:39:30Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kx6qs
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 192.168.50.115
    podIPs:
    - ip: 192.168.50.115
    qosClass: BestEffort
    startTime: "2025-04-05T11:39:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-05T11:39:15Z"
    generateName: pushprox-kube-proxy-client-
    labels:
      component: kube-proxy
      controller-revision-hash: 7d4ff9c5cd
      k8s-app: pushprox-kube-proxy-client
      pod-template-generation: "7"
      provider: kubernetes
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-client-rtp4q
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: pushprox-kube-proxy-client
      uid: 0787b257-cf34-4cd7-ab68-49495a5d40dc
    resourceVersion: "11096617"
    uid: c3a40643-2d3a-4e6e-bbf0-03b6d9771f6e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-3
    containers:
    - args:
      - --fqdn=$(HOST_IP)
      - --proxy-url=$(PROXY_URL)
      - --allow-port=10249
      command:
      - pushprox-client
      env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: PROXY_URL
        value: http://pushprox-kube-proxy-proxy.kube-system.svc:8080
      image: rancher/pushprox-client:v0.1.0-rancher2-client
      imagePullPolicy: IfNotPresent
      name: pushprox-client
      resources: {}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rphpw
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: pushprox-kube-proxy-client
    serviceAccountName: pushprox-kube-proxy-client
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: kube-api-access-rphpw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:39:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c220e68e6636085bc02a67243b605a67db446175f83892b068e5dcf2a7a60143
      image: docker.io/rancher/pushprox-client:v0.1.0-rancher2-client
      imageID: docker.io/rancher/pushprox-client@sha256:a41cd716c41208909102acfcb4b97d3c556f42c0b35f9123738d39759f3c2aec
      lastState: {}
      name: pushprox-client
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-05T11:39:15Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rphpw
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 192.168.50.108
    podIPs:
    - ip: 192.168.50.108
    qosClass: BestEffort
    startTime: "2025-04-05T11:39:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-05T12:04:18Z"
    generateName: pushprox-kube-proxy-client-
    labels:
      component: kube-proxy
      controller-revision-hash: 7d4ff9c5cd
      k8s-app: pushprox-kube-proxy-client
      pod-template-generation: "7"
      provider: kubernetes
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-client-w6w6w
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: pushprox-kube-proxy-client
      uid: 0787b257-cf34-4cd7-ab68-49495a5d40dc
    resourceVersion: "11100260"
    uid: a094c46a-6e45-47d2-a059-a9f91d954322
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-1
    containers:
    - args:
      - --fqdn=$(HOST_IP)
      - --proxy-url=$(PROXY_URL)
      - --allow-port=10249
      command:
      - pushprox-client
      env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: PROXY_URL
        value: http://pushprox-kube-proxy-proxy.kube-system.svc:8080
      image: rancher/pushprox-client:v0.1.0-rancher2-client
      imagePullPolicy: IfNotPresent
      name: pushprox-client
      resources: {}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7zr9l
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: pushprox-kube-proxy-client
    serviceAccountName: pushprox-kube-proxy-client
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: kube-api-access-7zr9l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:19Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:18Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://dcf3cd55a788a44442fc3664bed323d5b2db366902979f76aaf4ffff5cc8b662
      image: docker.io/rancher/pushprox-client:v0.1.0-rancher2-client
      imageID: docker.io/rancher/pushprox-client@sha256:a41cd716c41208909102acfcb4b97d3c556f42c0b35f9123738d39759f3c2aec
      lastState: {}
      name: pushprox-client
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-05T12:04:19Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7zr9l
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 192.168.50.237
    podIPs:
    - ip: 192.168.50.237
    qosClass: BestEffort
    startTime: "2025-04-05T12:04:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-05T11:38:25Z"
    generateName: pushprox-kube-proxy-proxy-59874cc749-
    labels:
      component: kube-proxy
      k8s-app: pushprox-kube-proxy-proxy
      pod-template-hash: 59874cc749
      provider: kubernetes
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-proxy-59874cc749-gpbzm
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: pushprox-kube-proxy-proxy-59874cc749
      uid: 104a2ebf-a6b7-4537-8536-ba50d977d587
    resourceVersion: "11096437"
    uid: b96d74a3-9797-44c1-9def-8f9e28604e13
  spec:
    containers:
    - command:
      - pushprox-proxy
      image: rancher/pushprox-proxy:v0.1.0-rancher2-proxy
      imagePullPolicy: IfNotPresent
      name: pushprox-proxy
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vkfpt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: pushprox-kube-proxy-proxy
    serviceAccountName: pushprox-kube-proxy-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-vkfpt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:38:26Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:38:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:38:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:38:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T11:38:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b6f2a9cfb4ee06064165cbc677f21559f663b02a84c887874fb36789bc6c3d3d
      image: docker.io/rancher/pushprox-proxy:v0.1.0-rancher2-proxy
      imageID: docker.io/rancher/pushprox-proxy@sha256:3126395b966c1e52bb64a02d6967f856d0a72263349178402364dcab0cafb289
      lastState: {}
      name: pushprox-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-05T11:38:25Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vkfpt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 10.42.3.85
    podIPs:
    - ip: 10.42.3.85
    qosClass: BestEffort
    startTime: "2025-04-05T11:38:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-16T14:24:17Z"
    generateName: svclb-traefik-32c60abe-
    labels:
      app: svclb-traefik-32c60abe
      controller-revision-hash: 74c7bfd55
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-32c60abe-4f8t9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-32c60abe
      uid: 9adcea51-40ac-44a8-9251-0a8abf5f9768
    resourceVersion: "9322544"
    uid: 020d7ee8-6120-4adc-b36d-8bab4ebe9b55
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-3
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.202.202
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.202.202
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:44Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T14:24:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T14:24:17Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c1e1e36aef7235601f28be1817948335f48c65d24478d6d15a2b6533e0c40e90
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://72ddad08b1e74086631410aa10881e3f2a28bc18b1a936f01aded5d56f9ac87d
          exitCode: 255
          finishedAt: "2025-03-26T19:24:26Z"
          reason: Unknown
          startedAt: "2025-03-16T14:24:32Z"
      name: lb-tcp-443
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:24:43Z"
    - containerID: containerd://dfdcc977d5757c442e10b8d009b30d380a02caf4d20538d8233b9eef5938e5d6
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://94ee956fa33335c7db9bff90bc46fb26576c378b4b84a2b84acaa43fbc8b7e8d
          exitCode: 255
          finishedAt: "2025-03-26T19:24:26Z"
          reason: Unknown
          startedAt: "2025-03-16T14:24:32Z"
      name: lb-tcp-80
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:24:43Z"
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 10.42.3.180
    podIPs:
    - ip: 10.42.3.180
    qosClass: BestEffort
    startTime: "2025-03-16T14:24:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-08T22:04:41Z"
    generateName: svclb-traefik-32c60abe-
    labels:
      app: svclb-traefik-32c60abe
      controller-revision-hash: 74c7bfd55
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-32c60abe-cbk8j
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-32c60abe
      uid: 9adcea51-40ac-44a8-9251-0a8abf5f9768
    resourceVersion: "6013899"
    uid: 353ba683-9e7b-4723-b026-efee7af70880
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-m-1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.202.202
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.202.202
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:04:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1a0b8fef3d4b5a051a2e6abcb325f7701fd564256e56341b952c9820a7d4f7ef
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://549fb11ddaa974f469a7be62067d96a09b566bf1558a1dac8db0451bc47470fd
          exitCode: 255
          finishedAt: "2025-03-09T20:21:08Z"
          reason: Unknown
          startedAt: "2025-02-08T22:04:46Z"
      name: lb-tcp-443
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:13Z"
    - containerID: containerd://32536b82096c87b3cd4bcd273f55c6fa23868006e79f61890ee824460c288305
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://b775d356dfb8b7769dd7c2cc25fd9191b7897a2295cb0d1a9078bbcc0a86b5d3
          exitCode: 255
          finishedAt: "2025-03-09T20:21:08Z"
          reason: Unknown
          startedAt: "2025-02-08T22:04:46Z"
      name: lb-tcp-80
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:12Z"
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.214
    podIPs:
    - ip: 10.42.0.214
    qosClass: BestEffort
    startTime: "2025-02-08T22:04:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-08T22:07:22Z"
    generateName: svclb-traefik-32c60abe-
    labels:
      app: svclb-traefik-32c60abe
      controller-revision-hash: 74c7bfd55
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-32c60abe-fdsnp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-32c60abe
      uid: 9adcea51-40ac-44a8-9251-0a8abf5f9768
    resourceVersion: "11099861"
    uid: 56763177-924c-4185-999a-d44c1c6018ca
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.202.202
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.202.202
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:03:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:07:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:03:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-08T22:07:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f9dacfc73ecfae158df737311f5da0e46cfbd89c0570738d1aa598ac2d4e10ee
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://23e586dd2b176680ebc5418d9599e4e949579c5792dafcaec3bae8690ad2674f
          exitCode: 255
          finishedAt: "2025-03-16T20:03:15Z"
          reason: Unknown
          startedAt: "2025-03-16T20:00:05Z"
      name: lb-tcp-443
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:03:30Z"
    - containerID: containerd://d4554f279ee17d167a81442f2387ef0e1f689a0545c77b749c59d6e4d200c46e
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://2d82004b758544838d7c3475f4d7680e1d63bc6d2526cf4af1f6f48b0a79c086
          exitCode: 255
          finishedAt: "2025-03-16T20:03:15Z"
          reason: Unknown
          startedAt: "2025-03-16T20:00:04Z"
      name: lb-tcp-80
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:03:30Z"
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.2
    podIPs:
    - ip: 10.42.1.2
    qosClass: BestEffort
    startTime: "2025-02-08T22:07:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-09T15:15:18Z"
    generateName: svclb-traefik-32c60abe-
    labels:
      app: svclb-traefik-32c60abe
      controller-revision-hash: 74c7bfd55
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-32c60abe-ljhpx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-32c60abe
      uid: 9adcea51-40ac-44a8-9251-0a8abf5f9768
    resourceVersion: "9411073"
    uid: 49414634-99e9-4c5b-a522-439acdbec85f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-2
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.202.202
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.202.202
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-07T19:21:57Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-09T15:15:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:05:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-07T19:21:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-09T15:15:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0e9c1785e48ab9729111413affba2de0084b74d4c4b70d511d72734e22c5d209
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://dd19ae1a82a5254a883f847bac5ba4ac99b20ef1d92f46785910b21ebfd149b7
          exitCode: 255
          finishedAt: "2025-03-07T19:10:44Z"
          reason: Unknown
          startedAt: "2025-02-09T15:15:28Z"
      name: lb-tcp-443
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-07T19:21:50Z"
    - containerID: containerd://e1dba09a33bb00d82df86f0b6d3b8b2fe9e138131d65764545d14aae29b21354
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://f31f4d36bddec24d30d679f87794b51c470c51dc56c83b94ec388f5c17dc4928
          exitCode: 255
          finishedAt: "2025-03-07T19:10:44Z"
          reason: Unknown
          startedAt: "2025-02-09T15:15:28Z"
      name: lb-tcp-80
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-07T19:21:44Z"
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 10.42.2.207
    podIPs:
    - ip: 10.42.2.207
    qosClass: BestEffort
    startTime: "2025-02-09T15:15:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-11T20:16:10+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9100"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-26T19:21:45Z"
    generateName: traefik-686b6b64c4-
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 686b6b64c4
    name: traefik-686b6b64c4-sntpt
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: traefik-686b6b64c4
      uid: 521ebdac-67a2-469a-883a-f7c1098dd34b
    resourceVersion: "11100072"
    uid: 3907a616-4af9-47aa-afd1-593f75c9d62b
  spec:
    containers:
    - args:
      - --global.checknewversion
      - --global.sendanonymoususage
      - --entrypoints.metrics.address=:9100/tcp
      - --entrypoints.traefik.address=:9000/tcp
      - --entrypoints.web.address=:8000/tcp
      - --entrypoints.websecure.address=:8443/tcp
      - --api.dashboard=true
      - --ping=true
      - --metrics.prometheus=true
      - --metrics.prometheus.entrypoint=metrics
      - --providers.kubernetescrd
      - --providers.kubernetesingress
      - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
      - --entrypoints.websecure.http.tls=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/mirrored-library-traefik:2.11.18
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      name: traefik
      ports:
      - containerPort: 9100
        name: metrics
        protocol: TCP
      - containerPort: 9000
        name: traefik
        protocol: TCP
      - containerPort: 8000
        name: web
        protocol: TCP
      - containerPort: 8443
        name: websecure
        protocol: TCP
      readinessProbe:
        failureThreshold: 1
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jkzbx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 65532
      runAsNonRoot: true
      runAsUser: 65532
    serviceAccount: traefik
    serviceAccountName: traefik
    terminationGracePeriodSeconds: 60
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-jkzbx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:22:11Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7cdd46edb9fd7d8ac38b833bc5dd8e6822f25403e197e6250a7777c943977379
      image: docker.io/rancher/mirrored-library-traefik:2.11.18
      imageID: docker.io/rancher/mirrored-library-traefik@sha256:25df7bff0b414867f16a74c571c0dc84920404e45cc7780976cec77809230e09
      lastState: {}
      name: traefik
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:22:11Z"
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jkzbx
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.92
    podIPs:
    - ip: 10.42.1.92
    qosClass: BestEffort
    startTime: "2025-03-26T19:21:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-26T19:21:45Z"
    generateName: kubeshark-front-6dd457cff6-
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: front
      helm.sh/chart: kubeshark-52.4
      pod-template-hash: 6dd457cff6
    name: kubeshark-front-6dd457cff6-gj6cj
    namespace: kubeshark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kubeshark-front-6dd457cff6
      uid: bc73433d-cd02-4726-ac25-5e1201989d87
    resourceVersion: "11099996"
    uid: 748b140f-b7fd-48cd-96a9-857014df2fa4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
    containers:
    - env:
      - name: REACT_APP_AUTH_ENABLED
        value: "true"
      - name: REACT_APP_AUTH_TYPE
        value: oidc
      - name: REACT_APP_AUTH_SAML_IDP_METADATA_URL
        value: ' '
      - name: REACT_APP_TIMEZONE
        value: ' '
      - name: REACT_APP_SCRIPTING_DISABLED
        value: "false"
      - name: REACT_APP_TARGETED_PODS_UPDATE_DISABLED
        value: "false"
      - name: REACT_APP_PRESET_FILTERS_CHANGING_ENABLED
        value: "true"
      - name: REACT_APP_BPF_OVERRIDE_DISABLED
        value: "false"
      - name: REACT_APP_RECORDING_DISABLED
        value: "false"
      - name: REACT_APP_STOP_TRAFFIC_CAPTURING_DISABLED
        value: "false"
      - name: REACT_APP_CLOUD_LICENSE_ENABLED
        value: "true"
      - name: REACT_APP_SUPPORT_CHAT_ENABLED
        value: "true"
      - name: REACT_APP_DISSECTORS_UPDATING_ENABLED
        value: "true"
      - name: REACT_APP_SENTRY_ENABLED
        value: "false"
      - name: REACT_APP_SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/front:v52.4
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 1
        successThreshold: 1
        tcpSocket:
          port: 8080
        timeoutSeconds: 1
      name: kubeshark-front
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 1
        successThreshold: 1
        tcpSocket:
          port: 8080
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 750m
          memory: 1Gi
        requests:
          cpu: 50m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx/conf.d/default.conf
        name: nginx-config
        readOnly: true
        subPath: default.conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mg4r2
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubeshark-service-account
    serviceAccountName: kubeshark-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kubeshark-nginx-config-map
      name: nginx-config
    - name: kube-api-access-mg4r2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:22:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1959e41dc35be9c9d089d0baf2365cb60e472563cc2b02728bf5097bc5686a00
      image: docker.io/kubeshark/front:v52.4
      imageID: docker.io/kubeshark/front@sha256:f0be3ba64600bac80584c482a4d85e5f8074a719be6406b9f797d7837e987668
      lastState: {}
      name: kubeshark-front
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:22:07Z"
      volumeMounts:
      - mountPath: /etc/nginx/conf.d/default.conf
        name: nginx-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mg4r2
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.93
    podIPs:
    - ip: 10.42.1.93
    qosClass: Burstable
    startTime: "2025-03-26T19:21:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-26T19:21:45Z"
    generateName: kubeshark-hub-7c9f6c7f7f-
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: hub
      helm.sh/chart: kubeshark-52.4
      pod-template-hash: 7c9f6c7f7f
    name: kubeshark-hub-7c9f6c7f7f-pqr9x
    namespace: kubeshark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kubeshark-hub-7c9f6c7f7f
      uid: 99a139fe-81d7-4fc6-942f-bb0f3074ad01
    resourceVersion: "11100106"
    uid: 661fa106-187a-41c0-be56-f04a1ec3339c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
    containers:
    - command:
      - ./hub
      - -port
      - "8080"
      - -loglevel
      - warning
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      - name: KUBESHARK_CLOUD_API_URL
        value: https://api.kubeshark.co
      - name: PROFILING_ENABLED
        value: "false"
      image: docker.io/kubeshark/hub:v52.4
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8080
        timeoutSeconds: 1
      name: hub
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8080
        timeoutSeconds: 1
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/saml/x509
        name: saml-x509-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xdq4b
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubeshark-service-account
    serviceAccountName: kubeshark-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: saml-x509-volume
      projected:
        defaultMode: 420
        sources:
        - secret:
            items:
            - key: AUTH_SAML_X509_CRT
              path: kubeshark.crt
            name: kubeshark-saml-x509-crt-secret
        - secret:
            items:
            - key: AUTH_SAML_X509_KEY
              path: kubeshark.key
            name: kubeshark-saml-x509-key-secret
    - name: kube-api-access-xdq4b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:22:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6b5cd9462d9b0a8f26d4526d556959062eb36cc15b904ec2a6879ed9b6f499f8
      image: docker.io/kubeshark/hub:v52.4
      imageID: docker.io/kubeshark/hub@sha256:b400d29d9b449b921c8f99e754e98d6ae1db878d690c1a34271a9cbfd51c11e4
      lastState: {}
      name: hub
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:22:03Z"
      volumeMounts:
      - mountPath: /etc/saml/x509
        name: saml-x509-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xdq4b
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.91
    podIPs:
    - ip: 10.42.1.91
    qosClass: Burstable
    startTime: "2025-03-26T19:21:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-03-02T16:29:14+01:00"
    creationTimestamp: "2025-03-02T15:30:07Z"
    generateName: kubeshark-worker-daemon-set-
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: worker
      controller-revision-hash: 6d6cc5675d
      helm.sh/chart: kubeshark-52.4
      pod-template-generation: "2"
    name: kubeshark-worker-daemon-set-qcfkf
    namespace: kubeshark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kubeshark-worker-daemon-set
      uid: a091a521-aef5-450a-9cf8-31e95d6836aa
    resourceVersion: "10640722"
    uid: 43bfa6d4-743a-403d-bc1b-031043b59833
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-m-1
    containers:
    - command:
      - ./worker
      - -i
      - any
      - -port
      - "48999"
      - -metrics-port
      - "49100"
      - -packet-capture
      - best
      - -loglevel
      - warning
      - -servicemesh
      - -procfs
      - /hostproc
      - -resolution-strategy
      - auto
      - -staletimeout
      - "30"
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: TCP_STREAM_CHANNEL_TIMEOUT_MS
        value: "10000"
      - name: TCP_STREAM_CHANNEL_TIMEOUT_SHOW
        value: "false"
      - name: KUBESHARK_CLOUD_API_URL
        value: https://api.kubeshark.co
      - name: PROFILING_ENABLED
        value: "false"
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/worker:v52.4
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 48999
        timeoutSeconds: 1
      name: sniffer
      ports:
      - containerPort: 49100
        hostPort: 49100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 48999
        timeoutSeconds: 1
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
      - mountPath: /sys
        mountPropagation: HostToContainer
        name: sys
        readOnly: true
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dz64g
        readOnly: true
    - command:
      - ./tracer
      - -procfs
      - /hostproc
      - -disable-tls-log
      - -loglevel
      - warning
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: PROFILING_ENABLED
        value: "false"
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/worker:v52.4
      imagePullPolicy: Always
      name: tracer
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
      - mountPath: /sys
        mountPropagation: HostToContainer
        name: sys
        readOnly: true
      - mountPath: /app/data
        name: data
      - mountPath: /etc/os-release
        name: os-release
        readOnly: true
      - mountPath: /hostroot
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dz64g
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubeshark-service-account
    serviceAccountName: kubeshark-service-account
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/os-release
        type: ""
      name: os-release
    - hostPath:
        path: /
        type: ""
      name: root
    - emptyDir:
        sizeLimit: 5000Mi
      name: data
    - name: kube-api-access-dz64g
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:27Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T15:30:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T15:30:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://741c890a26f7ce703eca2034d9dd56e1cddf64e298d2a8bc943faac128200282
      image: docker.io/kubeshark/worker:v52.4
      imageID: docker.io/kubeshark/worker@sha256:8c770ec7aab03f7c2875c1d8fca93ccbd5b67e378829ed44fabef0b1e328e10d
      lastState:
        terminated:
          containerID: containerd://98446e85a330867fdec091d9d63071b52d9ca70f7d25e18e7c58395bf851c9eb
          exitCode: 2
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Error
          startedAt: "2025-03-02T15:30:08Z"
      name: sniffer
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:23Z"
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dz64g
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://05673f480a8ad3ac89d0adaa5fc643e5f484c859a52fd2af91ca789443442968
      image: docker.io/kubeshark/worker:v52.4
      imageID: docker.io/kubeshark/worker@sha256:8c770ec7aab03f7c2875c1d8fca93ccbd5b67e378829ed44fabef0b1e328e10d
      lastState:
        terminated:
          containerID: containerd://d878f0ce9999d868d6f3107ef673ed4b975f7061cfd90770f92e4242e3381f9a
          exitCode: 2
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Error
          startedAt: "2025-03-02T15:30:09Z"
      name: tracer
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:25Z"
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /app/data
        name: data
      - mountPath: /etc/os-release
        name: os-release
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /hostroot
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dz64g
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 192.168.50.75
    podIPs:
    - ip: 192.168.50.75
    qosClass: Burstable
    startTime: "2025-03-02T15:30:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-03-02T16:29:14+01:00"
    creationTimestamp: "2025-03-16T14:24:16Z"
    generateName: kubeshark-worker-daemon-set-
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: worker
      controller-revision-hash: 6d6cc5675d
      helm.sh/chart: kubeshark-52.4
      pod-template-generation: "2"
    name: kubeshark-worker-daemon-set-qn9gx
    namespace: kubeshark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kubeshark-worker-daemon-set
      uid: a091a521-aef5-450a-9cf8-31e95d6836aa
    resourceVersion: "11084783"
    uid: e6c876a1-0843-4577-8015-2cc3043ae3c0
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-3
    containers:
    - command:
      - ./worker
      - -i
      - any
      - -port
      - "48999"
      - -metrics-port
      - "49100"
      - -packet-capture
      - best
      - -loglevel
      - warning
      - -servicemesh
      - -procfs
      - /hostproc
      - -resolution-strategy
      - auto
      - -staletimeout
      - "30"
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: TCP_STREAM_CHANNEL_TIMEOUT_MS
        value: "10000"
      - name: TCP_STREAM_CHANNEL_TIMEOUT_SHOW
        value: "false"
      - name: KUBESHARK_CLOUD_API_URL
        value: https://api.kubeshark.co
      - name: PROFILING_ENABLED
        value: "false"
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/worker:v52.4
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 48999
        timeoutSeconds: 1
      name: sniffer
      ports:
      - containerPort: 49100
        hostPort: 49100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 48999
        timeoutSeconds: 1
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
      - mountPath: /sys
        mountPropagation: HostToContainer
        name: sys
        readOnly: true
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9vkd
        readOnly: true
    - command:
      - ./tracer
      - -procfs
      - /hostproc
      - -disable-tls-log
      - -loglevel
      - warning
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: PROFILING_ENABLED
        value: "false"
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/worker:v52.4
      imagePullPolicy: Always
      name: tracer
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
      - mountPath: /sys
        mountPropagation: HostToContainer
        name: sys
        readOnly: true
      - mountPath: /app/data
        name: data
      - mountPath: /etc/os-release
        name: os-release
        readOnly: true
      - mountPath: /hostroot
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9vkd
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubeshark-service-account
    serviceAccountName: kubeshark-service-account
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/os-release
        type: ""
      name: os-release
    - hostPath:
        path: /
        type: ""
      name: root
    - emptyDir:
        sizeLimit: 5000Mi
      name: data
    - name: kube-api-access-b9vkd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T14:24:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T14:24:17Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1561b9bf07f2d97f1fa408e9838479a7b7e2c139fc04582ff5a990be550579c7
      image: docker.io/kubeshark/worker:v52.4
      imageID: docker.io/kubeshark/worker@sha256:1e7e956ae7ba42aed696be3716fc6a53d5bdbc69b97e2fc90a1262a9e22fa0c3
      lastState:
        terminated:
          containerID: containerd://94372feb653bab6c7542893dd17d470aa9ba6eaf9081fb22930ba63b8734faf3
          exitCode: 2
          finishedAt: "2025-03-26T19:25:05Z"
          reason: Error
          startedAt: "2025-03-26T19:24:29Z"
      name: sniffer
      ready: true
      restartCount: 35
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:25:06Z"
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9vkd
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://bd005305e64cdaf25d3a6d0c8233959b68dc7d83fac1071f7fc75ed4e7a374de
      image: docker.io/kubeshark/worker:v52.4
      imageID: docker.io/kubeshark/worker@sha256:1e7e956ae7ba42aed696be3716fc6a53d5bdbc69b97e2fc90a1262a9e22fa0c3
      lastState:
        terminated:
          containerID: containerd://f76365c82a7f173f270ecaea3d9a0473bd5184f0e48d5114ccdbbecfb6e1b2a4
          exitCode: 255
          finishedAt: "2025-03-26T19:24:26Z"
          reason: Unknown
          startedAt: "2025-03-16T14:24:27Z"
      name: tracer
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:24:30Z"
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /app/data
        name: data
      - mountPath: /etc/os-release
        name: os-release
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /hostroot
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9vkd
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 192.168.50.108
    podIPs:
    - ip: 192.168.50.108
    qosClass: Burstable
    startTime: "2025-03-16T14:24:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-03-02T16:29:14+01:00"
    creationTimestamp: "2025-03-02T15:29:14Z"
    generateName: kubeshark-worker-daemon-set-
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: worker
      controller-revision-hash: 6d6cc5675d
      helm.sh/chart: kubeshark-52.4
      pod-template-generation: "2"
    name: kubeshark-worker-daemon-set-v2tfc
    namespace: kubeshark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kubeshark-worker-daemon-set
      uid: a091a521-aef5-450a-9cf8-31e95d6836aa
    resourceVersion: "11100129"
    uid: 68ec0681-acc4-4edd-b2a8-651af9ae0073
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-1
    containers:
    - command:
      - ./worker
      - -i
      - any
      - -port
      - "48999"
      - -metrics-port
      - "49100"
      - -packet-capture
      - best
      - -loglevel
      - warning
      - -servicemesh
      - -procfs
      - /hostproc
      - -resolution-strategy
      - auto
      - -staletimeout
      - "30"
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: TCP_STREAM_CHANNEL_TIMEOUT_MS
        value: "10000"
      - name: TCP_STREAM_CHANNEL_TIMEOUT_SHOW
        value: "false"
      - name: KUBESHARK_CLOUD_API_URL
        value: https://api.kubeshark.co
      - name: PROFILING_ENABLED
        value: "false"
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/worker:v52.4
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 48999
        timeoutSeconds: 1
      name: sniffer
      ports:
      - containerPort: 49100
        hostPort: 49100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 48999
        timeoutSeconds: 1
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
      - mountPath: /sys
        mountPropagation: HostToContainer
        name: sys
        readOnly: true
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mr5rh
        readOnly: true
    - command:
      - ./tracer
      - -procfs
      - /hostproc
      - -disable-tls-log
      - -loglevel
      - warning
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: PROFILING_ENABLED
        value: "false"
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/worker:v52.4
      imagePullPolicy: Always
      name: tracer
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
      - mountPath: /sys
        mountPropagation: HostToContainer
        name: sys
        readOnly: true
      - mountPath: /app/data
        name: data
      - mountPath: /etc/os-release
        name: os-release
        readOnly: true
      - mountPath: /hostroot
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mr5rh
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubeshark-service-account
    serviceAccountName: kubeshark-service-account
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/os-release
        type: ""
      name: os-release
    - hostPath:
        path: /
        type: ""
      name: root
    - emptyDir:
        sizeLimit: 5000Mi
      name: data
    - name: kube-api-access-mr5rh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:03:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T15:29:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T15:29:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://59dc230caf90c866382b9f0c175971d2cc3c39fad31e32991344cb21b2b6c505
      image: docker.io/kubeshark/worker:v52.4
      imageID: docker.io/kubeshark/worker@sha256:8c770ec7aab03f7c2875c1d8fca93ccbd5b67e378829ed44fabef0b1e328e10d
      lastState:
        terminated:
          containerID: containerd://fefbb623060cbfaf8f94f76d22f38c5d8d2f81eae8c251b2141c55fcad4c0253
          exitCode: 255
          finishedAt: "2025-03-16T20:03:15Z"
          reason: Unknown
          startedAt: "2025-03-16T20:00:12Z"
      name: sniffer
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:03:32Z"
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mr5rh
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://ab2b8ccff73ba4703a635c0b50f3b10dd69bce6553991e8562a694da557c43ba
      image: docker.io/kubeshark/worker:v52.4
      imageID: docker.io/kubeshark/worker@sha256:8c770ec7aab03f7c2875c1d8fca93ccbd5b67e378829ed44fabef0b1e328e10d
      lastState:
        terminated:
          containerID: containerd://46797377838ad0b04bad85764dd4fb50b8c88d749e2b679b3b901f4dee293234
          exitCode: 255
          finishedAt: "2025-03-16T20:03:15Z"
          reason: Unknown
          startedAt: "2025-03-16T20:00:13Z"
      name: tracer
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:03:34Z"
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /app/data
        name: data
      - mountPath: /etc/os-release
        name: os-release
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /hostroot
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mr5rh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 192.168.50.237
    podIPs:
    - ip: 192.168.50.237
    qosClass: Burstable
    startTime: "2025-03-02T15:29:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-03-02T16:29:14+01:00"
    creationTimestamp: "2025-03-02T15:29:34Z"
    generateName: kubeshark-worker-daemon-set-
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: worker
      controller-revision-hash: 6d6cc5675d
      helm.sh/chart: kubeshark-52.4
      pod-template-generation: "2"
    name: kubeshark-worker-daemon-set-w659b
    namespace: kubeshark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kubeshark-worker-daemon-set
      uid: a091a521-aef5-450a-9cf8-31e95d6836aa
    resourceVersion: "11084624"
    uid: a48511bf-e572-4fdb-83ec-11e709d5f902
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-2
    containers:
    - command:
      - ./worker
      - -i
      - any
      - -port
      - "48999"
      - -metrics-port
      - "49100"
      - -packet-capture
      - best
      - -loglevel
      - warning
      - -servicemesh
      - -procfs
      - /hostproc
      - -resolution-strategy
      - auto
      - -staletimeout
      - "30"
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: TCP_STREAM_CHANNEL_TIMEOUT_MS
        value: "10000"
      - name: TCP_STREAM_CHANNEL_TIMEOUT_SHOW
        value: "false"
      - name: KUBESHARK_CLOUD_API_URL
        value: https://api.kubeshark.co
      - name: PROFILING_ENABLED
        value: "false"
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/worker:v52.4
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 48999
        timeoutSeconds: 1
      name: sniffer
      ports:
      - containerPort: 49100
        hostPort: 49100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 48999
        timeoutSeconds: 1
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
      - mountPath: /sys
        mountPropagation: HostToContainer
        name: sys
        readOnly: true
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wcccq
        readOnly: true
    - command:
      - ./tracer
      - -procfs
      - /hostproc
      - -disable-tls-log
      - -loglevel
      - warning
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: PROFILING_ENABLED
        value: "false"
      - name: SENTRY_ENABLED
        value: "false"
      - name: SENTRY_ENVIRONMENT
        value: production
      image: docker.io/kubeshark/worker:v52.4
      imagePullPolicy: Always
      name: tracer
      resources:
        limits:
          memory: 5Gi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
      - mountPath: /sys
        mountPropagation: HostToContainer
        name: sys
        readOnly: true
      - mountPath: /app/data
        name: data
      - mountPath: /etc/os-release
        name: os-release
        readOnly: true
      - mountPath: /hostroot
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wcccq
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubeshark-service-account
    serviceAccountName: kubeshark-service-account
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/os-release
        type: ""
      name: os-release
    - hostPath:
        path: /
        type: ""
      name: root
    - emptyDir:
        sizeLimit: 5000Mi
      name: data
    - name: kube-api-access-wcccq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-07T19:23:48Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T15:29:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:05:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:05:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T15:29:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3321bc38b2140a941314706f055339000bed83d1eed78335a90ec31973ec32d6
      image: docker.io/kubeshark/worker:v52.4
      imageID: docker.io/kubeshark/worker@sha256:1e7e956ae7ba42aed696be3716fc6a53d5bdbc69b97e2fc90a1262a9e22fa0c3
      lastState:
        terminated:
          containerID: containerd://d849576606eaacd838e11445badc5f65debdf9fdf7c435e47c757626f9cb89c0
          exitCode: 2
          finishedAt: "2025-03-27T03:55:06Z"
          reason: Error
          startedAt: "2025-03-27T02:24:58Z"
      name: sniffer
      ready: true
      restartCount: 8
      started: true
      state:
        running:
          startedAt: "2025-03-27T03:57:51Z"
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wcccq
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://4e016d0d65c2576ee053c579cb4de105dadb172a8526a9fd8e1e73fe13ae00b3
      image: sha256:3619a702044c5388cb9bdbd7ace7d7ab22713b3ed4c06e641b5ab846c617eaad
      imageID: docker.io/kubeshark/worker@sha256:8c770ec7aab03f7c2875c1d8fca93ccbd5b67e378829ed44fabef0b1e328e10d
      lastState:
        terminated:
          containerID: containerd://be1e695b2b64c083a9461204cbac6d8f4b44fd0abb63c81030b4f736b2f8b065
          exitCode: 1
          finishedAt: "2025-03-07T19:25:47Z"
          reason: Error
          startedAt: "2025-03-07T19:25:36Z"
      name: tracer
      ready: true
      restartCount: 4
      started: true
      state:
        running:
          startedAt: "2025-03-07T19:26:49Z"
      volumeMounts:
      - mountPath: /hostproc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /app/data
        name: data
      - mountPath: /etc/os-release
        name: os-release
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /hostroot
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wcccq
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 192.168.50.115
    podIPs:
    - ip: 192.168.50.115
    qosClass: Burstable
    startTime: "2025-03-02T15:29:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: f7c516d64b976f04d4c31a243b6c2091ced173f942f6315d3ee2ba37ca7c0bd9
      prometheus.io/port: http-metrics
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-30T10:32:05Z"
    generateName: loki-
    labels:
      app: loki
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: loki-845bb44c7d
      name: loki
      release: loki
      statefulset.kubernetes.io/pod-name: loki-0
    name: loki-0
    namespace: loki
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki
      uid: f291132c-427c-432b-ab51-c8b0a33ef2e4
    resourceVersion: "11084823"
    uid: f82ae7af-b721-47e5-82a4-a32753d73799
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: NotIn
              values:
              - "true"
            - key: node-role.kubernetes.io/master
              operator: NotIn
              values:
              - "true"
          weight: 100
    containers:
    - args:
      - -config.file=/etc/loki/loki.yaml
      image: grafana/loki:2.6.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: memberlist-port
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xmzsq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-0
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-headless
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-loki-0
    - emptyDir: {}
      name: tmp
    - name: config
      secret:
        defaultMode: 420
        secretName: loki
    - name: kube-api-access-xmzsq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d3b0657af4224c0b094db61e7e2b0a3f1d074ff20c1f4fa52fa92580f9214d6e
      image: docker.io/grafana/loki:2.6.1
      imageID: docker.io/grafana/loki@sha256:1ee60f980950b00e505bd564b40f720132a0653b110e993043bb5940673d060a
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-30T10:32:09Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xmzsq
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 10.42.3.68
    podIPs:
    - ip: 10.42.3.68
    qosClass: Burstable
    startTime: "2025-03-30T10:32:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4e697a34db1115f5776e3d42883d968d223d8474b8b0a42d933463a80606d4b5
    creationTimestamp: "2025-03-30T10:32:05Z"
    generateName: loki-promtail-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: 5dfc5784d6
      pod-template-generation: "1"
    name: loki-promtail-8bfqp
    namespace: loki
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: dc0f1295-2e33-40e9-8428-98fc9db0fd47
    resourceVersion: "10640729"
    uid: 194a1062-ea8b-4d9f-8bec-7b020026ce7a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-m-1
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 750m
          memory: 512Mi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bvsq9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-bvsq9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://726004660c33bc145b4e18791cabfae22f5717862b5499e4ee24e435f8ee8731
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-30T10:32:06Z"
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bvsq9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.187
    podIPs:
    - ip: 10.42.0.187
    qosClass: Burstable
    startTime: "2025-03-30T10:32:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4e697a34db1115f5776e3d42883d968d223d8474b8b0a42d933463a80606d4b5
    creationTimestamp: "2025-03-30T10:32:05Z"
    generateName: loki-promtail-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: 5dfc5784d6
      pod-template-generation: "1"
    name: loki-promtail-f22jb
    namespace: loki
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: dc0f1295-2e33-40e9-8428-98fc9db0fd47
    resourceVersion: "11084806"
    uid: 9e411c04-f481-49a7-878b-934f0f00c863
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-3
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 750m
          memory: 512Mi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5hlln
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-5hlln
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://aacd003523faf98d374557d176ee73e67c6c20e34a23fe449d86fc9430c1b4b0
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-30T10:32:06Z"
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5hlln
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 10.42.3.67
    podIPs:
    - ip: 10.42.3.67
    qosClass: Burstable
    startTime: "2025-03-30T10:32:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4e697a34db1115f5776e3d42883d968d223d8474b8b0a42d933463a80606d4b5
    creationTimestamp: "2025-03-30T10:32:05Z"
    generateName: loki-promtail-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: 5dfc5784d6
      pod-template-generation: "1"
    name: loki-promtail-mqpv4
    namespace: loki
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: dc0f1295-2e33-40e9-8428-98fc9db0fd47
    resourceVersion: "11100113"
    uid: 18009fc3-2143-42eb-81b4-bd70870baba7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-1
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 750m
          memory: 512Mi
        requests:
          cpu: 250m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q2xgg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-q2xgg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T10:32:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7d5e92c0bc97a295c880e19cc8385111093b34a4735543aeed1001256c64168a
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-30T10:32:07Z"
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q2xgg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.164
    podIPs:
    - ip: 10.42.1.164
    qosClass: Burstable
    startTime: "2025-03-30T10:32:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
      kubectl.kubernetes.io/restartedAt: "2025-04-03T22:46:15+02:00"
    creationTimestamp: "2025-04-04T19:25:14Z"
    generateName: alertmanager-kube-prometheus-stack-alertmanager-
    labels:
      alertmanager: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: kube-prometheus-stack-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.26.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-kube-prometheus-stack-alertmanager-959d94b5b
      statefulset.kubernetes.io/pod-name: alertmanager-kube-prometheus-stack-alertmanager-0
    name: alertmanager-kube-prometheus-stack-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-kube-prometheus-stack-alertmanager
      uid: 30417bf4-383e-43b3-81e6-e4b604b15655
    resourceVersion: "11084795"
    uid: 5bcdb3f0-30cf-4c2f-a29a-2704896c9516
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: NotIn
              values:
              - "true"
            - key: node-role.kubernetes.io/master
              operator: NotIn
              values:
              - "true"
          weight: 100
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://alertmanager.local
      - --web.route-prefix=/
      - --cluster.label=monitoring/kube-prometheus-stack-alertmanager
      - --cluster.peer=alertmanager-kube-prometheus-stack-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.26.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-kube-prometheus-stack-alertmanager-db
        subPath: alertmanager-db
      - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
        name: secret-alertmanager-notification-secret
        readOnly: true
      - mountPath: /etc/alertmanager/configmaps/alertmanager-config
        name: configmap-alertmanager-config
        readOnly: true
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rlrc
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      - --watched-dir=/etc/alertmanager/secrets/alertmanager-notification-secret
      - --watched-dir=/etc/alertmanager/configmaps/alertmanager-config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
        name: secret-alertmanager-notification-secret
        readOnly: true
      - mountPath: /etc/alertmanager/configmaps/alertmanager-config
        name: configmap-alertmanager-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rlrc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-kube-prometheus-stack-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      - --watched-dir=/etc/alertmanager/secrets/alertmanager-notification-secret
      - --watched-dir=/etc/alertmanager/configmaps/alertmanager-config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
        name: secret-alertmanager-notification-secret
        readOnly: true
      - mountPath: /etc/alertmanager/configmaps/alertmanager-config
        name: configmap-alertmanager-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rlrc
        readOnly: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prometheus-stack-alertmanager
    serviceAccountName: kube-prometheus-stack-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: alertmanager-kube-prometheus-stack-alertmanager-db
      persistentVolumeClaim:
        claimName: alertmanager-kube-prometheus-stack-alertmanager-db-alertmanager-kube-prometheus-stack-alertmanager-0
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-kube-prometheus-stack-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-kube-prometheus-stack-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: secret-alertmanager-notification-secret
      secret:
        defaultMode: 420
        secretName: alertmanager-notification-secret
    - configMap:
        defaultMode: 420
        name: alertmanager-config
      name: configmap-alertmanager-config
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-kube-prometheus-stack-alertmanager-web-config
    - name: kube-api-access-6rlrc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T19:25:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T19:25:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T19:25:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://85079f78177971fdf839f5552294ef27c6157fa1ded3f86e1017fcc1d5863e33
      image: quay.io/prometheus/alertmanager:v0.26.0
      imageID: quay.io/prometheus/alertmanager@sha256:361db356b33041437517f1cd298462055580585f26555c317df1a3caf2868552
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-04T19:25:32Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /alertmanager
        name: alertmanager-kube-prometheus-stack-alertmanager-db
      - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
        name: secret-alertmanager-notification-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/configmaps/alertmanager-config
        name: configmap-alertmanager-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rlrc
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://3ce7f64f459688c0680f3000385daf9a6ddfaeb327dfed10cc0149c44fba7d60
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-04T19:25:32Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
        name: secret-alertmanager-notification-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/configmaps/alertmanager-config
        name: configmap-alertmanager-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rlrc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    initContainerStatuses:
    - containerID: containerd://97b09ce14868df7d3484b49896beb60106a034520585018642345cc2207790a9
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://97b09ce14868df7d3484b49896beb60106a034520585018642345cc2207790a9
          exitCode: 0
          finishedAt: "2025-04-04T19:25:25Z"
          reason: Completed
          startedAt: "2025-04-04T19:25:25Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
        name: secret-alertmanager-notification-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/configmaps/alertmanager-config
        name: configmap-alertmanager-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6rlrc
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.3.80
    podIPs:
    - ip: 10.42.3.80
    qosClass: Burstable
    startTime: "2025-04-04T19:25:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: c7b16e62bdf0a9cd5b1d531b40884dc4e40e401c55b4ace6e201920579bc6c38
      checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
      checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
      kubectl.kubernetes.io/default-container: grafana
      kubectl.kubernetes.io/restartedAt: "2025-03-21T22:54:39+01:00"
    creationTimestamp: "2025-04-04T19:28:33Z"
    generateName: kube-prometheus-stack-grafana-56cc479d4b-
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 56cc479d4b
    name: kube-prometheus-stack-grafana-56cc479d4b-g8qxb
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-prometheus-stack-grafana-56cc479d4b
      uid: fb120f87-dd99-47c2-b0d6-6ed1c54c9afc
    resourceVersion: "11100115"
    uid: f3e5d12e-6e57-43e4-b6bc-4b7e2cc413fd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - k3s-w-3
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: monitoring
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prometheus-stack-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prometheus-stack-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.25.2
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2zgj
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prometheus-stack-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prometheus-stack-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.25.2
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2zgj
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prometheus-stack-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prometheus-stack-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:10.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 800m
          memory: 256Mi
        requests:
          cpu: 300m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2zgj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - chown
      - -R
      - 472:472
      - /var/lib/grafana
      image: docker.io/library/busybox:1.31.1
      imagePullPolicy: IfNotPresent
      name: init-chown-data
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2zgj
        readOnly: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: kube-prometheus-stack-grafana
    serviceAccountName: kube-prometheus-stack-grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-prometheus-stack-grafana
      name: config
    - name: storage
      persistentVolumeClaim:
        claimName: kube-prometheus-stack-grafana
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: kube-prometheus-stack-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-d2zgj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T19:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T19:28:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T19:28:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0eaa6112807e71736a1bfe68533c8299379b3d8ef5c3f40c33bc0956786edf69
      image: docker.io/grafana/grafana:10.2.2
      imageID: docker.io/grafana/grafana@sha256:e3e9c2b5776fe3657f4954dfa91579224f98a0316f51d431989b15425e95530f
      lastState:
        terminated:
          containerID: containerd://5d07a43e27281fe3e36bcd35532115eb82f049e6c30c8e44c6f0e8e9191f99c4
          exitCode: 137
          finishedAt: "2025-04-05T12:03:29Z"
          reason: Error
          startedAt: "2025-04-04T19:28:34Z"
      name: grafana
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-04-05T12:03:29Z"
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2zgj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://d71762f3ff6b0319d95da0dd1f45ec3001811488bde315b1f4bfd5740a2b6c5c
      image: quay.io/kiwigrid/k8s-sidecar:1.25.2
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:cb4c638ffb1fa1eb49678e0f0423564b39254533f63f4ca6a6c24260472e0c4f
      lastState: {}
      name: grafana-sc-dashboard
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-04T19:28:34Z"
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2zgj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://e4101c6804b3c89694ab43f02e0371e070ac952c8aac019dfd6f18e2f248ca31
      image: quay.io/kiwigrid/k8s-sidecar:1.25.2
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:cb4c638ffb1fa1eb49678e0f0423564b39254533f63f4ca6a6c24260472e0c4f
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-04T19:28:34Z"
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2zgj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    initContainerStatuses:
    - containerID: containerd://53cc5680387ee0a0140290aa0204c3bb5da56abd550d48be1964a47e27f833a9
      image: docker.io/library/busybox:1.31.1
      imageID: docker.io/library/busybox@sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209
      lastState: {}
      name: init-chown-data
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://53cc5680387ee0a0140290aa0204c3bb5da56abd550d48be1964a47e27f833a9
          exitCode: 0
          finishedAt: "2025-04-04T19:28:34Z"
          reason: Completed
          startedAt: "2025-04-04T19:28:34Z"
      volumeMounts:
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2zgj
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.3.81
    podIPs:
    - ip: 10.42.3.81
    qosClass: Burstable
    startTime: "2025-04-04T19:28:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-02T19:38:10Z"
    generateName: kube-prometheus-stack-kube-state-metrics-668b4bf9ff-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      pod-template-hash: 668b4bf9ff
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics-668b4bf9ff-dqtsx
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-prometheus-stack-kube-state-metrics-668b4bf9ff
      uid: ca01b861-68ad-48e0-bb76-49ffe60ea181
    resourceVersion: "11084644"
    uid: b345fe92-f311-4385-8a1a-6c003051053f
  spec:
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 300m
          memory: 128Mi
        requests:
          cpu: 100m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zw574
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prometheus-stack-kube-state-metrics
    serviceAccountName: kube-prometheus-stack-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-zw574
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T19:38:12Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T19:38:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:05:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:05:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T19:38:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://30035bbf8eb71718bf8bf2504b298bac37c12fd96ac55ff7848e62365a8bcb3a
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:af8220f534938de121a694cb7314313a6195c9d494fc30bfa6885b08a276bb82
      lastState: {}
      name: kube-state-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T19:38:12Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zw574
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 10.42.2.9
    podIPs:
    - ip: 10.42.2.9
    qosClass: Burstable
    startTime: "2025-04-02T19:38:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-02-23T21:46:58Z"
    generateName: kube-prometheus-stack-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      controller-revision-hash: 6bf6994887
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter-frcwb
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prometheus-stack-prometheus-node-exporter
      uid: 1e56b2b2-aac8-4ff1-be88-136c66dd2ed1
    resourceVersion: "11084636"
    uid: 65220254-5fc6-4350-a424-a4a6ea4f7450
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-2
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prometheus-stack-prometheus-node-exporter
    serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-07T19:21:46Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-23T21:47:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:05:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:05:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-23T21:47:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a07a6fc41f9483ca5d369fb6ee292b90485648385880a84110837d5b7fb4d74e
      image: quay.io/prometheus/node-exporter:v1.7.0
      imageID: quay.io/prometheus/node-exporter@sha256:4cb2b9019f1757be8482419002cb7afe028fdba35d47958829e4cfeaf6246d80
      lastState:
        terminated:
          containerID: containerd://b2d1af9d8d17a992642f7c3be3b37a6a47fabd6e7f0d67f79bb8c3ffe2c72ea6
          exitCode: 143
          finishedAt: "2025-03-27T03:55:05Z"
          reason: Error
          startedAt: "2025-03-27T02:24:55Z"
      name: node-exporter
      ready: true
      restartCount: 11
      started: true
      state:
        running:
          startedAt: "2025-03-27T03:56:10Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 192.168.50.115
    podIPs:
    - ip: 192.168.50.115
    qosClass: BestEffort
    startTime: "2025-02-23T21:47:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-02-23T21:46:55Z"
    generateName: kube-prometheus-stack-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      controller-revision-hash: 6bf6994887
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter-gzjlp
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prometheus-stack-prometheus-node-exporter
      uid: 1e56b2b2-aac8-4ff1-be88-136c66dd2ed1
    resourceVersion: "10640679"
    uid: d7481cb5-59de-40b8-86fe-c94af0bdd23a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-m-1
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prometheus-stack-prometheus-node-exporter
    serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-23T21:47:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-23T21:46:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ddece1d9603ba7eb963c005bf7289f6b55d84bb223a746cdaa99deb5b087d84c
      image: quay.io/prometheus/node-exporter:v1.7.0
      imageID: quay.io/prometheus/node-exporter@sha256:4cb2b9019f1757be8482419002cb7afe028fdba35d47958829e4cfeaf6246d80
      lastState:
        terminated:
          containerID: containerd://439136e6d06023ec932e93bcd43b4f277844f4b4646ac36e2541923b9258c929
          exitCode: 143
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Error
          startedAt: "2025-03-09T18:51:23Z"
      name: node-exporter
      ready: true
      restartCount: 3
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:16Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 192.168.50.75
    podIPs:
    - ip: 192.168.50.75
    qosClass: BestEffort
    startTime: "2025-02-23T21:47:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-03-16T14:24:16Z"
    generateName: kube-prometheus-stack-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      controller-revision-hash: 6bf6994887
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter-l2pfj
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prometheus-stack-prometheus-node-exporter
      uid: 1e56b2b2-aac8-4ff1-be88-136c66dd2ed1
    resourceVersion: "11084850"
    uid: 7f8328b7-d8e6-4e83-ac1b-36192b3be6a4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-3
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prometheus-stack-prometheus-node-exporter
    serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:29Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T14:24:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T14:24:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5a21d32304f5db71f7f663a68fa88dc8c362cec7a24e4538dc0122ad783ec70c
      image: quay.io/prometheus/node-exporter:v1.7.0
      imageID: quay.io/prometheus/node-exporter@sha256:4cb2b9019f1757be8482419002cb7afe028fdba35d47958829e4cfeaf6246d80
      lastState:
        terminated:
          containerID: containerd://cd94c0420f0afcab699f39e6f6dc41cdd1782889636452630eefa40b384cd348
          exitCode: 143
          finishedAt: "2025-03-26T19:25:23Z"
          reason: Error
          startedAt: "2025-03-26T19:24:53Z"
      name: node-exporter
      ready: true
      restartCount: 38
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:25:23Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 192.168.50.108
    podIPs:
    - ip: 192.168.50.108
    qosClass: BestEffort
    startTime: "2025-03-16T14:24:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-02-23T21:46:58Z"
    generateName: kube-prometheus-stack-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      controller-revision-hash: 6bf6994887
      helm.sh/chart: prometheus-node-exporter-4.24.0
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter-qzkh5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prometheus-stack-prometheus-node-exporter
      uid: 1e56b2b2-aac8-4ff1-be88-136c66dd2ed1
    resourceVersion: "11100060"
    uid: e97c643c-ae35-499c-8a16-b5e46da57ccc
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-1
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prometheus-stack-prometheus-node-exporter
    serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:03:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-23T21:47:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-23T21:47:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fc956fef98fd0e855515146c93b1391d31682cb8d633af7d90abf9574de72513
      image: quay.io/prometheus/node-exporter:v1.7.0
      imageID: quay.io/prometheus/node-exporter@sha256:4cb2b9019f1757be8482419002cb7afe028fdba35d47958829e4cfeaf6246d80
      lastState:
        terminated:
          containerID: containerd://275e83f7b02526d6bc5d5e72f87f517598440bb410bf11b8a84a16b8199b7b9f
          exitCode: 255
          finishedAt: "2025-03-16T20:03:15Z"
          reason: Unknown
          startedAt: "2025-03-16T20:00:04Z"
      name: node-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:03:30Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 192.168.50.237
    podIPs:
    - ip: 192.168.50.237
    qosClass: BestEffort
    startTime: "2025-02-23T21:47:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
      kubectl.kubernetes.io/restartedAt: "2025-04-05T16:21:53+02:00"
    creationTimestamp: "2025-04-05T14:21:56Z"
    generateName: prometheus-kube-prometheus-stack-prometheus-
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.48.1
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-kube-prometheus-stack-prometheus-78c697b68c
      operator.prometheus.io/name: kube-prometheus-stack-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: kube-prometheus-stack-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-kube-prometheus-stack-prometheus-0
    name: prometheus-kube-prometheus-stack-prometheus-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-kube-prometheus-stack-prometheus
      uid: e4ab85fa-5439-4bf7-818e-8ff5f02ba104
    resourceVersion: "11117773"
    uid: 96ab2197-d2d7-4651-8404-a3c9a00694ea
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: NotIn
              values:
              - "true"
            - key: node-role.kubernetes.io/master
              operator: NotIn
              values:
              - "true"
    automountServiceAccountToken: true
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=http://prometheus.local/
      - --web.route-prefix=/
      - --storage.tsdb.retention.time=15d
      - --storage.tsdb.retention.size=15GB
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v2.48.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 400m
          memory: 768Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-kube-prometheus-stack-prometheus-db
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvnxf
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvnxf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-kube-prometheus-stack-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvnxf
        readOnly: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prometheus-stack-prometheus
    serviceAccountName: kube-prometheus-stack-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: prometheus-kube-prometheus-stack-prometheus-db
      persistentVolumeClaim:
        claimName: prometheus-kube-prometheus-stack-prometheus-db-prometheus-kube-prometheus-stack-prometheus-0
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-kube-prometheus-stack-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-kube-prometheus-stack-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-kube-prometheus-stack-prometheus-web-config
    - name: kube-api-access-xvnxf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T14:22:02Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T14:22:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T14:21:56Z"
      message: 'containers with unready status: [prometheus]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T14:21:56Z"
      message: 'containers with unready status: [prometheus]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T14:21:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://df1d6b9b4ed7dc0159bd0a6d1e7b29605d7065080421834e86c6972a7f7dd198
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-05T14:22:03Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvnxf
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://3db0999f078f5f07b60765d42ae73300a4bb4221c85427d1aa7393bdbfd5bb5e
      image: quay.io/prometheus/prometheus:v2.48.1
      imageID: quay.io/prometheus/prometheus@sha256:a67e5e402ff5410b86ec48b39eab1a3c4df2a7e78a71bf025ec5e32e09090ad4
      lastState: {}
      name: prometheus
      ready: false
      restartCount: 0
      started: false
      state:
        running:
          startedAt: "2025-04-05T14:22:03Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /prometheus
        name: prometheus-kube-prometheus-stack-prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvnxf
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    initContainerStatuses:
    - containerID: containerd://a3a3622d303f7317b75e2848c81acec3933b19f468a80d9903942537d18f50cd
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a3a3622d303f7317b75e2848c81acec3933b19f468a80d9903942537d18f50cd
          exitCode: 0
          finishedAt: "2025-04-05T14:22:01Z"
          reason: Completed
          startedAt: "2025-04-05T14:22:01Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvnxf
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.1.184
    podIPs:
    - ip: 10.42.1.184
    qosClass: Burstable
    startTime: "2025-04-05T14:21:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-16T20:00:02Z"
    generateName: mosquitto-
    labels:
      app: mosquitto
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: mosquitto-5dbc9f7958
      statefulset.kubernetes.io/pod-name: mosquitto-0
    name: mosquitto-0
    namespace: mosquitto
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: mosquitto
      uid: 6170679b-cfcd-496c-8db5-9439f94955e1
    resourceVersion: "11099876"
    uid: 7921c6c8-2e5e-4fa3-b5fc-4396050bcc74
  spec:
    containers:
    - image: eclipse-mosquitto:latest
      imagePullPolicy: Always
      name: mosquitto-broker
      ports:
      - containerPort: 1883
        protocol: TCP
      resources:
        limits:
          cpu: 300m
          memory: 1Gi
        requests:
          cpu: 50m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mosquitto/config
        name: mosquitto-config
      - mountPath: /mosquitto/data
        name: mosquitto-data
      - mountPath: /mosquitto/log
        name: mosquitto-log
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b5ghr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: mosquitto-0
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: mosquitto
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: mosquitto-config
      name: mosquitto-config
    - name: mosquitto-data
      persistentVolumeClaim:
        claimName: mosquitto-data-claim
    - name: mosquitto-log
      persistentVolumeClaim:
        claimName: mosquitto-log-claim
    - name: kube-api-access-b5ghr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:05:14Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:00:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:05:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:00:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b45060bc740ec32a204b69bf1661279e9d3eb26d34239c66111f48cb3c8b31b5
      image: docker.io/library/eclipse-mosquitto:latest
      imageID: docker.io/library/eclipse-mosquitto@sha256:94f5a3d7deafa59fa3440d227ddad558f59d293c612138de841eec61bfa4d353
      lastState: {}
      name: mosquitto-broker
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:05:14Z"
      volumeMounts:
      - mountPath: /mosquitto/config
        name: mosquitto-config
      - mountPath: /mosquitto/data
        name: mosquitto-data
      - mountPath: /mosquitto/log
        name: mosquitto-log
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b5ghr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.8
    podIPs:
    - ip: 10.42.1.8
    qosClass: Burstable
    startTime: "2025-03-16T20:00:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-03T00:00:00Z"
    generateName: openhab-backup-29060640-
    labels:
      batch.kubernetes.io/controller-uid: 7cafa485-f819-4c78-9bda-400a566fea9d
      batch.kubernetes.io/job-name: openhab-backup-29060640
      controller-uid: 7cafa485-f819-4c78-9bda-400a566fea9d
      job-name: openhab-backup-29060640
    name: openhab-backup-29060640-cnc6v
    namespace: openhab
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: openhab-backup-29060640
      uid: 7cafa485-f819-4c78-9bda-400a566fea9d
    resourceVersion: "11099988"
    uid: 90298a47-990e-40fc-8037-e1d178633742
  spec:
    containers:
    - args:
      - |
        # Install required packages
        apk add --no-cache curl openssh-client sshpass && \
        # Install kubectl
        curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
        chmod +x ./kubectl && \
        mv ./kubectl /usr/local/bin/kubectl && \
        /backup-script.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: NAS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: nas-credentials
      image: alpine:3.18
      imagePullPolicy: IfNotPresent
      name: backup
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
        subPath: backup-script.sh
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wbhkb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: openhab-backup
    serviceAccountName: openhab-backup
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 511
        name: openhab-backup-script-mtkth67t5h
      name: backup-script
    - name: kube-api-access-wbhkb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T00:04:05Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T00:00:00Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T00:04:03Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T00:04:03Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-03T00:00:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d7e902fed7c5854e4482fe8e9013859c36d7dc6646c54c90d9dc1d8e0580372d
      image: docker.io/library/alpine:3.18
      imageID: docker.io/library/alpine@sha256:de0eb0b3f2a47ba1eb89389859a9bd88b28e82f5826b6969ad604979713c2d4f
      lastState: {}
      name: backup
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d7e902fed7c5854e4482fe8e9013859c36d7dc6646c54c90d9dc1d8e0580372d
          exitCode: 0
          finishedAt: "2025-04-03T00:04:03Z"
          reason: Completed
          startedAt: "2025-04-03T00:00:01Z"
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wbhkb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-04-03T00:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-04T00:00:00Z"
    generateName: openhab-backup-29062080-
    labels:
      batch.kubernetes.io/controller-uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
      batch.kubernetes.io/job-name: openhab-backup-29062080
      controller-uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
      job-name: openhab-backup-29062080
    name: openhab-backup-29062080-mws75
    namespace: openhab
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: openhab-backup-29062080
      uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
    resourceVersion: "11084772"
    uid: 6ce709c0-aebd-4430-9157-e70c75ff09ca
  spec:
    containers:
    - args:
      - |
        # Install required packages
        apk add --no-cache curl openssh-client sshpass && \
        # Install kubectl
        curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
        chmod +x ./kubectl && \
        mv ./kubectl /usr/local/bin/kubectl && \
        /backup-script.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: NAS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: nas-credentials
      image: alpine:3.18
      imagePullPolicy: IfNotPresent
      name: backup
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
        subPath: backup-script.sh
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h9vsc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: openhab-backup
    serviceAccountName: openhab-backup
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 511
        name: openhab-backup-script-mtkth67t5h
      name: backup-script
    - name: kube-api-access-h9vsc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T00:03:50Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T00:00:00Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T00:03:49Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T00:03:49Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-04T00:00:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e9d9b2a127060567e718c392c493b9763c37aaf0580376990a16b5cbba882471
      image: docker.io/library/alpine:3.18
      imageID: docker.io/library/alpine@sha256:de0eb0b3f2a47ba1eb89389859a9bd88b28e82f5826b6969ad604979713c2d4f
      lastState: {}
      name: backup
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e9d9b2a127060567e718c392c493b9763c37aaf0580376990a16b5cbba882471
          exitCode: 0
          finishedAt: "2025-04-04T00:03:48Z"
          reason: Completed
          startedAt: "2025-04-04T00:00:00Z"
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h9vsc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-04-04T00:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-05T00:00:00Z"
    generateName: openhab-backup-29063520-
    labels:
      batch.kubernetes.io/controller-uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
      batch.kubernetes.io/job-name: openhab-backup-29063520
      controller-uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
      job-name: openhab-backup-29063520
    name: openhab-backup-29063520-mrjrx
    namespace: openhab
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: openhab-backup-29063520
      uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
    resourceVersion: "11084776"
    uid: 406fcc3a-fd5b-424b-b33b-c1548a1bba02
  spec:
    containers:
    - args:
      - |
        # Install required packages
        apk add --no-cache curl openssh-client sshpass && \
        # Install kubectl
        curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
        chmod +x ./kubectl && \
        mv ./kubectl /usr/local/bin/kubectl && \
        /backup-script.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: NAS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: nas-credentials
      image: alpine:3.18
      imagePullPolicy: IfNotPresent
      name: backup
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
        subPath: backup-script.sh
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jdn2j
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: openhab-backup
    serviceAccountName: openhab-backup
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 511
        name: openhab-backup-script-mtkth67t5h
      name: backup-script
    - name: kube-api-access-jdn2j
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T00:03:09Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T00:00:00Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T00:03:08Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T00:03:08Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T00:00:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://26bad5014da120144cbe8366850552120c172b8611d5511e319a5a6db7c7f54f
      image: docker.io/library/alpine:3.18
      imageID: docker.io/library/alpine@sha256:de0eb0b3f2a47ba1eb89389859a9bd88b28e82f5826b6969ad604979713c2d4f
      lastState: {}
      name: backup
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://26bad5014da120144cbe8366850552120c172b8611d5511e319a5a6db7c7f54f
          exitCode: 0
          finishedAt: "2025-04-05T00:03:07Z"
          reason: Completed
          startedAt: "2025-04-05T00:00:00Z"
      volumeMounts:
      - mountPath: /backup-script.sh
        name: backup-script
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jdn2j
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-04-05T00:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/change-cause: Update to OpenHAB 4.3.4 using official image
    creationTimestamp: "2025-04-01T19:42:39Z"
    generateName: openhab-production-
    labels:
      app: openhab-production
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: openhab-production-557cbffd6f
      statefulset.kubernetes.io/pod-name: openhab-production-0
    name: openhab-production-0
    namespace: openhab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: openhab-production
      uid: 342c564c-1508-4982-8767-fccfeb62e1be
    resourceVersion: "11099906"
    uid: 877c9247-aeb5-48f8-a475-ed3e0fdc2932
  spec:
    containers:
    - env:
      - name: EXTRA_JAVA_OPTS
        value: -Xmx1500m -Duser.timezone=Europe/Stockholm
      image: openhab/openhab:4.3.4
      imagePullPolicy: Always
      name: openhab434
      ports:
      - containerPort: 8443
        name: https
        protocol: TCP
      - containerPort: 8101
        name: console
        protocol: TCP
      resources:
        limits:
          cpu: 2500m
          memory: 2000Mi
        requests:
          cpu: 800m
          memory: 1500Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/localtime
        name: etc-localtime
        readOnly: true
      - mountPath: /etc/timezone
        name: etc-timezone
        readOnly: true
      - mountPath: /openhab/conf
        name: openhab-conf
      - mountPath: /openhab/userdata
        name: openhab-userdata
      - mountPath: /openhab/addons
        name: openhab-addons
      - mountPath: /openhab/.karaf
        name: openhab-karaf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lw4t5
        readOnly: true
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: grafana/promtail:2.9.2
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 9080
        name: http-metrics
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: promtail-config
      - mountPath: /logs
        name: openhab-userdata
        readOnly: true
        subPath: logs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lw4t5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: openhab-production-0
    nodeName: k3s-w-1
    nodeSelector:
      kubernetes.io/hostname: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: openhab-production
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /usr/share/zoneinfo/Europe/Stockholm
        type: ""
      name: etc-localtime
    - hostPath:
        path: /etc/timezone
        type: ""
      name: etc-timezone
    - name: openhab-conf
      persistentVolumeClaim:
        claimName: openhab-production-conf-claim
    - name: openhab-userdata
      persistentVolumeClaim:
        claimName: openhab-production-userdata-claim
    - name: openhab-addons
      persistentVolumeClaim:
        claimName: openhab-production-addons-claim
    - name: openhab-karaf
      persistentVolumeClaim:
        claimName: openhab-production-karaf-claim
    - configMap:
        defaultMode: 420
        name: promtail-sidecar-config
      name: promtail-config
    - name: kube-api-access-lw4t5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-01T19:42:48Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-01T19:42:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-01T19:42:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-01T19:42:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cffd39c733e45dd0d6fda2b5c13e121f90e2e5b391b645dcf916e248d328fa40
      image: docker.io/openhab/openhab:4.3.4
      imageID: docker.io/openhab/openhab@sha256:18b88d08a18ea7fbb1c592d21553f614dbb1e40306a1646dfd4af7eb125d27d4
      lastState: {}
      name: openhab434
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-01T19:42:48Z"
      volumeMounts:
      - mountPath: /etc/localtime
        name: etc-localtime
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/timezone
        name: etc-timezone
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /openhab/conf
        name: openhab-conf
      - mountPath: /openhab/userdata
        name: openhab-userdata
      - mountPath: /openhab/addons
        name: openhab-addons
      - mountPath: /openhab/.karaf
        name: openhab-karaf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lw4t5
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://b0abd46e3f08b5492ae502467ade4614932f1911c571caad6e3af44668dc5730
      image: docker.io/grafana/promtail:2.9.2
      imageID: docker.io/grafana/promtail@sha256:3ec78a089e5cb5173f5348ee29de4d3cdab29493776ac5704db8727fa0cda60f
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-01T19:42:48Z"
      volumeMounts:
      - mountPath: /etc/promtail
        name: promtail-config
      - mountPath: /logs
        name: openhab-userdata
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lw4t5
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.168
    podIPs:
    - ip: 10.42.1.168
    qosClass: Burstable
    startTime: "2025-04-01T19:42:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-09T07:52:15Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 5c56f48fdf
      pod-template-generation: "2"
    name: csi-cephfsplugin-g6hz6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: 0196e9dd-6e93-4848-87ab-f8caf2071ee7
    resourceVersion: "9411089"
    uid: 25805f4c-feb8-4c83-88e4-377a97880dfe
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-2
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sb97f
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sb97f
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-sb97f
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:52:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:52:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:05:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:54:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:52:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e4fefd0d195449202c29a6b8dd3f043edcff93a12651ef3c992d5d2950d7576b
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-09T07:52:31Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sb97f
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://72f2c51e181ca7a19cbfbd53e23d116ea01d8cc26490a155e3c7d41071dca674
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://9c8fc459c0b4365e77ddc7a98dd73a22e5e110aa9fbfc253ca788fe10a7c7f1f
          exitCode: 1
          finishedAt: "2025-03-09T07:53:00Z"
          reason: Error
          startedAt: "2025-03-09T07:52:24Z"
      name: driver-registrar
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T07:54:27Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sb97f
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 192.168.50.115
    podIPs:
    - ip: 192.168.50.115
    qosClass: BestEffort
    startTime: "2025-03-09T07:52:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-16T14:24:17Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 5c56f48fdf
      pod-template-generation: "2"
    name: csi-cephfsplugin-lt5br
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: 0196e9dd-6e93-4848-87ab-f8caf2071ee7
    resourceVersion: "11084770"
    uid: 9ca2adb0-0622-47b9-a1f3-ad5ce9554dc2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-3
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5h8fr
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5h8fr
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-5h8fr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T14:24:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T14:24:17Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4c4011bc7ab5ad5abfb965a3b40b85ec994723e5aa647cc7a67e2ef9637cb619
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://4496d37a5d77b771f6dc1219a7016dd81e310cce28dc02053eb3d8e40963ec44
          exitCode: 255
          finishedAt: "2025-03-26T19:24:26Z"
          reason: Unknown
          startedAt: "2025-03-16T14:25:04Z"
      name: csi-cephfsplugin
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:24:29Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5h8fr
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://b55ea1d58bd785838b514e72e30cf170f738e8741b8d52a254518d88e800264a
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://cd7e54457a15428bd9883447f2adef564d42cb0d2ccb01ed811b4c969bf8359b
          exitCode: 255
          finishedAt: "2025-03-26T19:24:26Z"
          reason: Unknown
          startedAt: "2025-03-16T14:25:05Z"
      name: driver-registrar
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:24:29Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5h8fr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 192.168.50.108
    podIPs:
    - ip: 192.168.50.108
    qosClass: BestEffort
    startTime: "2025-03-16T14:24:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-09T07:52:32Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 5c56f48fdf
      pod-template-generation: "2"
    name: csi-cephfsplugin-ntmgm
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: 0196e9dd-6e93-4848-87ab-f8caf2071ee7
    resourceVersion: "11099874"
    uid: f01d111a-5864-4912-af31-d2163c5ea3d2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-1
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n5sm8
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n5sm8
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-n5sm8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:03:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:52:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:03:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:52:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://86eaba9a8ce027f5676f09eb7c9dfb416edef4ba25ca0daca48f1b03989d7fc3
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://2aec0a5b47b62c1bebbd44a8588abd0823b372bd9aad0628a32ba792e7792400
          exitCode: 255
          finishedAt: "2025-03-16T20:03:15Z"
          reason: Unknown
          startedAt: "2025-03-16T20:00:04Z"
      name: csi-cephfsplugin
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:03:32Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n5sm8
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://b035e4477e51941da52b226d8ef052f72e01668fc1d0297e28f261b7709fe802
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://aef14e9eb8579f7c95e32ff213d85b14b5a7cfb5f955964a8dd2eac1691e2123
          exitCode: 255
          finishedAt: "2025-03-16T20:03:15Z"
          reason: Unknown
          startedAt: "2025-03-16T20:00:03Z"
      name: driver-registrar
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:03:31Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n5sm8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 192.168.50.237
    podIPs:
    - ip: 192.168.50.237
    qosClass: BestEffort
    startTime: "2025-03-09T07:52:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-26T19:21:45Z"
    generateName: csi-cephfsplugin-provisioner-87fd75775-
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 87fd75775
    name: csi-cephfsplugin-provisioner-87fd75775-6lhnq
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-cephfsplugin-provisioner-87fd75775
      uid: 7556307a-91a5-4946-91d2-a15bca6d3dee
    resourceVersion: "11099969"
    uid: ede61409-6a79-4f76-8487-654370fbcbfd
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-cephfsplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --v=0
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --timeout=2m30s
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --feature-gates=HonorPVReclaimPolicy=true
      - --prevent-volume-mode-conversion=true
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --controllerserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-provisioner-sa
    serviceAccountName: rook-csi-cephfs-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: socket-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: kube-api-access-sktm8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:21:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bec450e674bd40757037a8c53b05829e03663cad9d14aa5ec31ad6ffb841c846
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imageID: registry.k8s.io/sig-storage/csi-attacher@sha256:9dcd469f02bbb7592ad61b0f848ec242f9ea2102187a0cd8407df33c2d633e9c
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:21:48Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://297bd38fa92398343cc38cd4f84248bfdcea5abe123251ccaa5da40eb4d8e516
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:21:52Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://28373bc5e1557a0f22420ecad739b47ac9d759cd19046093364b514ad62402b0
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imageID: registry.k8s.io/sig-storage/csi-provisioner@sha256:bf5a235b67d8aea00f5b8ec24d384a2480e1017d5458d8a63b361e9eeb1608a9
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:21:50Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://2989b9d38588ae90c306d7f8ba0bbc8c71e139bfd5fc6d9f83c3aa89fa710d99
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imageID: registry.k8s.io/sig-storage/csi-resizer@sha256:4ecda2818f6d88a8f217babd459fdac31588f85581aa95ac7092bb0471ff8541
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:21:49Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://1a8e38d2bc2ffcbb322db9dbaabb3a087268bd278a47308c52cc3f3fc5f97187
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imageID: registry.k8s.io/sig-storage/csi-snapshotter@sha256:c4b6b02737bc24906fcce57fe6626d1a36cb2b91baa971af2a5e5a919093c34e
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:21:49Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sktm8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 10.42.1.88
    podIPs:
    - ip: 10.42.1.88
    qosClass: BestEffort
    startTime: "2025-03-26T19:21:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-09T07:50:58Z"
    generateName: csi-cephfsplugin-provisioner-87fd75775-
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 87fd75775
    name: csi-cephfsplugin-provisioner-87fd75775-xjf94
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-cephfsplugin-provisioner-87fd75775
      uid: 7556307a-91a5-4946-91d2-a15bca6d3dee
    resourceVersion: "6013979"
    uid: ab43c203-76ca-4787-96a3-e3ee4b3f88fb
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-cephfsplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --v=0
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --timeout=2m30s
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --feature-gates=HonorPVReclaimPolicy=true
      - --prevent-volume-mode-conversion=true
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --controllerserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-provisioner-sa
    serviceAccountName: rook-csi-cephfs-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: socket-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: kube-api-access-ptzxv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:29Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:50:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:50:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://38168e8f2b85e28c1792be26ea41086306ea3c81c8d85af9bc964108b091d518
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imageID: registry.k8s.io/sig-storage/csi-attacher@sha256:9dcd469f02bbb7592ad61b0f848ec242f9ea2102187a0cd8407df33c2d633e9c
      lastState:
        terminated:
          containerID: containerd://006deb147657467baacbd6f4f82ef73aa8d999735be7503fc77ce3700e645787
          exitCode: 2
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Error
          startedAt: "2025-03-09T07:51:00Z"
      name: csi-attacher
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:19Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://9dc45f08559e59df1eb2342bbb27a4b71b6f669794b3df90f084e39f44a5f74f
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://e867c5e92d01aece6883660c2d039ff139788defe34924a917b95ddd846ca305
          exitCode: 2
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Error
          startedAt: "2025-03-09T07:51:01Z"
      name: csi-cephfsplugin
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:22Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://9f8ae623accbfdbb5cec476b43909fa7c2576005ece8e93e6ebf034ce47d63aa
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imageID: registry.k8s.io/sig-storage/csi-provisioner@sha256:bf5a235b67d8aea00f5b8ec24d384a2480e1017d5458d8a63b361e9eeb1608a9
      lastState:
        terminated:
          containerID: containerd://3f560e62198435700f56dcfac82c96004b4221789ab22152e8ecb91575135aa0
          exitCode: 2
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Error
          startedAt: "2025-03-09T18:51:25Z"
      name: csi-provisioner
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:22Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://9ae98efe3d5a21ce7b31978d6c5b6811aaa1888824d2cd26fd9baa8c4a4047d1
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imageID: registry.k8s.io/sig-storage/csi-resizer@sha256:4ecda2818f6d88a8f217babd459fdac31588f85581aa95ac7092bb0471ff8541
      lastState:
        terminated:
          containerID: containerd://edf67f4b0112a94bdee531c56beb9ea433b087d94f5d10f6af3d66deccc3e349
          exitCode: 2
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Error
          startedAt: "2025-03-09T18:51:23Z"
      name: csi-resizer
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:21Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://a29be08164116f2e45378f6a966b7716dc89b5197d914e93be8c9fc5aa643553
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imageID: registry.k8s.io/sig-storage/csi-snapshotter@sha256:c4b6b02737bc24906fcce57fe6626d1a36cb2b91baa971af2a5e5a919093c34e
      lastState:
        terminated:
          containerID: containerd://968597f3ac42fd28bfe8645801569a1263ac7a0981869be3135619b460b8d221
          exitCode: 255
          finishedAt: "2025-03-09T19:01:12Z"
          reason: Error
          startedAt: "2025-03-09T07:51:00Z"
      name: csi-snapshotter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:20Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptzxv
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.230
    podIPs:
    - ip: 10.42.0.230
    qosClass: BestEffort
    startTime: "2025-03-09T07:50:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-09T07:52:35Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 5c56f48fdf
      pod-template-generation: "2"
    name: csi-cephfsplugin-qkvx7
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: 0196e9dd-6e93-4848-87ab-f8caf2071ee7
    resourceVersion: "6013992"
    uid: 553499d1-a541-419a-a941-9fab049d5f8c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-m-1
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d529q
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --forcecephkernelclient=true
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d529q
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
        type: DirectoryOrCreate
      name: ceph-csi-mountinfo
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: kube-api-access-d529q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:52:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T07:52:35Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d90efbf24fce8b7bbe54f897940650bde79160e4ccce8f33fec496d95e71f9ab
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://6fef8fb07c66f88fd3f14f75e412b9a92f07ab5103c26e22d45ac92699452511
          exitCode: 2
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Error
          startedAt: "2025-03-09T07:52:36Z"
      name: csi-cephfsplugin
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:13Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /csi/mountinfo
        name: ceph-csi-mountinfo
      - mountPath: /var/lib/kubelet/plugins
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d529q
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://fb21da9a90e50decfe26244830f657ca69c589622fbbbde851644d77d1c84057
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://9d9171ba890ee512223d8d58a984f2c8545defc68cd898332650f0ef3cf98533
          exitCode: 0
          finishedAt: "2025-03-09T20:20:41Z"
          reason: Completed
          startedAt: "2025-03-09T07:52:36Z"
      name: driver-registrar
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:11Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d529q
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 192.168.50.75
    podIPs:
    - ip: 192.168.50.75
    qosClass: BestEffort
    startTime: "2025-03-09T07:52:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-17T20:18:08Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 57bb74d47b
      pod-template-generation: "4"
    name: csi-rbdplugin-45dvd
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: e1c88784-5d82-4dc5-8041-070a8f33e50e
    resourceVersion: "11099889"
    uid: b44bb1b6-3961-4d9c-965c-a6e5e03dd768
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-1
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z28vp
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z28vp
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-z28vp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://87aceded3015ec32bf912a80bb8af8b08c77b6dd900c5e52745095ff937c2739
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-17T20:18:10Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z28vp
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://eb468dd779a74e14f4a310f7fbfea2c61332766368f6cc38690b273c1f8340e8
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-17T20:18:09Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z28vp
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    phase: Running
    podIP: 192.168.50.237
    podIPs:
    - ip: 192.168.50.237
    qosClass: BestEffort
    startTime: "2025-03-17T20:18:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-17T20:17:59Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 57bb74d47b
      pod-template-generation: "4"
    name: csi-rbdplugin-96p5p
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: e1c88784-5d82-4dc5-8041-070a8f33e50e
    resourceVersion: "7669760"
    uid: 80935cdb-d73f-4a77-be91-1efbc0e538ee
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-m-1
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6xmm9
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6xmm9
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-6xmm9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:01Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:17:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:17:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f7a69bed90b56e8e93afdeace82a0c46bb5262a77fabd1ed66fc0ba41dbb3b08
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-17T20:18:00Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6xmm9
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://29f0ad43317e14679b7cefcdbf61db85b8e6ca00abb19ed47bc8a2dae625f4d0
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-17T20:18:00Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6xmm9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 192.168.50.75
    podIPs:
    - ip: 192.168.50.75
    qosClass: BestEffort
    startTime: "2025-03-17T20:17:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-17T20:18:04Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 57bb74d47b
      pod-template-generation: "4"
    name: csi-rbdplugin-dcm6w
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: e1c88784-5d82-4dc5-8041-070a8f33e50e
    resourceVersion: "9411095"
    uid: 99ea7558-cbaf-43b0-a456-8ba48096da46
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-2
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2kxh2
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2kxh2
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-2kxh2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://65bd08d700b49f3317d91bbac256c27cb6e8c524e6281f74a31960a9273650b4
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-17T20:18:06Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2kxh2
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://3743a2cf7b894b760372b51a9ee5dc831bd2bafc5f2acf8d8718142d4880a097
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-17T20:18:05Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2kxh2
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 192.168.50.115
    podIPs:
    - ip: 192.168.50.115
    qosClass: BestEffort
    startTime: "2025-03-17T20:18:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-02T16:27:06Z"
    generateName: csi-rbdplugin-provisioner-86b55ccbdd-
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 86b55ccbdd
    name: csi-rbdplugin-provisioner-86b55ccbdd-bb5rj
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-rbdplugin-provisioner-86b55ccbdd
      uid: c1254a58-30c5-460c-b45b-383d726c10c2
    resourceVersion: "10605982"
    uid: 925fb37a-539f-469c-b7de-a9d2617f8a34
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-rbdplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      - --extra-create-metadata=true
      - --prevent-volume-mode-conversion=true
      - --feature-gates=HonorPVReclaimPolicy=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
    - args:
      - --v=0
      - --timeout=2m30s
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --controllerserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-provisioner-sa
    serviceAccountName: rook-csi-rbd-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - emptyDir:
        medium: Memory
      name: socket-dir
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-mrgkj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4c58c7056e2770972466c93c656c1c4564fe560be504b32e80489eb135897651
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imageID: registry.k8s.io/sig-storage/csi-attacher@sha256:9dcd469f02bbb7592ad61b0f848ec242f9ea2102187a0cd8407df33c2d633e9c
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:07Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://d4affc79f07d82984858f4788e132fddf0b31c658a7fcdbc37f0607e470e93e0
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imageID: registry.k8s.io/sig-storage/csi-provisioner@sha256:bf5a235b67d8aea00f5b8ec24d384a2480e1017d5458d8a63b361e9eeb1608a9
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:07Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://012f0b6a351a158ca7cd7ab16291095deaa8f8f8b89993f225002a3fe34e5ef3
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:07Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://56be2f37b2a655ab787ba4aba1d386739b5795b175032e061cdcbdd83bfaa273
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imageID: registry.k8s.io/sig-storage/csi-resizer@sha256:4ecda2818f6d88a8f217babd459fdac31588f85581aa95ac7092bb0471ff8541
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:07Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://338bfe5c66fee4306b622a7dfbb6fb16da7931e324ae72e7c2afdda8914c77f5
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imageID: registry.k8s.io/sig-storage/csi-snapshotter@sha256:c4b6b02737bc24906fcce57fe6626d1a36cb2b91baa971af2a5e5a919093c34e
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:07Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrgkj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 10.42.3.76
    podIPs:
    - ip: 10.42.3.76
    qosClass: BestEffort
    startTime: "2025-04-02T16:27:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-02T16:27:06Z"
    generateName: csi-rbdplugin-provisioner-86b55ccbdd-
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 86b55ccbdd
    name: csi-rbdplugin-provisioner-86b55ccbdd-txqpp
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-rbdplugin-provisioner-86b55ccbdd
      uid: c1254a58-30c5-460c-b45b-383d726c10c2
    resourceVersion: "10606020"
    uid: a70f6545-38ad-4ebe-a95b-471dc60fe567
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-rbdplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      - --extra-create-metadata=true
      - --prevent-volume-mode-conversion=true
      - --feature-gates=HonorPVReclaimPolicy=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
    - args:
      - --v=0
      - --timeout=2m30s
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --default-fstype=ext4
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=2m30s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --leader-election-lease-duration=2m17s
      - --leader-election-renew-deadline=1m47s
      - --leader-election-retry-period=26s
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --controllerserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-provisioner-sa
    serviceAccountName: rook-csi-rbd-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - emptyDir:
        medium: Memory
      name: socket-dir
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-t5wkh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:17Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T16:27:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1c019a26916ca282735b8b6f35a535aea4787755413af44be20e142c11c77c41
      image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
      imageID: registry.k8s.io/sig-storage/csi-attacher@sha256:9dcd469f02bbb7592ad61b0f848ec242f9ea2102187a0cd8407df33c2d633e9c
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:12Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://a83e81bfc6ceba26304cfb6e36c93eedc5d936a00faa85ecc8a7d864dda21485
      image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
      imageID: registry.k8s.io/sig-storage/csi-provisioner@sha256:bf5a235b67d8aea00f5b8ec24d384a2480e1017d5458d8a63b361e9eeb1608a9
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:12Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://f2b7f3148fcb313d819e0cbbdf218eb93c6fbc40e6663097b123abcbca7dc1cb
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:15Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://e4d1b301a93d17b28d9bb5e9efe120a7182026eda73d4fb487ef33a104c8c2f2
      image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
      imageID: registry.k8s.io/sig-storage/csi-resizer@sha256:4ecda2818f6d88a8f217babd459fdac31588f85581aa95ac7092bb0471ff8541
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:12Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://156996dadc4189c9f034b14d439209bfa6738155f6524d4ded02a153c78fd03d
      image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
      imageID: registry.k8s.io/sig-storage/csi-snapshotter@sha256:c4b6b02737bc24906fcce57fe6626d1a36cb2b91baa971af2a5e5a919093c34e
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-04-02T16:27:13Z"
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t5wkh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 10.42.2.6
    podIPs:
    - ip: 10.42.2.6
    qosClass: BestEffort
    startTime: "2025-04-02T16:27:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-17T20:18:01Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 57bb74d47b
      pod-template-generation: "4"
    name: csi-rbdplugin-rh2np
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: e1c88784-5d82-4dc5-8041-070a8f33e50e
    resourceVersion: "11084774"
    uid: 990da452-e0ce-4892-bfea-12c956789798
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - k3s-w-3
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4rk84
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
          drop:
          - ALL
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4rk84
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: ceph-csi-configs
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
        - configMap:
            items:
            - key: csi-mapping-config-json
              path: cluster-mapping.json
            name: rook-ceph-csi-mapping-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: oidc-token
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            audience: ceph-csi-kms
            expirationSeconds: 3600
            path: oidc-token
    - name: kube-api-access-4rk84
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-17T20:18:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f95a6c8f392d51bfea6968e2ec6411395433441e7fce17124219af30d2d84a5c
      image: quay.io/cephcsi/cephcsi:v3.11.0
      imageID: quay.io/cephcsi/cephcsi@sha256:a338c0dec57b8033e01f277dc342607d7610411422cc3ce47f176ab059aeaf21
      lastState:
        terminated:
          containerID: containerd://c0b05e450ad7944802dca8cf7562f509270b7e9f005666c1d8610aaa6a577695
          exitCode: 255
          finishedAt: "2025-03-26T19:24:26Z"
          reason: Unknown
          startedAt: "2025-03-17T20:18:01Z"
      name: csi-rbdplugin
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:24:29Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-configs
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /run/secrets/tokens
        name: oidc-token
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4rk84
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://876f4b966a1144a86ee74e95080e46db16fc0c9bd5fc34a0efcef4d81349504b
      image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
      imageID: registry.k8s.io/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac
      lastState:
        terminated:
          containerID: containerd://15a7fb49a8a6124eea10e27267261e6839c2d982e42d578d762d3bd1320eed4e
          exitCode: 255
          finishedAt: "2025-03-26T19:24:26Z"
          reason: Unknown
          startedAt: "2025-03-17T20:18:01Z"
      name: driver-registrar
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:24:29Z"
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4rk84
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    phase: Running
    podIP: 192.168.50.108
    podIPs:
    - ip: 192.168.50.108
    qosClass: BestEffort
    startTime: "2025-03-17T20:18:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "9926"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-09T22:35:21Z"
    generateName: rook-ceph-exporter-k3s-m-1-df76dc8d8-
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-m-1
      node_name: k3s-m-1
      pod-template-hash: df76dc8d8
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-m-1-df76dc8d8-ld55q
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-exporter-k3s-m-1-df76dc8d8
      uid: accbabc7-d2c5-4641-ba55-ff988b0cdbdc
    resourceVersion: "9750109"
    uid: b37472f5-dced-4665-bbc6-0137b69b2926
  spec:
    containers:
    - args:
      - --sock-dir
      - /run/ceph
      - --port
      - "9926"
      - --prio-limit
      - "5"
      - --stats-period
      - "5"
      command:
      - ceph-exporter
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: ceph-exporter
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-crw8t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-crw8t
        readOnly: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/hostname: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-exporter-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-exporter-keyring
    - name: kube-api-access-crw8t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T22:35:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T22:35:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-28T19:39:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-28T19:39:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T22:35:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://19d0d64011d22e42c43c4245110c4c8c657280b41711e09f3409e46f56584777
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState:
        terminated:
          containerID: containerd://1a43c73a76d2046319e4e15c6cda75d2011861da35b57432fefdf68918869453
          exitCode: 1
          finishedAt: "2025-03-28T19:39:22Z"
          reason: Error
          startedAt: "2025-03-13T12:38:40Z"
      name: ceph-exporter
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-03-28T19:39:23Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-crw8t
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    initContainerStatuses:
    - containerID: containerd://b45528b7bf834f211ccfaeaa16aebbfc9e9213c9c63f38283548f2ca7adbbe9b
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://b45528b7bf834f211ccfaeaa16aebbfc9e9213c9c63f38283548f2ca7adbbe9b
          exitCode: 0
          finishedAt: "2025-03-09T22:35:22Z"
          reason: Completed
          startedAt: "2025-03-09T22:35:22Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-crw8t
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.0.246
    podIPs:
    - ip: 10.42.0.246
    qosClass: BestEffort
    startTime: "2025-03-09T22:35:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "9926"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-16T19:59:57Z"
    generateName: rook-ceph-exporter-k3s-w-1-58fdfc478-
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-1
      node_name: k3s-w-1
      pod-template-hash: 58fdfc478
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-1-58fdfc478-8rnkg
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-exporter-k3s-w-1-58fdfc478
      uid: 67a434c0-fe5b-4a13-85e8-bea28bc8b919
    resourceVersion: "11099869"
    uid: 606296ae-5986-4ec6-af19-a1e6acbc9548
  spec:
    containers:
    - args:
      - --sock-dir
      - /run/ceph
      - --port
      - "9926"
      - --prio-limit
      - "5"
      - --stats-period
      - "5"
      command:
      - ceph-exporter
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: ceph-exporter
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-thrpp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-thrpp
        readOnly: true
    nodeName: k3s-w-1
    nodeSelector:
      kubernetes.io/hostname: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-exporter-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-exporter-keyring
    - name: kube-api-access-thrpp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:03:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:00:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T20:03:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-16T19:59:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://70388705293eef485039e3cd75f5c0eaf3b093fe696097e4ab967b08c78cd676
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState:
        terminated:
          containerID: containerd://db98f6a53a9bb25ade5e2f3239f283b195f3ab2cd5d9b27ce9e764fb11320de9
          exitCode: 255
          finishedAt: "2025-03-16T20:03:15Z"
          reason: Unknown
          startedAt: "2025-03-16T20:00:06Z"
      name: ceph-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-16T20:03:33Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-thrpp
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    initContainerStatuses:
    - containerID: containerd://2a710a35dcb99cd638431842c3647bced7fcef519b7dbe5676dc11bc15987e2f
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://2a710a35dcb99cd638431842c3647bced7fcef519b7dbe5676dc11bc15987e2f
          exitCode: 0
          finishedAt: "2025-03-16T20:03:31Z"
          reason: Completed
          startedAt: "2025-03-16T20:03:31Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-thrpp
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.1.4
    podIPs:
    - ip: 10.42.1.4
    qosClass: BestEffort
    startTime: "2025-03-16T20:00:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "9926"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-27T03:55:07Z"
    generateName: rook-ceph-exporter-k3s-w-2-ccc459589-
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-2
      node_name: k3s-w-2
      pod-template-hash: ccc459589
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-2-ccc459589-qmncg
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-exporter-k3s-w-2-ccc459589
      uid: 1a610de5-2d34-44d5-a9b4-8b5156657dd9
    resourceVersion: "9964633"
    uid: 7e1323b1-fcd3-4b2e-b5d0-2cb44e632c6f
  spec:
    containers:
    - args:
      - --sock-dir
      - /run/ceph
      - --port
      - "9926"
      - --prio-limit
      - "5"
      - --stats-period
      - "5"
      command:
      - ceph-exporter
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: ceph-exporter
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zqpgl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zqpgl
        readOnly: true
    nodeName: k3s-w-2
    nodeSelector:
      kubernetes.io/hostname: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-exporter-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-exporter-keyring
    - name: kube-api-access-zqpgl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-27T03:57:37Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-27T03:57:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T00:46:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T00:46:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-27T03:55:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://773dae685acf58143ade3593dbcfff7b6972571e73187069638caf74d4b94439
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState:
        terminated:
          containerID: containerd://36650167d4c5a1e1cd6c3f43420a875403dba25ea883a866310c207b7968a8cc
          exitCode: 1
          finishedAt: "2025-03-30T00:46:38Z"
          reason: Error
          startedAt: "2025-03-27T03:57:50Z"
      name: ceph-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-30T00:46:51Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zqpgl
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    initContainerStatuses:
    - containerID: containerd://bfae45847e4f30fc1b2d7f39c8f2f69e05720f7218a00dd8fab367e593062dc8
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://bfae45847e4f30fc1b2d7f39c8f2f69e05720f7218a00dd8fab367e593062dc8
          exitCode: 0
          finishedAt: "2025-03-27T03:57:06Z"
          reason: Completed
          startedAt: "2025-03-27T03:56:37Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zqpgl
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.2.38
    podIPs:
    - ip: 10.42.2.38
    qosClass: BestEffort
    startTime: "2025-03-27T03:55:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "9926"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-26T19:21:45Z"
    generateName: rook-ceph-exporter-k3s-w-3-8576f74844-
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-3
      node_name: k3s-w-3
      pod-template-hash: 8576f74844
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-3-8576f74844-mwx8h
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-exporter-k3s-w-3-8576f74844
      uid: de1cc63f-42f1-4b58-8ecd-c925b6ed1518
    resourceVersion: "9322519"
    uid: 6ae4bf82-973c-4618-b8ac-c96929b40b17
  spec:
    containers:
    - args:
      - --sock-dir
      - /run/ceph
      - --port
      - "9926"
      - --prio-limit
      - "5"
      - --stats-period
      - "5"
      command:
      - ceph-exporter
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: ceph-exporter
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9kqxh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9kqxh
        readOnly: true
    nodeName: k3s-w-3
    nodeSelector:
      kubernetes.io/hostname: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-exporter-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-exporter-keyring
    - name: kube-api-access-9kqxh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T19:24:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://86ad06db7b6de86746d11bff63f48619c25eebe2514cd1b787484ceaa020f381
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: ceph-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T19:24:30Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/exporter-keyring-store/
        name: rook-ceph-exporter-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9kqxh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    initContainerStatuses:
    - containerID: containerd://26ebf1469ff5e2936a7c43accfaaf17c2944708ffd0612c5b987a01745830ee4
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://26ebf1469ff5e2936a7c43accfaaf17c2944708ffd0612c5b987a01745830ee4
          exitCode: 0
          finishedAt: "2025-03-26T19:24:29Z"
          reason: Completed
          startedAt: "2025-03-26T19:24:29Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9kqxh
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.3.179
    podIPs:
    - ip: 10.42.3.179
    qosClass: BestEffort
    startTime: "2025-03-26T19:24:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "9283"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-30T07:44:43Z"
    generateName: rook-ceph-mgr-a-dcf48548-
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: dcf48548
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-dcf48548-f6xgr
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mgr-a-dcf48548
      uid: d437df49-ec79-4cd5-bdd9-d7234a847336
    resourceVersion: "10640787"
    uid: cd195dea-7a23-4516-ab27-87d2e63726d8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - k3s-m-1
    containers:
    - args:
      - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --client-mount-uid=0
      - --client-mount-gid=0
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mgr
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_OPERATOR_NAMESPACE
        value: rook-ceph
      - name: ROOK_CEPH_CLUSTER_CRD_VERSION
        value: v1
      - name: ROOK_CEPH_CLUSTER_CRD_NAME
        value: rook-ceph
      - name: CEPH_ARGS
        value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: mgr
      ports:
      - containerPort: 6800
        name: mgr
        protocol: TCP
      - containerPort: 9283
        name: http-metrics
        protocol: TCP
      - containerPort: 7000
        name: dashboard
        protocol: TCP
      resources:
        limits:
          cpu: "3"
          memory: 1Gi
        requests:
          cpu: 800m
          memory: 500Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9ndg
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mgr/ceph-a
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "3"
          memory: 1Gi
        requests:
          cpu: 800m
          memory: 500Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9ndg
        readOnly: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-mgr
    serviceAccountName: rook-ceph-mgr
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mgr-a-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mgr-a-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: kube-api-access-x9ndg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T07:44:44Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T07:44:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T07:44:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c5500e07d192d5ac4a47fe26dc1b0597abd4fb609db1d6fb846c6e0a061bc44b
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: mgr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-30T07:44:44Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9ndg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    initContainerStatuses:
    - containerID: containerd://ca091f2d09ada0e63074af2a07ce583f43ae137d00cb4dadd422f7e3e87e838d
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ca091f2d09ada0e63074af2a07ce583f43ae137d00cb4dadd422f7e3e87e838d
          exitCode: 0
          finishedAt: "2025-03-30T07:44:44Z"
          reason: Completed
          startedAt: "2025-03-30T07:44:44Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9ndg
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.0.168
    podIPs:
    - ip: 10.42.0.168
    qosClass: Burstable
    startTime: "2025-03-30T07:44:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-29T17:12:06Z"
    generateName: rook-ceph-mon-a-6d68789b6b-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 6d68789b6b
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-6d68789b6b-brrsm
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-a-6d68789b6b
      uid: e7da8e60-740e-4156-863a-68153530ded5
    resourceVersion: "11100136"
    uid: aece8fe7-3e55-4119-958b-a4ba6b136bc2
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.130.194
      - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4nwpx
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-a
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4nwpx
        readOnly: true
    - args:
      - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.130.194
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4nwpx
        readOnly: true
    nodeName: k3s-w-1
    nodeSelector:
      kubernetes.io/hostname: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-a/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-4nwpx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:12:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:12:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:03:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:12:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f397ba06b565dccd01812d54527e3d8681b10005c1d94e2e019c3225685cd5dc
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-29T17:12:09Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4nwpx
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    initContainerStatuses:
    - containerID: containerd://f26ee9fb5c7fb320c29c4aa2ff4b90921630514f2aa340b0a7d10ae3b1cee78d
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://f26ee9fb5c7fb320c29c4aa2ff4b90921630514f2aa340b0a7d10ae3b1cee78d
          exitCode: 0
          finishedAt: "2025-03-29T17:12:07Z"
          reason: Completed
          startedAt: "2025-03-29T17:12:07Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4nwpx
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://e8d0c730f5adc739096c2315456c80b10f7b63b439ccf3215022aadd037e45be
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e8d0c730f5adc739096c2315456c80b10f7b63b439ccf3215022aadd037e45be
          exitCode: 0
          finishedAt: "2025-03-29T17:12:08Z"
          reason: Completed
          startedAt: "2025-03-29T17:12:08Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4nwpx
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.1.146
    podIPs:
    - ip: 10.42.1.146
    qosClass: Burstable
    startTime: "2025-03-29T17:12:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-29T17:11:40Z"
    generateName: rook-ceph-mon-e-78588fff8b-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      pod-template-hash: 78588fff8b
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e-78588fff8b-dmcmf
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-e-78588fff8b
      uid: 13e9aff6-3342-4ae9-bcb4-13ba217e4464
    resourceVersion: "10640765"
    uid: 27a6db87-49f5-4730-9f59-e7f02687b2fe
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=e
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.219.20
      - --setuser-match-path=/var/lib/ceph/mon/ceph-e/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n42fw
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-e
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n42fw
        readOnly: true
    - args:
      - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=e
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.219.20
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n42fw
        readOnly: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/hostname: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-e/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-n42fw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:11:41Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:11:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:11:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5447c893c5a33820cc7809a9b218158f286ed1076a7ea080ac6ddae1eb6eb9b1
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-29T17:11:42Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n42fw
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    initContainerStatuses:
    - containerID: containerd://26f5a5bfc10871617d7b43224d9a350a9b6537eac69b83e6b663eba2c1118bae
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://26f5a5bfc10871617d7b43224d9a350a9b6537eac69b83e6b663eba2c1118bae
          exitCode: 0
          finishedAt: "2025-03-29T17:11:40Z"
          reason: Completed
          startedAt: "2025-03-29T17:11:40Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n42fw
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://dc5b1cf3bff43edbca3f5ca383dda7a17e3801db3b3bb6db73931deceb64b2d9
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://dc5b1cf3bff43edbca3f5ca383dda7a17e3801db3b3bb6db73931deceb64b2d9
          exitCode: 0
          finishedAt: "2025-03-29T17:11:41Z"
          reason: Completed
          startedAt: "2025-03-29T17:11:41Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-e
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n42fw
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.0.163
    podIPs:
    - ip: 10.42.0.163
    qosClass: Burstable
    startTime: "2025-03-29T17:11:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-29T17:12:30Z"
    generateName: rook-ceph-mon-h-9bc6b5db7-
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: h
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: h
      ceph_daemon_type: mon
      mon: h
      mon_cluster: rook-ceph
      pod-template-hash: 9bc6b5db7
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-h-9bc6b5db7-686wf
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-h-9bc6b5db7
      uid: b4efcf29-33b5-49a4-b937-7f6431ef2c62
    resourceVersion: "11084664"
    uid: f2869acc-b1d2-4b09-b0bf-0c8b52d2e4c8
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=h
      - --setuser=ceph
      - --setgroup=ceph
      - --foreground
      - --public-addr=10.43.167.194
      - --setuser-match-path=/var/lib/ceph/mon/ceph-h/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.h.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: mon
      ports:
      - containerPort: 3300
        name: tcp-msgr2
        protocol: TCP
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.h.asok mon_status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with the
            following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-h
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8rr2f
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      - /var/lib/ceph/mon/ceph-h
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-h
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8rr2f
        readOnly: true
    - args:
      - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
      - --keyring=/etc/ceph/keyring-store/keyring
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=h
      - --setuser=ceph
      - --setgroup=ceph
      - --public-addr=10.43.167.194
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources:
        limits:
          cpu: 1500m
          memory: 500Mi
        requests:
          cpu: 300m
          memory: 250Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-h
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8rr2f
        readOnly: true
    nodeName: k3s-w-2
    nodeSelector:
      kubernetes.io/hostname: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-default
    serviceAccountName: rook-ceph-default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-h/data
        type: ""
      name: ceph-daemon-data
    - name: kube-api-access-8rr2f
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:12:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:12:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:05:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:05:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:12:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0f70f396eabf880a5dceb6c8c1218e0417d56c10d8bbd37f756cf5f230541257
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-29T17:12:35Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-h
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8rr2f
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    initContainerStatuses:
    - containerID: containerd://5515169405af44e0fec976a8a9461e6fb3c945b3f15074eee83baef2bc2c06eb
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://5515169405af44e0fec976a8a9461e6fb3c945b3f15074eee83baef2bc2c06eb
          exitCode: 0
          finishedAt: "2025-03-29T17:12:31Z"
          reason: Completed
          startedAt: "2025-03-29T17:12:31Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-h
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8rr2f
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://bd2051c154334d03a8c027ab8fe63a68129eef29ddc7cbb2658bfe77ae929539
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://bd2051c154334d03a8c027ab8fe63a68129eef29ddc7cbb2658bfe77ae929539
          exitCode: 0
          finishedAt: "2025-03-29T17:12:34Z"
          reason: Completed
          startedAt: "2025-03-29T17:12:34Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-h
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8rr2f
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.2.228
    podIPs:
    - ip: 10.42.2.228
    qosClass: Burstable
    startTime: "2025-03-29T17:12:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-26T17:50:13Z"
    generateName: rook-ceph-operator-fc4766689-
    labels:
      app: rook-ceph-operator
      pod-template-hash: fc4766689
    name: rook-ceph-operator-fc4766689-fbwpt
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-operator-fc4766689
      uid: 0aec6bb7-b43c-44a0-9917-8bf0f8d363d8
    resourceVersion: "9309698"
    uid: 0007efc1-2129-43df-a450-78ea39d50c06
  spec:
    containers:
    - args:
      - ceph
      - operator
      env:
      - name: ROOK_CURRENT_NAMESPACE_ONLY
        value: "false"
      - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
        value: "false"
      - name: DISCOVER_DAEMON_UDEV_BLACKLIST
        value: (?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+
      - name: ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS
        value: "5"
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rook/ceph:v1.14.8
      imagePullPolicy: IfNotPresent
      name: rook-ceph-operator
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        runAsGroup: 2016
        runAsNonRoot: true
        runAsUser: 2016
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-config
      - mountPath: /etc/ceph
        name: default-config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-82rgb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-system
    serviceAccountName: rook-ceph-system
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: rook-config
    - emptyDir: {}
      name: default-config-dir
    - name: kube-api-access-82rgb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T17:50:14Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T17:50:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T17:50:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T17:50:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-26T17:50:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bd930d918b56d87a5e0656d49c640d200c2ee2933a149b064947d0a12e0262fd
      image: docker.io/rook/ceph:v1.14.8
      imageID: docker.io/rook/ceph@sha256:b63db448a08f313fc88dedb89d3d713fe4576dc72a3f458e7427fc460f1094b3
      lastState: {}
      name: rook-ceph-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-26T17:50:14Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-config
      - mountPath: /etc/ceph
        name: default-config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-82rgb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.107
    podIPs:
    - ip: 10.42.0.107
    qosClass: BestEffort
    startTime: "2025-03-26T17:50:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-29T17:13:39Z"
    generateName: rook-ceph-osd-0-cddcdb46d-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      pod-template-hash: cddcdb46d
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0-cddcdb46d-tp2sg
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-0-cddcdb46d
      uid: 95c1d2aa-d2cc-4002-ac1a-81e123f20e7a
    resourceVersion: "10640775"
    uid: 0e5cfc3b-caf9-4348-b477-d73fea4e38e5
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "0"
      - --fsid
      - e2e946a6-06e8-4818-8395-ea14fe84164d
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k3s-m-1
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: k3s-m-1
      - name: ROOK_CLUSTER_ID
        value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k3s-m-1
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: e319b7b8-8843-4ff4-a053-f07520dbe330
      - name: ROOK_OSD_ID
        value: "0"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda1
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: hdd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: osd
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6b2vw
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda1
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "0"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6b2vw
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-0
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6b2vw
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6b2vw
        readOnly: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/hostname: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-6b2vw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:13:40Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:13:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T21:04:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:13:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5ad85a051a3c4673640bd094d812a57da0159ba9eb12ba5bdc187f6def727474
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-29T17:13:55Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6b2vw
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    initContainerStatuses:
    - containerID: containerd://cf70cd703564c887e50b5116d157c2446e0073b02dc2c13d9971d2549a2b3125
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://cf70cd703564c887e50b5116d157c2446e0073b02dc2c13d9971d2549a2b3125
          exitCode: 0
          finishedAt: "2025-03-29T17:13:42Z"
          reason: Completed
          startedAt: "2025-03-29T17:13:40Z"
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6b2vw
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://3da9d749798d7d5c429fe710f34ac8b779dc9a6453636529baa067243c3e9567
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3da9d749798d7d5c429fe710f34ac8b779dc9a6453636529baa067243c3e9567
          exitCode: 0
          finishedAt: "2025-03-29T17:13:53Z"
          reason: Completed
          startedAt: "2025-03-29T17:13:43Z"
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6b2vw
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://1b10a5e1e2b9b7984977804ab2e057519755ecbbc6cd0d76ebcdbb973190cc81
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://1b10a5e1e2b9b7984977804ab2e057519755ecbbc6cd0d76ebcdbb973190cc81
          exitCode: 0
          finishedAt: "2025-03-29T17:13:54Z"
          reason: Completed
          startedAt: "2025-03-29T17:13:54Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6b2vw
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.0.165
    podIPs:
    - ip: 10.42.0.165
    qosClass: Burstable
    startTime: "2025-03-29T17:13:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-29T17:14:27Z"
    generateName: rook-ceph-osd-1-756ccd44c-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 756ccd44c
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1-756ccd44c-w9hp7
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-1-756ccd44c
      uid: 87a37012-88a1-4f4b-98e5-29878ecda3b7
    resourceVersion: "11100319"
    uid: b988a760-1f32-4685-aa13-f5df89920b70
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "1"
      - --fsid
      - e2e946a6-06e8-4818-8395-ea14fe84164d
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k3s-w-1
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: k3s-w-1
      - name: ROOK_CLUSTER_ID
        value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k3s-w-1
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
      - name: ROOK_OSD_ID
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda3
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: hdd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: osd
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bhdnk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda3
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "1"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bhdnk
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-1
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bhdnk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bhdnk
        readOnly: true
    nodeName: k3s-w-1
    nodeSelector:
      kubernetes.io/hostname: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-bhdnk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:14:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:15:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:14:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bb886ef67a25bba315aff518b7f6bef6d5af68f3d7609e98ecb8441f754a5194
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState:
        terminated:
          containerID: containerd://40dfabced20b910bb15a78b23139e28c44759c5093f892352689f35b1b9e363f
          exitCode: 133
          finishedAt: "2025-04-05T12:03:53Z"
          reason: Error
          startedAt: "2025-03-29T17:15:11Z"
      name: osd
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-04-05T12:03:55Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bhdnk
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    initContainerStatuses:
    - containerID: containerd://4fb572d38c4d8a09bc68642bc16b552274b41a7c00523f0329f06ce9297fc48c
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4fb572d38c4d8a09bc68642bc16b552274b41a7c00523f0329f06ce9297fc48c
          exitCode: 0
          finishedAt: "2025-03-29T17:14:40Z"
          reason: Completed
          startedAt: "2025-03-29T17:14:29Z"
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bhdnk
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://f29a2003303b1b6054155d55708a97ec1393d42e477d3d5273f24a96a0687ecb
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://f29a2003303b1b6054155d55708a97ec1393d42e477d3d5273f24a96a0687ecb
          exitCode: 0
          finishedAt: "2025-03-29T17:15:09Z"
          reason: Completed
          startedAt: "2025-03-29T17:14:41Z"
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bhdnk
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://e70b14672bc05c5d60c3953e7e7efd99236368058059e3e0e50399b9267edb1e
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e70b14672bc05c5d60c3953e7e7efd99236368058059e3e0e50399b9267edb1e
          exitCode: 0
          finishedAt: "2025-03-29T17:15:10Z"
          reason: Completed
          startedAt: "2025-03-29T17:15:10Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bhdnk
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.1.148
    podIPs:
    - ip: 10.42.1.148
    qosClass: Burstable
    startTime: "2025-03-29T17:14:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-29T17:15:36Z"
    generateName: rook-ceph-osd-2-56c7cdd86f-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k3s-w-2
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 56c7cdd86f
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-2
      topology-location-root: default
    name: rook-ceph-osd-2-56c7cdd86f-5qj68
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-2-56c7cdd86f
      uid: 70604cd3-722c-431e-a2dd-881516328eb8
    resourceVersion: "11084689"
    uid: 84ecd183-e30a-4018-8686-f6ec87b7ee70
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "2"
      - --fsid
      - e2e946a6-06e8-4818-8395-ea14fe84164d
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k3s-w-2
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: k3s-w-2
      - name: ROOK_CLUSTER_ID
        value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k3s-w-2
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: d0c27c6b-8b46-4935-b66b-1b22f85feda5
      - name: ROOK_OSD_ID
        value: "2"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: ssd
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: osd
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv7p
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=d0c27c6b-8b46-4935-b66b-1b22f85feda5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "2"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv7p
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-2
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv7p
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv7p
        readOnly: true
    nodeName: k3s-w-2
    nodeSelector:
      kubernetes.io/hostname: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_d0c27c6b-8b46-4935-b66b-1b22f85feda5
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-dvv7p
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:15:48Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:16:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:15:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f48e17c3e924925fd3957aff4c5ed8c67afacd32d1099fb7100b0102ab0a82f8
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-29T17:17:00Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv7p
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    initContainerStatuses:
    - containerID: containerd://cd62a2f41604fc3e83221c354108abaa7bae7002a43fc7939e2bc27bdb786876
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://cd62a2f41604fc3e83221c354108abaa7bae7002a43fc7939e2bc27bdb786876
          exitCode: 0
          finishedAt: "2025-03-29T17:15:52Z"
          reason: Completed
          startedAt: "2025-03-29T17:15:47Z"
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv7p
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://45c6fda8527c554d4219451b5f9d14c0b4baa341ba10bab34155107006931f0a
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://45c6fda8527c554d4219451b5f9d14c0b4baa341ba10bab34155107006931f0a
          exitCode: 0
          finishedAt: "2025-03-29T17:16:52Z"
          reason: Completed
          startedAt: "2025-03-29T17:16:19Z"
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv7p
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://a2248e863448655c3998bdc0c9819b5921607e07f572ac448508c8b73e99b3b0
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a2248e863448655c3998bdc0c9819b5921607e07f572ac448508c8b73e99b3b0
          exitCode: 0
          finishedAt: "2025-03-29T17:16:54Z"
          reason: Completed
          startedAt: "2025-03-29T17:16:53Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dvv7p
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.2.230
    podIPs:
    - ip: 10.42.2.230
    qosClass: Burstable
    startTime: "2025-03-29T17:15:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-29T17:16:06Z"
    generateName: rook-ceph-osd-3-c984f9545-
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "3"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "3"
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      device-class: nvme
      failure-domain: k3s-w-3
      osd: "3"
      osd-store: bluestore
      pod-template-hash: c984f9545
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-3
      topology-location-root: default
    name: rook-ceph-osd-3-c984f9545-gtq2k
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-3-c984f9545
      uid: 1e9e668a-08ea-44a3-9e2a-ca9e32128b98
    resourceVersion: "11084868"
    uid: 575ba063-de30-4e20-8533-b68da1b955dc
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "3"
      - --fsid
      - e2e946a6-06e8-4818-8395-ea14fe84164d
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=k3s-w-3
      - --default-log-to-stderr=true
      - --default-err-to-stderr=true
      - --default-mon-cluster-log-to-stderr=true
      - '--default-log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: k3s-w-3
      - name: ROOK_CLUSTER_ID
        value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k3s-w-3
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: CONTAINER_IMAGE
        value: quay.io/ceph/ceph:v18.2.2
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: CEPH_USE_RANDOM_NONCE
        value: "true"
      - name: ROOK_OSD_RESTART_INTERVAL
        value: "0"
      - name: ROOK_OSD_UUID
        value: 9c8eabc6-e7b1-4837-8d10-9d1b9e107222
      - name: ROOK_OSD_ID
        value: "3"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/nvme0n1p3
      - name: ROOK_CV_MODE
        value: raw
      - name: ROOK_OSD_DEVICE_CLASS
        value: nvme
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 5
        initialDelaySeconds: 10
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: osd
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
            [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
            check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
            /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpbmc
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are
        unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=9c8eabc6-e7b1-4837-8d10-9d1b9e107222\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
        \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
        OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
        This limitation will be removed later. After that, we can remove this\n# fsid
        injection code. Probably a good time is when to remove Quincy support.\n#
        https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
        /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
        = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif not
        config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
        \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
        'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
        -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd'
        mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
        \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
        this is needed because when the init container exits, the tmpfs goes away
        and its content with it\n\t# this will result in the emptydir to be empty
        when accessed by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
        \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
        \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
        osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
        retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
        ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
        --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
        job uses to report OSDs on nodes)\n\t#  returns user-friendly device names
        which can change when systems reboot. To\n\t# keep OSD pods from crashing
        repeatedly after a reboot, we need to check if the\n\t# block device we have
        is still correct, and if it isn't correct, we need to\n\t# scan all the disks
        to find the right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device()
        {\n\t\t# jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
        python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
        sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
        == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device: '
        + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
        \ # don't keep processing once the disk is found\nsys.exit('no disk found
        with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
        > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
        '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device < \"$OSD_LIST\";
        then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
        < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ;
        exit 1 ; }\n\n\t# If a kernel device name change happens and a block device
        file\n\t# in the OSD directory becomes missing, this OSD fails to start\n\t#
        continuously. This problem can be resolved by confirming\n\t# the validity
        of the device file and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
        [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
        $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore
        so we don't need to pass a store flag\n\tceph-volume raw activate --device
        \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/nvme0n1p3
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      - name: ROOK_OSD_ID
        value: "3"
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpbmc
        readOnly: true
    - args:
      - bluefs-bdev-expand
      - --path
      - /var/lib/ceph/osd/ceph-3
      command:
      - ceph-bluestore-tool
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpbmc
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /run/ceph
      command:
      - chown
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpbmc
        readOnly: true
    nodeName: k3s-w-3
    nodeSelector:
      kubernetes.io/hostname: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_9c8eabc6-e7b1-4837-8d10-9d1b9e107222
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-bpbmc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:16:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:16:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T10:06:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-29T17:16:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c3fcf56b13c04295f25c284dfbc23cb63cccadb7bb8fcb8043d107f86786029a
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-29T17:16:21Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpbmc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    initContainerStatuses:
    - containerID: containerd://c2f8457aa0e1db2b5cb753762d11510c97da14bb6323d666b359c2dafce6d470
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c2f8457aa0e1db2b5cb753762d11510c97da14bb6323d666b359c2dafce6d470
          exitCode: 0
          finishedAt: "2025-03-29T17:16:08Z"
          reason: Completed
          startedAt: "2025-03-29T17:16:06Z"
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/temp-ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ceph/admin-keyring-store/
        name: rook-ceph-admin-keyring
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpbmc
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://d413563ef7f977ee7317d54a28b79990f9d4b9bf5c8a2e2857bb48c4213d6593
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: expand-bluefs
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d413563ef7f977ee7317d54a28b79990f9d4b9bf5c8a2e2857bb48c4213d6593
          exitCode: 0
          finishedAt: "2025-03-29T17:16:19Z"
          reason: Completed
          startedAt: "2025-03-29T17:16:08Z"
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpbmc
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://ef3fb80ca118ed359982b4855522e9fdc7c806c0196fe347f343788e079e8937
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://ef3fb80ca118ed359982b4855522e9fdc7c806c0196fe347f343788e079e8937
          exitCode: 0
          finishedAt: "2025-03-29T17:16:20Z"
          reason: Completed
          startedAt: "2025-03-29T17:16:20Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpbmc
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.3.29
    podIPs:
    - ip: 10.42.3.29
    qosClass: Burstable
    startTime: "2025-03-29T17:16:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-30T08:48:08Z"
    generateName: rook-ceph-osd-prepare-k3s-m-1-
    labels:
      app: rook-ceph-osd-prepare
      batch.kubernetes.io/controller-uid: 7c35dbe4-c62c-4edb-bcdb-fd4c996e58d2
      batch.kubernetes.io/job-name: rook-ceph-osd-prepare-k3s-m-1
      ceph.rook.io/pvc: ""
      controller-uid: 7c35dbe4-c62c-4edb-bcdb-fd4c996e58d2
      job-name: rook-ceph-osd-prepare-k3s-m-1
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-k3s-m-1-ckbs4
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-k3s-m-1
      uid: 7c35dbe4-c62c-4edb-bcdb-fd4c996e58d2
    resourceVersion: "10640641"
    uid: 71732f9e-102e-4139-89ce-8af9e2b42e3f
  spec:
    affinity: {}
    containers:
    - args:
      - ceph
      - osd
      - provision
      command:
      - /rook/rook
      env:
      - name: ROOK_NODE_NAME
        value: k3s-m-1
      - name: ROOK_CLUSTER_ID
        value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: ROOK_OSD_STORE_TYPE
        value: bluestore
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k3s-m-1
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: all
      - name: ROOK_CEPH_VERSION
        value: ceph version 18.2.2-0 reef
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: provision
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fsk42
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --archive
      - --force
      - --verbose
      - /usr/local/bin/rook
      - /rook
      command:
      - cp
      image: rook/ceph:v1.14.8
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fsk42
        readOnly: true
    nodeName: k3s-m-1
    nodeSelector:
      kubernetes.io/hostname: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: kube-api-access-fsk42
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:29Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:10Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:28Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:28Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f1e7c63037e94184b3ac4b95192562e602444ed216074119702337a07c32d286
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://f1e7c63037e94184b3ac4b95192562e602444ed216074119702337a07c32d286
          exitCode: 0
          finishedAt: "2025-03-30T08:48:27Z"
          reason: Completed
          startedAt: "2025-03-30T08:48:10Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fsk42
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    initContainerStatuses:
    - containerID: containerd://1948c4b477fbb4f44cdb787e5190e706dbcd347b8cc1ff1757d19661480a1b99
      image: docker.io/rook/ceph:v1.14.8
      imageID: docker.io/rook/ceph@sha256:b63db448a08f313fc88dedb89d3d713fe4576dc72a3f458e7427fc460f1094b3
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://1948c4b477fbb4f44cdb787e5190e706dbcd347b8cc1ff1757d19661480a1b99
          exitCode: 0
          finishedAt: "2025-03-30T08:48:09Z"
          reason: Completed
          startedAt: "2025-03-30T08:48:09Z"
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fsk42
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-03-30T08:48:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-30T08:48:11Z"
    generateName: rook-ceph-osd-prepare-k3s-w-1-
    labels:
      app: rook-ceph-osd-prepare
      batch.kubernetes.io/controller-uid: 0fde204e-d36e-4b00-ad56-da70c7c7a2e0
      batch.kubernetes.io/job-name: rook-ceph-osd-prepare-k3s-w-1
      ceph.rook.io/pvc: ""
      controller-uid: 0fde204e-d36e-4b00-ad56-da70c7c7a2e0
      job-name: rook-ceph-osd-prepare-k3s-w-1
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-k3s-w-1-9h55x
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-k3s-w-1
      uid: 0fde204e-d36e-4b00-ad56-da70c7c7a2e0
    resourceVersion: "11099893"
    uid: 037c7d0d-ff6e-49e0-a463-1236766b144c
  spec:
    affinity: {}
    containers:
    - args:
      - ceph
      - osd
      - provision
      command:
      - /rook/rook
      env:
      - name: ROOK_NODE_NAME
        value: k3s-w-1
      - name: ROOK_CLUSTER_ID
        value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: ROOK_OSD_STORE_TYPE
        value: bluestore
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k3s-w-1
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: all
      - name: ROOK_CEPH_VERSION
        value: ceph version 18.2.2-0 reef
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: provision
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-djxpm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --archive
      - --force
      - --verbose
      - /usr/local/bin/rook
      - /rook
      command:
      - cp
      image: rook/ceph:v1.14.8
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-djxpm
        readOnly: true
    nodeName: k3s-w-1
    nodeSelector:
      kubernetes.io/hostname: k3s-w-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: kube-api-access-djxpm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:48Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:15Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:46Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:46Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3c595a270ae81794eac70c073e6af0b708b77490e02c7fec78bec192904c6273
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3c595a270ae81794eac70c073e6af0b708b77490e02c7fec78bec192904c6273
          exitCode: 0
          finishedAt: "2025-03-30T08:48:46Z"
          reason: Completed
          startedAt: "2025-03-30T08:48:16Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-djxpm
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.237
    hostIPs:
    - ip: 192.168.50.237
    initContainerStatuses:
    - containerID: containerd://cf538ec667e34c52bdc80c20730529d4018fd5deab46f0166c81fb1efb564c01
      image: docker.io/rook/ceph:v1.14.8
      imageID: docker.io/rook/ceph@sha256:b63db448a08f313fc88dedb89d3d713fe4576dc72a3f458e7427fc460f1094b3
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://cf538ec667e34c52bdc80c20730529d4018fd5deab46f0166c81fb1efb564c01
          exitCode: 0
          finishedAt: "2025-03-30T08:48:14Z"
          reason: Completed
          startedAt: "2025-03-30T08:48:12Z"
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-djxpm
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-03-30T08:48:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-30T08:46:09Z"
    generateName: rook-ceph-osd-prepare-k3s-w-2-
    labels:
      app: rook-ceph-osd-prepare
      batch.kubernetes.io/controller-uid: c4c19d1e-46a1-46bf-a3e4-bfd573e5a7f1
      batch.kubernetes.io/job-name: rook-ceph-osd-prepare-k3s-w-2
      ceph.rook.io/pvc: ""
      controller-uid: c4c19d1e-46a1-46bf-a3e4-bfd573e5a7f1
      job-name: rook-ceph-osd-prepare-k3s-w-2
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-k3s-w-2-wxdvl
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-k3s-w-2
      uid: c4c19d1e-46a1-46bf-a3e4-bfd573e5a7f1
    resourceVersion: "11084590"
    uid: 2f1ad342-774a-4f30-a45d-69058f568bd8
  spec:
    affinity: {}
    containers:
    - args:
      - ceph
      - osd
      - provision
      command:
      - /rook/rook
      env:
      - name: ROOK_NODE_NAME
        value: k3s-w-2
      - name: ROOK_CLUSTER_ID
        value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: ROOK_OSD_STORE_TYPE
        value: bluestore
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k3s-w-2
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: all
      - name: ROOK_CEPH_VERSION
        value: ceph version 18.2.2-0 reef
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: provision
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v89bb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --archive
      - --force
      - --verbose
      - /usr/local/bin/rook
      - /rook
      command:
      - cp
      image: rook/ceph:v1.14.8
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v89bb
        readOnly: true
    nodeName: k3s-w-2
    nodeSelector:
      kubernetes.io/hostname: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: kube-api-access-v89bb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:19Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:46:50Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:47:57Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:47:57Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:46:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1cb5aaa17ba5a131cd40b71d38651689c54659e98143f90ebf5f997bb047bfd5
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://1cb5aaa17ba5a131cd40b71d38651689c54659e98143f90ebf5f997bb047bfd5
          exitCode: 0
          finishedAt: "2025-03-30T08:47:21Z"
          reason: Completed
          startedAt: "2025-03-30T08:46:52Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v89bb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    initContainerStatuses:
    - containerID: containerd://9b152b9ad9720464f8ab0e33b2719eb224bb08cd7709feb0311be1afa42adc6c
      image: docker.io/rook/ceph:v1.14.8
      imageID: docker.io/rook/ceph@sha256:b63db448a08f313fc88dedb89d3d713fe4576dc72a3f458e7427fc460f1094b3
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://9b152b9ad9720464f8ab0e33b2719eb224bb08cd7709feb0311be1afa42adc6c
          exitCode: 0
          finishedAt: "2025-03-30T08:46:12Z"
          reason: Completed
          startedAt: "2025-03-30T08:46:12Z"
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v89bb
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-03-30T08:46:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-30T08:48:14Z"
    generateName: rook-ceph-osd-prepare-k3s-w-3-
    labels:
      app: rook-ceph-osd-prepare
      batch.kubernetes.io/controller-uid: 94908c75-685c-4f74-ad29-e0e9f24ec701
      batch.kubernetes.io/job-name: rook-ceph-osd-prepare-k3s-w-3
      ceph.rook.io/pvc: ""
      controller-uid: 94908c75-685c-4f74-ad29-e0e9f24ec701
      job-name: rook-ceph-osd-prepare-k3s-w-3
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-k3s-w-3-njkqd
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-k3s-w-3
      uid: 94908c75-685c-4f74-ad29-e0e9f24ec701
    resourceVersion: "11084771"
    uid: 940053f4-aa37-4a89-b7a0-90ad663c3a51
  spec:
    affinity: {}
    containers:
    - args:
      - ceph
      - osd
      - provision
      command:
      - /rook/rook
      env:
      - name: ROOK_NODE_NAME
        value: k3s-w-3
      - name: ROOK_CLUSTER_ID
        value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: ROOK_OSD_STORE_TYPE
        value: bluestore
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: k3s-w-3
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: all
      - name: ROOK_CEPH_VERSION
        value: ceph version 18.2.2-0 reef
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      envFrom:
      - configMapRef:
          name: rook-ceph-osd-env-override
          optional: true
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: provision
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c5nhb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --archive
      - --force
      - --verbose
      - /usr/local/bin/rook
      - /rook
      command:
      - cp
      image: rook/ceph:v1.14.8
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c5nhb
        readOnly: true
    nodeName: k3s-w-3
    nodeSelector:
      kubernetes.io/hostname: k3s-w-3
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - name: ceph-admin-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret.keyring
        secretName: rook-ceph-mon
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: kube-api-access-c5nhb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:26Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:15Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:25Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:25Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-30T08:48:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://16ce09fce16770c9f9e85b571b63d591ca1f955c22a411ccdf7879942848d258
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://16ce09fce16770c9f9e85b571b63d591ca1f955c22a411ccdf7879942848d258
          exitCode: 0
          finishedAt: "2025-03-30T08:48:24Z"
          reason: Completed
          startedAt: "2025-03-30T08:48:16Z"
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /run/ceph
        name: ceph-daemons-sock-dir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/lib/rook-ceph-mon
        name: ceph-admin-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c5nhb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.108
    hostIPs:
    - ip: 192.168.50.108
    initContainerStatuses:
    - containerID: containerd://f453e78895037fcdd4369212e1518eb0c4a8badc6afaf240890776c65bf3da03
      image: docker.io/rook/ceph:v1.14.8
      imageID: docker.io/rook/ceph@sha256:b63db448a08f313fc88dedb89d3d713fe4576dc72a3f458e7427fc460f1094b3
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://f453e78895037fcdd4369212e1518eb0c4a8badc6afaf240890776c65bf3da03
          exitCode: 0
          finishedAt: "2025-03-30T08:48:15Z"
          reason: Completed
          startedAt: "2025-03-30T08:48:15Z"
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c5nhb
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-03-30T08:48:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-08T14:01:27Z"
    generateName: rook-ceph-tools-95c4b57b6-
    labels:
      app: rook-ceph-tools
      pod-template-hash: 95c4b57b6
    name: rook-ceph-tools-95c4b57b6-xlmcl
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-tools-95c4b57b6
      uid: 8505abe8-fee5-4c63-be11-2d28708e5956
    resourceVersion: "6013892"
    uid: 135bc2fc-8318-43ed-bb95-da6fef839c25
  spec:
    containers:
    - args:
      - -c
      - "# Wait for the Ceph configuration to be available\nwhile [ ! -f /etc/ceph/ceph.conf
        ]; do\n  echo 'Waiting for Ceph configuration to be available...'\n  \n  #
        Get monitor endpoints\n  MONS=$(grep -v '^#' /etc/rook/mon-endpoints | tr
        ',' ' ' | sed 's/=/ /g' | awk '{print $2}' | tr '\\n' ',' | sed 's/,$//')\n
        \ \n  # Create initial Ceph configuration\n  if [ ! -z \"$MONS\" ]; then\n
        \   echo '[global]' > /etc/ceph/ceph.conf\n    echo 'mon_host = '$MONS >>
        /etc/ceph/ceph.conf\n    echo 'auth_cluster_required = cephx' >> /etc/ceph/ceph.conf\n
        \   echo 'auth_service_required = cephx' >> /etc/ceph/ceph.conf\n    echo
        'auth_client_required = cephx' >> /etc/ceph/ceph.conf\n    \n    # Create
        keyring file\n    SECRET=$(cat /etc/ceph/secret)\n    echo '[client.admin]'
        > /etc/ceph/ceph.client.admin.keyring\n    echo '  key = '$SECRET >> /etc/ceph/ceph.client.admin.keyring\n
        \ fi\n  \n  sleep 5\ndone\n\necho 'Ceph configuration is available.'\n\n#
        Keep the container running\nwhile true; do\n  sleep 5\ndone"
      command:
      - /bin/bash
      image: quay.io/ceph/ceph:v18.2.2
      imagePullPolicy: IfNotPresent
      name: rook-ceph-tools
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: ceph-config
      - mountPath: /etc/rook
        name: mon-endpoint-volume
      - mountPath: /etc/ceph/secret
        name: ceph-secret
        subPath: secret
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gr8sq
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: k3s-m-1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: data
          path: mon-endpoints
        name: rook-ceph-mon-endpoints
      name: mon-endpoint-volume
    - emptyDir: {}
      name: ceph-config
    - name: ceph-secret
      secret:
        defaultMode: 420
        items:
        - key: ceph-secret
          path: secret
        secretName: rook-ceph-mon
    - name: kube-api-access-gr8sq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:29Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-08T14:01:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-09T20:43:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-08T14:01:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f8c853d8cca4b942b2e1573e3d7c2372aa94eb28a57f2c40cb2ddd0067dff688
      image: quay.io/ceph/ceph:v18.2.2
      imageID: quay.io/ceph/ceph@sha256:f8d467dcf49d13b8ea42229d89be642581110175d8ce36e216aefc9b32b0854d
      lastState:
        terminated:
          containerID: containerd://affb97e62a6004c411984b425efd8f2b38b308dff1e08288d1e8d9edce107e3e
          exitCode: 255
          finishedAt: "2025-03-09T20:21:08Z"
          reason: Unknown
          startedAt: "2025-03-08T14:01:28Z"
      name: rook-ceph-tools
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-09T20:43:11Z"
      volumeMounts:
      - mountPath: /etc/ceph
        name: ceph-config
      - mountPath: /etc/rook
        name: mon-endpoint-volume
      - mountPath: /etc/ceph/secret
        name: ceph-secret
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gr8sq
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.75
    hostIPs:
    - ip: 192.168.50.75
    phase: Running
    podIP: 10.42.0.215
    podIPs:
    - ip: 10.42.0.215
    qosClass: BestEffort
    startTime: "2025-03-08T14:01:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-04-02T19:37:24Z"
    generateName: uptime-kuma-844847fb6b-
    labels:
      app: uptime-kuma
      pod-template-hash: 844847fb6b
    name: uptime-kuma-844847fb6b-t9mdv
    namespace: uptime-kuma
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: uptime-kuma-844847fb6b
      uid: 996acf53-0fa7-4f29-b0e4-cf1f27b9f337
    resourceVersion: "11100220"
    uid: b6afe2d5-8527-4b2e-8016-180bdccb8787
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: NotIn
              values:
              - "true"
            - key: node-role.kubernetes.io/master
              operator: NotIn
              values:
              - "true"
          weight: 100
    containers:
    - env:
      - name: TZ
        value: Europe/Stockholm
      image: louislam/uptime-kuma:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: uptime-kuma
      ports:
      - containerPort: 3001
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 400m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j782c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: k3s-w-2
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: uptime-kuma-data
    - name: kube-api-access-j782c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T19:37:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T19:37:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-04-05T12:04:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-04-02T19:37:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b2519d1d8b785ad1bdf148a80bcfdc8490f0af0118f7121a293b9975fe493d43
      image: docker.io/louislam/uptime-kuma:latest
      imageID: docker.io/louislam/uptime-kuma@sha256:431fee3be822b04861cf0e35daf4beef6b7cb37391c5f26c3ad6e12ce280fe18
      lastState:
        terminated:
          containerID: containerd://11d2bb186a420fb6abbeab1c7c39798f0a5974cebd67744f635b8bde10e2853f
          exitCode: 137
          finishedAt: "2025-04-05T12:03:30Z"
          reason: Error
          startedAt: "2025-04-02T19:37:25Z"
      name: uptime-kuma
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-04-05T12:03:43Z"
      volumeMounts:
      - mountPath: /app/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j782c
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.50.115
    hostIPs:
    - ip: 192.168.50.115
    phase: Running
    podIP: 10.42.2.7
    podIPs:
    - ip: 10.42.2.7
    qosClass: Burstable
    startTime: "2025-04-02T19:37:24Z"
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-15T13:41:51Z"
    labels:
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: bitwarden
    namespace: bitwarden
    resourceVersion: "951086"
    uid: c596e705-291d-4732-b40a-afce077827e2
  spec:
    clusterIP: 10.43.180.251
    clusterIPs:
    - 10.43.180.251
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
    - name: websocket
      port: 3012
      protocol: TCP
      targetPort: 3012
    selector:
      app: bitwarden
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T19:38:21Z"
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      helm.toolkit.fluxcd.io/name: cert-manager
      helm.toolkit.fluxcd.io/namespace: cert-manager
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "467137"
    uid: bfa20c70-ea1b-4389-bdbb-fd88ace150e5
  spec:
    clusterIP: 10.43.234.53
    clusterIPs:
    - 10.43.234.53
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T20:36:43Z"
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      helm.toolkit.fluxcd.io/name: cert-manager
      helm.toolkit.fluxcd.io/namespace: cert-manager
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "467136"
    uid: 8990e2de-eb38-440c-a6c8-c62314239a26
  spec:
    clusterIP: 10.43.51.85
    clusterIPs:
    - 10.43.51.85
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T19:38:21Z"
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      helm.toolkit.fluxcd.io/name: cert-manager
      helm.toolkit.fluxcd.io/namespace: cert-manager
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "467142"
    uid: f7977a07-4771-4538-8854-30d7309aad5f
  spec:
    clusterIP: 10.43.16.116
    clusterIPs:
    - 10.43.16.116
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    - name: metrics
      port: 9402
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-18T20:14:12Z"
    labels:
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: kubelet
    namespace: default
    resourceVersion: "7896329"
    uid: e338a61c-fc0e-4fb1-b282-30931eb03912
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-08T22:04:20Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "195"
    uid: 4776d47b-9246-4f0f-a4aa-a6531fb280ee
  spec:
    clusterIP: 10.43.0.1
    clusterIPs:
    - 10.43.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"openhab-production"},"name":"openhab-production","namespace":"default"},"spec":{"ports":[{"name":"openhab-http","port":8080,"protocol":"TCP","targetPort":8080},{"name":"https","port":8443,"protocol":"TCP","targetPort":8443},{"name":"openhab-console","port":8101,"protocol":"TCP","targetPort":8101}],"selector":{"app":"openhab-production"},"type":"NodePort"}}
    creationTimestamp: "2025-02-11T19:53:58Z"
    labels:
      app: openhab-production
    name: openhab-production
    namespace: default
    resourceVersion: "301331"
    uid: e5159df6-8f40-404b-84dd-abfb6f806fb5
  spec:
    clusterIP: 10.43.166.75
    clusterIPs:
    - 10.43.166.75
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: openhab-http
      nodePort: 30830
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: https
      nodePort: 32568
      port: 8443
      protocol: TCP
      targetPort: 8443
    - name: openhab-console
      nodePort: 30308
      port: 8101
      protocol: TCP
      targetPort: 8101
    selector:
      app: openhab-production
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-10T19:24:00Z"
    labels:
      app.kubernetes.io/component: notification-controller
      app.kubernetes.io/instance: flux-system
      app.kubernetes.io/part-of: flux
      app.kubernetes.io/version: v2.4.0
      control-plane: controller
    name: notification-controller
    namespace: flux-system
    resourceVersion: "134631"
    uid: 3717ead1-383c-44ca-9226-cc82c703648b
  spec:
    clusterIP: 10.43.246.148
    clusterIPs:
    - 10.43.246.148
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app: notification-controller
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-10T19:24:00Z"
    labels:
      app.kubernetes.io/component: source-controller
      app.kubernetes.io/instance: flux-system
      app.kubernetes.io/part-of: flux
      app.kubernetes.io/version: v2.4.0
      control-plane: controller
    name: source-controller
    namespace: flux-system
    resourceVersion: "134637"
    uid: 88a8912a-87b8-452e-bb0d-e60de9fc5589
  spec:
    clusterIP: 10.43.169.2
    clusterIPs:
    - 10.43.169.2
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app: source-controller
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-10T19:24:00Z"
    labels:
      app.kubernetes.io/component: notification-controller
      app.kubernetes.io/instance: flux-system
      app.kubernetes.io/part-of: flux
      app.kubernetes.io/version: v2.4.0
      control-plane: controller
    name: webhook-receiver
    namespace: flux-system
    resourceVersion: "134641"
    uid: 0401d12a-c388-47f8-b4d7-8ca9692cb217
  spec:
    clusterIP: 10.43.49.148
    clusterIPs:
    - 10.43.49.148
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http-webhook
    selector:
      app: notification-controller
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ww-gitops
      meta.helm.sh/release-namespace: flux-system
    creationTimestamp: "2025-02-12T19:33:03Z"
    labels:
      app.kubernetes.io/instance: ww-gitops
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: weave-gitops
      app.kubernetes.io/version: v0.38.0
      helm.sh/chart: weave-gitops-4.0.36
      helm.toolkit.fluxcd.io/name: ww-gitops
      helm.toolkit.fluxcd.io/namespace: flux-system
    name: ww-gitops-weave-gitops
    namespace: flux-system
    resourceVersion: "459607"
    uid: cbbf84e2-c328-4be9-b466-25ccf01147df
  spec:
    clusterIP: 10.43.25.177
    clusterIPs:
    - 10.43.25.177
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9001
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: ww-gitops
      app.kubernetes.io/name: weave-gitops
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-23T19:27:01Z"
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: headlamp
      app.kubernetes.io/version: 0.28.1
      helm.sh/chart: headlamp-0.28.1
      helm.toolkit.fluxcd.io/name: headlamp
      helm.toolkit.fluxcd.io/namespace: headlamp
    name: headlamp
    namespace: headlamp
    resourceVersion: "2595818"
    uid: 32045227-e4f8-4e4a-91b9-994fd6375c9d
  spec:
    clusterIP: 10.43.249.78
    clusterIPs:
    - 10.43.249.78
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"k3s-kubelet"},"name":"k3s-kubelet-metrics","namespace":"kube-system"},"spec":{"clusterIP":"None","ports":[{"name":"https-metrics","port":10250,"protocol":"TCP","targetPort":10250}],"selector":{}}}
    creationTimestamp: "2025-04-04T20:45:44Z"
    labels:
      k8s-app: k3s-kubelet
    name: k3s-kubelet-metrics
    namespace: kube-system
    resourceVersion: "10987816"
    uid: f98b97c7-6c74-4594-8cdb-4cc67673004f
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"k3s"},"name":"k3s-metrics","namespace":"kube-system"},"spec":{"ports":[{"name":"https","port":10250,"protocol":"TCP","targetPort":10250}],"selector":{"node-role.kubernetes.io/control-plane":"true"}}}
    creationTimestamp: "2025-04-03T20:35:43Z"
    labels:
      app: k3s
      k8s-app: k3s
    name: k3s-metrics
    namespace: kube-system
    resourceVersion: "11111995"
    uid: 3c691be6-857a-4244-80cd-6403497f9fed
  spec:
    clusterIP: 10.43.68.144
    clusterIPs:
    - 10.43.68.144
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    selector:
      node-role.kubernetes.io/control-plane: "true"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4ySQYvbMBCF/0p5Z9m142TjFfRQdimUQgmk7aXsQZYnG9W2JKRJSgj+70WJl00b0vZm8958vHmjI5Q33yhE4ywk9iUEOmNbSKwp7I0mCAzEqlWsII9Q1jpWbJyN6dc1P0hzJM6DcblWzD3lxr01iQBxU3c/LYXsed9BoqvihbIvxZtPxrbv3rets/9EWDUQJLQL1Nr4X/bolU4z3a6hLB4i0wABH9xAvKVdTG7vAkPivlxUV1rUQfkE4LAjjAK9aqg/1dHVMVPev8DPidJnsMR0mtb9LjKFLE71Tpg/bdNeDy7Q4+f1X/baqriFRKNpVlez+7ouy+W8UkVV36lmURab2eZuSZvlfDYv9GKZ8k7si4i3ahkFoiedVptyf1xBoizyeZUXeVlAvAoR8vul9CRg/Ac1mP6wcr3Rh/SojH3uac1Kd6lXFzhNHV8indOcy19Up+LZaddD4uvjCqO4dGas/S33l4ff3ANxMPqVne567X8SiNSTZhduHHMcx18BAAD//5X9LCMyAwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-08T22:04:23Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: kube-dns
    namespace: kube-system
    resourceVersion: "261"
    uid: 613688cf-370a-4e4d-b6f7-4ad3fa68ea44
  spec:
    clusterIP: 10.43.0.10
    clusterIPs:
    - 10.43.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:11Z"
    labels:
      app: kube-prometheus-stack-coredns
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      jobLabel: coredns
      release: kube-prometheus-stack
    name: kube-prometheus-stack-coredns
    namespace: kube-system
    resourceVersion: "2611742"
    uid: 17f7bcd3-e483-4c66-91ca-1587fe0d022a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-02T20:58:53Z"
    labels:
      app: kube-prometheus-stack-kube-controller-manager
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      jobLabel: kube-controller-manager
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-controller-manager
    namespace: kube-system
    resourceVersion: "10639901"
    uid: 39605e0c-0f1e-418f-b214-ae64c1820a54
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:11Z"
    labels:
      app: kube-prometheus-stack-kube-etcd
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      jobLabel: kube-etcd
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-etcd
    namespace: kube-system
    resourceVersion: "2611739"
    uid: 2c4ab349-b13a-4e20-b3cf-962ac9345f1f
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
    selector:
      component: etcd
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-02T20:58:54Z"
    labels:
      app: kube-prometheus-stack-kube-proxy
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      jobLabel: kube-proxy
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-proxy
    namespace: kube-system
    resourceVersion: "10639902"
    uid: 7c26ff3e-71e2-4ba4-a21b-1daa879117aa
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10249
      protocol: TCP
      targetPort: 10249
    selector:
      k8s-app: pushprox-kube-proxy-client
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-02T20:58:54Z"
    labels:
      app: kube-prometheus-stack-kube-scheduler
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      jobLabel: kube-scheduler
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-scheduler
    namespace: kube-system
    resourceVersion: "10639905"
    uid: 9d10da20-c15a-4887-9384-97e53bca39bb
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-23T21:48:39Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: kube-prometheus-stack-kubelet
    namespace: kube-system
    resourceVersion: "2612132"
    uid: ba40f442-e721-4dc9-a512-4eede30ea2f5
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4SQQWsbMRCF/0p5Z9nNep04FfRQWnopBUNKL6WHWe04VleWhGa8xZj970UbFxLaJCchvZn3vqczKPvvXMSnCIuxgcHgYw+LOy6jdwyDAyv1pAR7BsWYlNSnKPWaul/sVFiXxaelI9XAS5/e+uoA86yefkcui/txgMXQyiNlbMybLz727z/0fYqvWkQ6MGxFLN7JQriMXObjgf31bcnkqsVw7HghJ1E+YDII1HGYO1ahRFaWuujCUfRRhIWWY016Onbh+vqE6wWePckeFnTdt527uWrc7abhZtXuqF11q83uev2uu2HabK46t1tTJfxvdTy8P1NKMrtayefPdPDhtE3BuxMstoV3XD4dKdwpuQEGORUV2B/nvzl71SwXAXa9bg1ySZpcCrD49nELA6Vyz7qdJy4L008D4cBOU5l/81YWlPO/4NM0/QkAAP//sKxN444CAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-service
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:23Z"
    labels:
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Metrics-server
      objectset.rio.cattle.io/hash: a5d3bc601c871e123fa32b27f549b6ea770bcf4a
    name: metrics-server
    namespace: kube-system
    resourceVersion: "306"
    uid: 1dcabd0b-7711-43c5-935c-c86ce43bbc3a
  spec:
    clusterIP: 10.43.43.41
    clusterIPs:
    - 10.43.43.41
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"pushprox-kube-proxy-client"},"name":"pushprox-kube-proxy-client","namespace":"kube-system"},"spec":{"ports":[{"name":"http","port":10249,"protocol":"TCP","targetPort":10249}],"selector":{"k8s-app":"pushprox-kube-proxy-client"},"type":"ClusterIP"}}
    creationTimestamp: "2025-04-05T11:46:16Z"
    labels:
      k8s-app: pushprox-kube-proxy-client
    name: pushprox-kube-proxy-client
    namespace: kube-system
    resourceVersion: "11097503"
    uid: 15d758ad-f4c6-47af-a257-d7ac3dffbdb8
  spec:
    clusterIP: 10.43.86.131
    clusterIPs:
    - 10.43.86.131
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 10249
      protocol: TCP
      targetPort: 10249
    selector:
      k8s-app: pushprox-kube-proxy-client
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: pushprox-kube-proxy
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-04-02T20:58:18Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      component: kube-proxy
      helm.toolkit.fluxcd.io/name: pushprox-kube-proxy
      helm.toolkit.fluxcd.io/namespace: kube-system
      k8s-app: pushprox-kube-proxy-proxy
      provider: kubernetes
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-proxy
    namespace: kube-system
    resourceVersion: "10974291"
    uid: 2b13a477-b19f-48c4-b721-6b689318bf05
  spec:
    clusterIP: 10.43.187.41
    clusterIPs:
    - 10.43.187.41
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: pp-proxy
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      component: kube-proxy
      k8s-app: pushprox-kube-proxy-proxy
      provider: kubernetes
      release: pushprox-kube-proxy
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:41Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
    name: traefik
    namespace: kube-system
    resourceVersion: "11099863"
    uid: 32c60abe-0675-4e74-a8d4-4385f735be77
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.43.202.202
    clusterIPs:
    - 10.43.202.202
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: web
      nodePort: 31479
      port: 80
      protocol: TCP
      targetPort: web
    - name: websecure
      nodePort: 32006
      port: 443
      protocol: TCP
      targetPort: websecure
    selector:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/name: traefik
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.50.108
        ipMode: VIP
      - ip: 192.168.50.115
        ipMode: VIP
      - ip: 192.168.50.237
        ipMode: VIP
      - ip: 192.168.50.75
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
    creationTimestamp: "2025-02-26T18:45:23Z"
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      helm.sh/chart: kubeshark-52.4
      helm.toolkit.fluxcd.io/name: kubeshark
      helm.toolkit.fluxcd.io/namespace: kubeshark
    name: kubeshark-front
    namespace: kubeshark
    resourceVersion: "3658780"
    uid: 3c1b9d3b-a9fc-496a-8c1b-c5632a4d5d2e
  spec:
    clusterIP: 10.43.30.156
    clusterIPs:
    - 10.43.30.156
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: kubeshark-front
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubeshark.co/app: front
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
    creationTimestamp: "2025-02-26T18:45:23Z"
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: hub
      helm.sh/chart: kubeshark-52.4
      helm.toolkit.fluxcd.io/name: kubeshark
      helm.toolkit.fluxcd.io/namespace: kubeshark
    name: kubeshark-hub
    namespace: kubeshark
    resourceVersion: "3658776"
    uid: 6bdb7d7f-80a2-487b-8069-de4fcedd2f13
  spec:
    clusterIP: 10.43.57.230
    clusterIPs:
    - 10.43.57.230
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: kubeshark-hub
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubeshark.co/app: hub
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
      prometheus.io/port: "9100"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-26T18:45:23Z"
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      helm.sh/chart: kubeshark-52.4
      helm.toolkit.fluxcd.io/name: kubeshark
      helm.toolkit.fluxcd.io/namespace: kubeshark
    name: kubeshark-hub-metrics
    namespace: kubeshark
    resourceVersion: "3658782"
    uid: cbfca0ed-115d-45dc-91a6-bbc54eb4ea05
  spec:
    clusterIP: 10.43.1.171
    clusterIPs:
    - 10.43.1.171
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: hub
      helm.sh/chart: kubeshark-52.4
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
      prometheus.io/port: "49100"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-26T18:45:23Z"
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      helm.sh/chart: kubeshark-52.4
      helm.toolkit.fluxcd.io/name: kubeshark
      helm.toolkit.fluxcd.io/namespace: kubeshark
    name: kubeshark-worker-metrics
    namespace: kubeshark
    resourceVersion: "3658778"
    uid: 92614a4a-1da6-412b-9c89-ec51c0e20ad5
  spec:
    clusterIP: 10.43.175.208
    clusterIPs:
    - 10.43.175.208
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 49100
      protocol: TCP
      targetPort: 49100
    selector:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: worker
      helm.sh/chart: kubeshark-52.4
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: loki
    creationTimestamp: "2025-03-30T10:32:05Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      helm.toolkit.fluxcd.io/name: loki
      helm.toolkit.fluxcd.io/namespace: loki
      heritage: Helm
      release: loki
    name: loki
    namespace: loki
    resourceVersion: "10040387"
    uid: c82e4ceb-9a11-4ff2-b2ad-5416cca59a66
  spec:
    clusterIP: 10.43.19.150
    clusterIPs:
    - 10.43.19.150
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: loki
    creationTimestamp: "2025-03-30T10:32:05Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      helm.toolkit.fluxcd.io/name: loki
      helm.toolkit.fluxcd.io/namespace: loki
      heritage: Helm
      release: loki
      variant: headless
    name: loki-headless
    namespace: loki
    resourceVersion: "10040386"
    uid: 2b62c321-331f-4e0c-b1f2-d0cf3967412a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: loki
    creationTimestamp: "2025-03-30T10:32:05Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      helm.toolkit.fluxcd.io/name: loki
      helm.toolkit.fluxcd.io/namespace: loki
      heritage: Helm
      release: loki
    name: loki-memberlist
    namespace: loki
    resourceVersion: "10040384"
    uid: 045a75c7-a575-4ee1-8d8f-067764d2217a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 7946
      protocol: TCP
      targetPort: memberlist-port
    publishNotReadyAddresses: true
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-23T21:49:24Z"
    labels:
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: kube-prometheus-stack-alertmanager
      uid: 8537e4f7-60d8-4ff9-a3a6-1c60a113c611
    resourceVersion: "2612209"
    uid: 880f1ede-05b2-4a88-8784-5d87f0cdda53
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: 9094
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: 9094
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-28T18:57:07Z"
    labels:
      app: k3s
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: k3s-metrics
    namespace: monitoring
    resourceVersion: "10809443"
    uid: 011b0d2c-76e0-411f-a862-d84c886fe482
  spec:
    clusterIP: 10.43.45.61
    clusterIPs:
    - 10.43.45.61
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 10250
      protocol: TCP
      targetPort: 10250
    selector:
      node-role.kubernetes.io/control-plane: "true"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:11Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      release: kube-prometheus-stack
      self-monitor: "true"
    name: kube-prometheus-stack-alertmanager
    namespace: monitoring
    resourceVersion: "2611788"
    uid: 7055cdcf-af3e-4a86-9c23-67e675101d71
  spec:
    clusterIP: 10.43.205.89
    clusterIPs:
    - 10.43.205.89
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: kube-prometheus-stack-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:11Z"
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.2.2
      helm.sh/chart: grafana-7.0.17
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    resourceVersion: "2611765"
    uid: 79a3f994-89ab-49e5-83a8-e069863a49b7
  spec:
    clusterIP: 10.43.193.38
    clusterIPs:
    - 10.43.193.38
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-23T21:46:11Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    resourceVersion: "2611802"
    uid: ada47fb5-6acf-488f-99ee-ab5010eeddf0
  spec:
    clusterIP: 10.43.210.235
    clusterIPs:
    - 10.43.210.235
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:11Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator
    namespace: monitoring
    resourceVersion: "2611816"
    uid: f73c2dda-f18c-4a28-a941-5fb8685f6561
  spec:
    clusterIP: 10.43.159.148
    clusterIPs:
    - 10.43.159.148
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: kube-prometheus-stack
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:11Z"
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      release: kube-prometheus-stack
      self-monitor: "true"
    name: kube-prometheus-stack-prometheus
    namespace: monitoring
    resourceVersion: "2611751"
    uid: f0b7ff5e-c070-4991-bf2a-2c6d2a379728
  spec:
    clusterIP: 10.43.134.54
    clusterIPs:
    - 10.43.134.54
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: kube-prometheus-stack-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-23T21:46:11Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      helm.sh/chart: prometheus-node-exporter-4.24.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      jobLabel: node-exporter
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "2611779"
    uid: f4974455-9352-4a5a-a5c9-f2e8347a5b6c
  spec:
    clusterIP: 10.43.10.206
    clusterIPs:
    - 10.43.10.206
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-23T21:49:43Z"
    labels:
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: kube-prometheus-stack-prometheus
      uid: b20ea06a-5b57-4c14-bf85-7c9d771bb2bf
    resourceVersion: "2612244"
    uid: 07bf8869-c924-4719-a20b-899a4bea6a93
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-15T12:57:34Z"
    labels:
      app: mosquitto
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: mosquitto
    namespace: mosquitto
    resourceVersion: "899350"
    uid: 1a1e686b-1116-4164-a442-bf7fc25264d2
  spec:
    clusterIP: 10.43.222.113
    clusterIPs:
    - 10.43.222.113
    externalIPs:
    - 192.168.50.75
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: mosquitto
      port: 1883
      protocol: TCP
      targetPort: 1883
    selector:
      app: mosquitto
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-10T19:49:31Z"
    labels:
      app: openhab-production
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: openhab-production
    namespace: openhab
    resourceVersion: "791220"
    uid: 36bb8d7f-0faf-4495-971b-1d3c257d6ae6
  spec:
    clusterIP: 10.43.132.185
    clusterIPs:
    - 10.43.132.185
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: openhab-http
      nodePort: 30080
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: https
      nodePort: 31872
      port: 8443
      protocol: TCP
      targetPort: 8443
    - name: openhab-console
      nodePort: 30720
      port: 8101
      protocol: TCP
      targetPort: 8101
    selector:
      app: openhab-production
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-17T21:35:50Z"
    labels:
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: ceph-dashboard-http
    namespace: rook-ceph
    resourceVersion: "7685672"
    uid: 15cecf42-38b7-47f2-8db3-d37c7eb8f496
  spec:
    clusterIP: 10.43.63.102
    clusterIPs:
    - 10.43.63.102
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-dashboard
      port: 7000
      protocol: TCP
      targetPort: 7000
    selector:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-08T15:02:28Z"
    labels:
      app: rook-ceph-exporter
      rook_cluster: rook-ceph
    name: rook-ceph-exporter
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "5751643"
    uid: dec39a6a-e3ac-4784-85a8-ea6c92acce1a
  spec:
    clusterIP: 10.43.230.216
    clusterIPs:
    - 10.43.230.216
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: ceph-exporter-http-metrics
      port: 9926
      protocol: TCP
      targetPort: 9926
    selector:
      app: rook-ceph-exporter
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-08T15:03:12Z"
    labels:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
    name: rook-ceph-mgr
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "5751833"
    uid: f43de2d8-c615-4d14-adb5-cb4d8ce71575
  spec:
    clusterIP: 10.43.196.135
    clusterIPs:
    - 10.43.196.135
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9283
      protocol: TCP
      targetPort: 9283
    selector:
      app: rook-ceph-mgr
      mgr_role: active
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-08T15:01:27Z"
    labels:
      app: rook-ceph-mgr
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-dashboard
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "10027537"
    uid: b7ea6651-8c79-4a1e-9a15-62ee3747a228
  spec:
    clusterIP: 10.43.143.3
    clusterIPs:
    - 10.43.143.3
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-dashboard
      port: 7000
      protocol: TCP
      targetPort: 7000
    - name: dashboard
      port: 8443
      protocol: TCP
      targetPort: 8443
    selector:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-08T14:57:30Z"
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "5750850"
    uid: 28331616-bd51-4e29-b64b-18a079ffcbe2
  spec:
    clusterIP: 10.43.130.194
    clusterIPs:
    - 10.43.130.194
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: a
      mon: a
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-11T18:40:25Z"
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "6457608"
    uid: eea50819-a347-4cbb-81a4-3d06e6fd805e
  spec:
    clusterIP: 10.43.219.20
    clusterIPs:
    - 10.43.219.20
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: e
      mon: e
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-26T15:01:31Z"
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: h
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: h
      ceph_daemon_type: mon
      mon: h
      mon_cluster: rook-ceph
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-h
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "9278334"
    uid: 7ee7c7bf-2622-4fc1-8971-6ffd7b4c9e1a
  spec:
    clusterIP: 10.43.167.194
    clusterIPs:
    - 10.43.167.194
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: h
      mon: h
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-29T14:40:30Z"
    labels:
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: uptime-kuma
    namespace: uptime-kuma
    resourceVersion: "9888258"
    uid: c1e6e979-af66-4a7a-993b-74b116f10eb0
  spec:
    clusterIP: 10.43.1.48
    clusterIPs:
    - 10.43.1.48
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 3001
    selector:
      app: uptime-kuma
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "7"
      meta.helm.sh/release-name: pushprox-kube-proxy
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-04-02T20:58:18Z"
    generation: 7
    labels:
      app.kubernetes.io/managed-by: Helm
      component: kube-proxy
      helm.toolkit.fluxcd.io/name: pushprox-kube-proxy
      helm.toolkit.fluxcd.io/namespace: kube-system
      k8s-app: pushprox-kube-proxy-client
      provider: kubernetes
      pushprox-exporter: client
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-client
    namespace: kube-system
    resourceVersion: "11100264"
    uid: 0787b257-cf34-4cd7-ab68-49495a5d40dc
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        component: kube-proxy
        k8s-app: pushprox-kube-proxy-client
        provider: kubernetes
        release: pushprox-kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          component: kube-proxy
          k8s-app: pushprox-kube-proxy-client
          provider: kubernetes
          release: pushprox-kube-proxy
      spec:
        containers:
        - args:
          - --fqdn=$(HOST_IP)
          - --proxy-url=$(PROXY_URL)
          - --allow-port=10249
          command:
          - pushprox-client
          env:
          - name: HOST_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          - name: PROXY_URL
            value: http://pushprox-kube-proxy-proxy.kube-system.svc:8080
          image: rancher/pushprox-client:v0.1.0-rancher2-client
          imagePullPolicy: IfNotPresent
          name: pushprox-client
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: pushprox-kube-proxy-client
        serviceAccountName: pushprox-kube-proxy-client
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 7
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RV3W7jNhN9lQ9zLcvyz+ZzBPQiSIIiaNcxbG9vFkYwokYxa4okyJE2RqB3L8g4WaXrOEGBbmEYBsmZw+E54zOPgFb+Qc5LoyEHtNYP2xEksJO6hByukGqjV8SQQE2MJTJC/giotWFkabQPS1P8SYI9ceqkSQUyK0qlGcqAAcmb5+abJje4b3eQw7AdJf/7TerylxW5Vgp6N09jTZADO6RK7j4U7i2KkLNrChr4vWeqoUtAOIqPWcuaPGNtIdeNUgkoLEidfOIW/RZymE3OKlEIysbj8lOJ9KmYnJ2dTysaEVaT2bk4n2D1/1JAAr4Vwmh2Rily6W7ie2jalORJkWDjIIcKlad3UnwrfiDiA/FvMHGA8q1QxeAAOJiMxVmGRajkRKq3JAJT3+t/hBpZbH9/IRGtfRu86xJgqq1Cppjb67cPCHQS++dR2CMCGza1aTQfGvpCiLBamx1pyKO2CYRLUGpyHvKvj0C6jb+HelbLy7vF7XINCbSomrA1y6BLXgUsL+a/Xq96IVkaP8NXkVfXq/XdYnm7vu1Fri8XP8acui9G3Cz6t42ydDpJx9k4fKHbJCBrvA8nDrXYkhvulLSW3EAVeZul0/QcDjGLRqmFUVLsIYebam544ciTZnjpxKCmsINZBglY4/iJphfWFsYx5LMsga3x/H11LNsZNsKo52dvEnDkTeMEhQYKwpFonOT9pdFMDxwbDy0WUkmW9NRlZQn5V5hfr+8urj7fzGHTdYGe93WbTic/V7i/XfhfKRfKOCHddDrpaxeXRwH+NfU2AVyamKrQ+/nBAuMfehAceSCcZClQwdFb/N4LVr6vvyZOpW2nqbR3lXHf0JV93qHbxIL7tjDvOS8kwEaRe56wwRiqigRDDnOzElsqGxVcbUeB/1ijM4rSYEVOE5MPPlWjZ3JhMNqAFUfK9YP07GNj/BPIgycOrEJNbyI/YVweWLsoS6P9rVb74wmbYJqNLZFpxQ6Z7veB1mC9Ut9/iQdPw+Thi8YWpcJCEeSjMDD2NrC2fBUbTZiRmyi6aJwjzfOmLsg9P7SEPEugJC8dlceOdNz7LL0/sr0kLPeQZ133VwAAAP//5AvF/kEJAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:41Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 836fcbce022d5dae5b36694fe1eaf389c93af7dc
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-32c60abe
    namespace: kube-system
    resourceVersion: "11099862"
    uid: 9adcea51-40ac-44a8-9251-0a8abf5f9768
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-traefik-32c60abe
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-traefik-32c60abe
          svccontroller.k3s.cattle.io/svcname: traefik
          svccontroller.k3s.cattle.io/svcnamespace: kube-system
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "80"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "80"
          - name: DEST_IPS
            value: 10.43.202.202
          image: rancher/klipper-lb:v0.4.9
          imagePullPolicy: IfNotPresent
          name: lb-tcp-80
          ports:
          - containerPort: 80
            hostPort: 80
            name: lb-tcp-80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "443"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "443"
          - name: DEST_IPS
            value: 10.43.202.202
          image: rancher/klipper-lb:v0.4.9
          imagePullPolicy: IfNotPresent
          name: lb-tcp-443
          ports:
          - containerPort: 443
            hostPort: 443
            name: lb-tcp-443
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
    creationTimestamp: "2025-02-26T18:45:23Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: worker
      helm.sh/chart: kubeshark-52.4
      helm.toolkit.fluxcd.io/name: kubeshark
      helm.toolkit.fluxcd.io/namespace: kubeshark
      sidecar.istio.io/inject: "false"
    name: kubeshark-worker-daemon-set
    namespace: kubeshark
    resourceVersion: "11100132"
    uid: a091a521-aef5-450a-9cf8-31e95d6836aa
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kubeshark
        app.kubernetes.io/name: kubeshark
        app.kubeshark.co/app: worker
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-03-02T16:29:14+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kubeshark
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kubeshark
          app.kubernetes.io/version: "52.4"
          app.kubeshark.co/app: worker
          helm.sh/chart: kubeshark-52.4
        name: kubeshark-worker-daemon-set
        namespace: kubeshark
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux
        containers:
        - command:
          - ./worker
          - -i
          - any
          - -port
          - "48999"
          - -metrics-port
          - "49100"
          - -packet-capture
          - best
          - -loglevel
          - warning
          - -servicemesh
          - -procfs
          - /hostproc
          - -resolution-strategy
          - auto
          - -staletimeout
          - "30"
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: TCP_STREAM_CHANNEL_TIMEOUT_MS
            value: "10000"
          - name: TCP_STREAM_CHANNEL_TIMEOUT_SHOW
            value: "false"
          - name: KUBESHARK_CLOUD_API_URL
            value: https://api.kubeshark.co
          - name: PROFILING_ENABLED
            value: "false"
          - name: SENTRY_ENABLED
            value: "false"
          - name: SENTRY_ENVIRONMENT
            value: production
          image: docker.io/kubeshark/worker:v52.4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 48999
            timeoutSeconds: 1
          name: sniffer
          ports:
          - containerPort: 49100
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 48999
            timeoutSeconds: 1
          resources:
            limits:
              memory: 5Gi
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /hostproc
            name: proc
            readOnly: true
          - mountPath: /sys
            mountPropagation: HostToContainer
            name: sys
            readOnly: true
          - mountPath: /app/data
            name: data
        - command:
          - ./tracer
          - -procfs
          - /hostproc
          - -disable-tls-log
          - -loglevel
          - warning
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: PROFILING_ENABLED
            value: "false"
          - name: SENTRY_ENABLED
            value: "false"
          - name: SENTRY_ENVIRONMENT
            value: production
          image: docker.io/kubeshark/worker:v52.4
          imagePullPolicy: Always
          name: tracer
          resources:
            limits:
              memory: 5Gi
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /hostproc
            name: proc
            readOnly: true
          - mountPath: /sys
            mountPropagation: HostToContainer
            name: sys
            readOnly: true
          - mountPath: /app/data
            name: data
          - mountPath: /etc/os-release
            name: os-release
            readOnly: true
          - mountPath: /hostroot
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubeshark-service-account
        serviceAccountName: kubeshark-service-account
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /etc/os-release
            type: ""
          name: os-release
        - hostPath:
            path: /
            type: ""
          name: root
        - emptyDir:
            sizeLimit: 5000Mi
          name: data
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 2
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: loki
    creationTimestamp: "2025-03-30T10:32:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: promtail
      app.kubernetes.io/version: 2.9.3
      helm.sh/chart: promtail-6.15.5
      helm.toolkit.fluxcd.io/name: loki
      helm.toolkit.fluxcd.io/namespace: loki
    name: loki-promtail
    namespace: loki
    resourceVersion: "11100114"
    uid: dc0f1295-2e33-40e9-8428-98fc9db0fd47
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: promtail
    template:
      metadata:
        annotations:
          checksum/config: 4e697a34db1115f5776e3d42883d968d223d8474b8b0a42d933463a80606d4b5
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: promtail
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: NotIn
                  values:
                  - k3s-w-2
        containers:
        - args:
          - -config.file=/etc/promtail/promtail.yaml
          env:
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/grafana/promtail:2.9.3
          imagePullPolicy: IfNotPresent
          name: promtail
          ports:
          - containerPort: 3101
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 750m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/promtail
            name: config
          - mountPath: /run/promtail
            name: run
          - mountPath: /var/lib/docker/containers
            name: containers
            readOnly: true
          - mountPath: /var/log/pods
            name: pods
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 0
          runAsUser: 0
        serviceAccount: loki-promtail
        serviceAccountName: loki-promtail
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: loki-promtail
        - hostPath:
            path: /run/promtail
            type: ""
          name: run
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: containers
        - hostPath:
            path: /var/log/pods
            type: ""
          name: pods
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.7.0
      helm.sh/chart: prometheus-node-exporter-4.24.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      jobLabel: node-exporter
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "11100063"
    uid: 1e56b2b2-aac8-4ff1-be88-136c66dd2ed1
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.7.0
          helm.sh/chart: prometheus-node-exporter-4.24.0
          jobLabel: node-exporter
          release: kube-prometheus-stack
      spec:
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.7.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: kube-prometheus-stack-prometheus-node-exporter
        serviceAccountName: kube-prometheus-stack-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
    creationTimestamp: "2025-03-08T14:56:56Z"
    generation: 2
    name: csi-cephfsplugin
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 3e5f5635-f606-4b2d-a2c3-f46259c81f7e
    resourceVersion: "11099875"
    uid: 0196e9dd-6e93-4848-87ab-f8caf2071ee7
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-cephfsplugin
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin
          contains: csi-cephfsplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
        containers:
        - args:
          - --v=0
          - --csi-address=/csi/csi.sock
          - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
          env:
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
          imagePullPolicy: IfNotPresent
          name: driver-registrar
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /registration
            name: registration-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --nodeserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --forcecephkernelclient=true
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
              drop:
              - ALL
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /csi/mountinfo
            name: ceph-csi-mountinfo
          - mountPath: /var/lib/kubelet/plugins
            mountPropagation: Bidirectional
            name: csi-plugins-dir
          - mountPath: /var/lib/kubelet/pods
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/mount
            name: host-run-mount
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-plugin-sa
        serviceAccountName: rook-csi-cephfs-plugin-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/mountinfo
            type: DirectoryOrCreate
          name: ceph-csi-mountinfo
        - hostPath:
            path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: Directory
          name: csi-plugins-dir
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - hostPath:
            path: /run/mount
            type: ""
          name: host-run-mount
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 2
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "4"
    creationTimestamp: "2025-03-08T14:56:56Z"
    generation: 4
    name: csi-rbdplugin
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 3e5f5635-f606-4b2d-a2c3-f46259c81f7e
    resourceVersion: "11099890"
    uid: e1c88784-5d82-4dc5-8041-070a8f33e50e
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-rbdplugin
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin
          contains: csi-rbdplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
        containers:
        - args:
          - --v=0
          - --csi-address=/csi/csi.sock
          - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
          env:
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
          imagePullPolicy: IfNotPresent
          name: driver-registrar
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /registration
            name: registration-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --nodeserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          - --stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
              drop:
              - ALL
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /var/lib/kubelet/pods
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /var/lib/kubelet/plugins
            mountPropagation: Bidirectional
            name: plugin-mount-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-configs
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/mount
            name: host-run-mount
          - mountPath: /run/secrets/tokens
            name: oidc-token
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        hostPID: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-plugin-sa
        serviceAccountName: rook-csi-rbd-plugin-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: Directory
          name: plugin-mount-dir
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - name: ceph-csi-configs
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: csi-cluster-config-json
                  path: config.json
                name: rook-ceph-csi-config
            - configMap:
                items:
                - key: csi-mapping-config-json
                  path: cluster-mapping.json
                name: rook-ceph-csi-mapping-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - hostPath:
            path: /run/mount
            type: ""
          name: host-run-mount
        - name: oidc-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: ceph-csi-kms
                expirationSeconds: 3600
                path: oidc-token
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 4
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T19:38:22Z"
    generation: 2
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      helm.toolkit.fluxcd.io/name: cert-manager
      helm.toolkit.fluxcd.io/namespace: cert-manager
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "9409221"
    uid: 16a7cd95-1255-47e9-a762-59eb356a4be6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.16.2
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-12T19:38:22Z"
      lastUpdateTime: "2025-02-12T20:36:56Z"
      message: ReplicaSet "cert-manager-b6fd485d9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-27T03:42:11Z"
      lastUpdateTime: "2025-03-27T03:42:11Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T19:38:22Z"
    generation: 2
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      helm.toolkit.fluxcd.io/name: cert-manager
      helm.toolkit.fluxcd.io/namespace: cert-manager
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "5852056"
    uid: 9e918745-7e79-4a8b-9263-8a5356d7b9dd
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.16.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-12T19:38:22Z"
      lastUpdateTime: "2025-02-12T20:37:05Z"
      message: ReplicaSet "cert-manager-cainjector-dcc5966bc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-09T01:33:27Z"
      lastUpdateTime: "2025-03-09T01:33:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T19:38:22Z"
    generation: 2
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      helm.toolkit.fluxcd.io/name: cert-manager
      helm.toolkit.fluxcd.io/namespace: cert-manager
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "10640701"
    uid: 723a9bd6-a5cc-4d2b-aa13-bba5bb5f133f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-12T19:38:22Z"
      lastUpdateTime: "2025-02-12T20:37:04Z"
      message: ReplicaSet "cert-manager-webhook-dfb76c7bd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T21:04:14Z"
      lastUpdateTime: "2025-04-02T21:04:14Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "17"
    creationTimestamp: "2025-02-12T17:19:11Z"
    generation: 19
    labels:
      kustomize.toolkit.fluxcd.io/name: infrastructure
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: cloudflared
    namespace: cloudflare
    resourceVersion: "11099888"
    uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: cloudflared
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cloudflared
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-12T17:19:11Z"
      lastUpdateTime: "2025-02-16T11:14:55Z"
      message: ReplicaSet "cloudflared-c8b747df5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:36Z"
      lastUpdateTime: "2025-04-05T12:03:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 19
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-10T19:24:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: helm-controller
      app.kubernetes.io/instance: flux-system
      app.kubernetes.io/part-of: flux
      app.kubernetes.io/version: v2.4.0
      control-plane: controller
    name: helm-controller
    namespace: flux-system
    resourceVersion: "10640661"
    uid: 7f10ca95-4c9a-4215-9345-ab2b58e6fe6d
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: helm-controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: helm-controller
      spec:
        containers:
        - args:
          - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
          - --watch-all-namespaces=true
          - --log-level=info
          - --log-encoding=json
          - --enable-leader-election
          env:
          - name: RUNTIME_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.cpu
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.memory
          image: ghcr.io/fluxcd/helm-controller:v1.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 8080
            name: http-prom
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: temp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1337
        serviceAccount: helm-controller
        serviceAccountName: helm-controller
        terminationGracePeriodSeconds: 600
        volumes:
        - emptyDir: {}
          name: temp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-10T19:24:01Z"
      lastUpdateTime: "2025-02-10T19:24:29Z"
      message: ReplicaSet "helm-controller-7f788c795c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T21:04:11Z"
      lastUpdateTime: "2025-04-02T21:04:11Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-10T19:24:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: kustomize-controller
      app.kubernetes.io/instance: flux-system
      app.kubernetes.io/part-of: flux
      app.kubernetes.io/version: v2.4.0
      control-plane: controller
    name: kustomize-controller
    namespace: flux-system
    resourceVersion: "10640667"
    uid: 94bbf3e6-d673-4bcd-98a4-b6e4dca7386f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kustomize-controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: kustomize-controller
      spec:
        containers:
        - args:
          - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
          - --watch-all-namespaces=true
          - --log-level=info
          - --log-encoding=json
          - --enable-leader-election
          env:
          - name: RUNTIME_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.cpu
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.memory
          image: ghcr.io/fluxcd/kustomize-controller:v1.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 8080
            name: http-prom
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: temp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1337
        serviceAccount: kustomize-controller
        serviceAccountName: kustomize-controller
        terminationGracePeriodSeconds: 60
        volumes:
        - emptyDir: {}
          name: temp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-10T19:24:01Z"
      lastUpdateTime: "2025-02-10T19:24:21Z"
      message: ReplicaSet "kustomize-controller-b4f45fff6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T21:04:12Z"
      lastUpdateTime: "2025-04-02T21:04:12Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-10T19:24:01Z"
    generation: 1
    labels:
      app.kubernetes.io/component: notification-controller
      app.kubernetes.io/instance: flux-system
      app.kubernetes.io/part-of: flux
      app.kubernetes.io/version: v2.4.0
      control-plane: controller
    name: notification-controller
    namespace: flux-system
    resourceVersion: "10640716"
    uid: ddf2940b-c850-47e1-8124-297516d39341
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: notification-controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: notification-controller
      spec:
        containers:
        - args:
          - --watch-all-namespaces=true
          - --log-level=info
          - --log-encoding=json
          - --enable-leader-election
          env:
          - name: RUNTIME_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.cpu
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.memory
          image: ghcr.io/fluxcd/notification-controller:v1.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          - containerPort: 9292
            name: http-webhook
            protocol: TCP
          - containerPort: 8080
            name: http-prom
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: temp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1337
        serviceAccount: notification-controller
        serviceAccountName: notification-controller
        terminationGracePeriodSeconds: 10
        volumes:
        - emptyDir: {}
          name: temp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-10T19:24:01Z"
      lastUpdateTime: "2025-02-10T19:24:30Z"
      message: ReplicaSet "notification-controller-556b8867f8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T21:04:15Z"
      lastUpdateTime: "2025-04-02T21:04:15Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-10T19:24:01Z"
    generation: 1
    labels:
      app.kubernetes.io/component: source-controller
      app.kubernetes.io/instance: flux-system
      app.kubernetes.io/part-of: flux
      app.kubernetes.io/version: v2.4.0
      control-plane: controller
    name: source-controller
    namespace: flux-system
    resourceVersion: "10640743"
    uid: b4f9f035-fe6b-46e2-8083-3df2ba5ba478
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: source-controller
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: source-controller
      spec:
        containers:
        - args:
          - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
          - --watch-all-namespaces=true
          - --log-level=info
          - --log-encoding=json
          - --enable-leader-election
          - --storage-path=/data
          - --storage-adv-addr=source-controller.$(RUNTIME_NAMESPACE).svc.cluster.local.
          env:
          - name: RUNTIME_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: TUF_ROOT
            value: /tmp/.sigstore
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.cpu
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.memory
          image: ghcr.io/fluxcd/source-controller:v1.4.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          - containerPort: 8080
            name: http-prom
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1337
        serviceAccount: source-controller
        serviceAccountName: source-controller
        terminationGracePeriodSeconds: 10
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-10T19:24:01Z"
      lastUpdateTime: "2025-02-10T19:24:22Z"
      message: ReplicaSet "source-controller-77d6cd56c9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T21:04:17Z"
      lastUpdateTime: "2025-04-02T21:04:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ww-gitops
      meta.helm.sh/release-namespace: flux-system
    creationTimestamp: "2025-02-12T19:33:03Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: ww-gitops
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: weave-gitops
      app.kubernetes.io/part-of: weave-gitops
      app.kubernetes.io/version: v0.38.0
      helm.sh/chart: weave-gitops-4.0.36
      helm.toolkit.fluxcd.io/name: ww-gitops
      helm.toolkit.fluxcd.io/namespace: flux-system
      weave.works/app: weave-gitops-oss
    name: ww-gitops-weave-gitops
    namespace: flux-system
    resourceVersion: "5852041"
    uid: 65121c9d-1e50-4604-b577-1d4cc50ef892
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: ww-gitops
        app.kubernetes.io/name: weave-gitops
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: ww-gitops
          app.kubernetes.io/name: weave-gitops
          app.kubernetes.io/part-of: weave-gitops
          weave.works/app: weave-gitops-oss
      spec:
        containers:
        - args:
          - --log-level
          - info
          - --insecure
          env:
          - name: WEAVE_GITOPS_FEATURE_TENANCY
            value: "true"
          - name: WEAVE_GITOPS_FEATURE_CLUSTER
            value: "false"
          image: ghcr.io/weaveworks/wego-app:v0.38.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: weave-gitops
          ports:
          - containerPort: 9001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ww-gitops-weave-gitops
        serviceAccountName: ww-gitops-weave-gitops
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-12T19:33:03Z"
      lastUpdateTime: "2025-02-12T19:33:29Z"
      message: ReplicaSet "ww-gitops-weave-gitops-6bd7fbd8c4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-09T01:33:27Z"
      lastUpdateTime: "2025-03-09T01:33:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "13"
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-23T19:27:07Z"
    generation: 13
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: headlamp
      app.kubernetes.io/version: 0.28.1
      helm.sh/chart: headlamp-0.28.1
      helm.toolkit.fluxcd.io/name: headlamp
      helm.toolkit.fluxcd.io/namespace: headlamp
    name: headlamp
    namespace: headlamp
    resourceVersion: "11100013"
    uid: aa274b9e-79cb-405d-a788-e3477a9cd460
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - --plugins-dir=/headlamp/plugins
          - --kubeconfig=/home/headlamp/.config/Headlamp/kubeconfigs/config
          env:
          - name: KUBECONFIG
            value: /home/headlamp/.config/Headlamp/kubeconfigs/config
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/headlamp
            name: config-dir
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            mkdir -p /home/headlamp/.config/Headlamp/kubeconfigs/
            cat > /home/headlamp/.config/Headlamp/kubeconfigs/config << EOF
            apiVersion: v1
            kind: Config
            current-context: default
            clusters:
            - name: default
              cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: https://10.43.0.1:443
            contexts:
            - name: default
              context:
                cluster: default
                user: default
            users:
            - name: default
              user:
                tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
            EOF
            chown -R 100:101 /home/headlamp
          command:
          - /bin/sh
          - -c
          image: busybox
          imagePullPolicy: Always
          name: init-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/headlamp
            name: config-dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: config-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-23T19:27:20Z"
      lastUpdateTime: "2025-02-25T21:00:03Z"
      message: ReplicaSet "headlamp-66557f8596" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:38Z"
      lastUpdateTime: "2025-04-05T12:03:38Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 13
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QjbPboq3XqJNeiqCgqZHFNcXhkiMnRqD/vhhJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HG0ySGBtXAE5TNFb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfkk2DoRCtmiyeGRkaSQPLDfbp1GNLVZg05rM/iwc4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilpPYqq830H6vPIZHDJGyTYce0EBp7PFE8dWKlaQw1Lj6eTs9PVkkmXnL87U+GzySi1fZuPytHx1juX5i9MXY/3yXIh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5OIKJFzRQEVCvW1YenymwlJQfFuNp2acla41bXvlCMfYq7a6c2yli1tAh51ibAWy/MPh1hZR1rb3dxBy30pNDtQVGaHCvjMETIv9yDCiv5gFSTKyGBEbIeDSqN5CZKYxFuEjC1WgmjoJyuMIxqE4LA0gG8+82zk+z0ZAxDxLyxdk7W6C3k8K6cEc8Dxt4C1mzQYYzzQMuuoFIZ2wS8qgLGimwB+VkCFbP/A1n2vWK591GFynIFCXgKDPlkPJFL0RV2d/z26mouUhln2Cg7Rau2C9Tkigj5q3ECHoOhYr+USXCjNcZ4cHKWAJsaqeEH4GN9JBR6KffKzjtWL8/26AEZiEmThRyup8LwmZCUtT8Ou7p4NOx1dhBYIwej4yOBNwkEVIX5V5JL5PZB8WyS/azi3wt++gt6B4zUBI1da1txYOxbv6YgLZWdjz8a6IB/Nxj7Xe0b2RqP627QDtAeKVZA3QTD2wtyjHddmcpaup0HszEWV3gZtbLdPIa8VDZiAlp5tTTWsOmpqKIQ28wur77+9m42/bq4/PT53cWlOKUI5GVPWQs3bS/6n85uPxHx78biMGhyDg22CWzINjV+pMYNfVTL53zQ/cCOcNB9rjSrtI+EhxN2OX+cY6SbyFQfpOr+p89kvJHmKVzcO3mKpWqsmNhRgYuDeXg80ilCDta45k7uyAdDnfBWxTjrCfRqpNo2kTGkOhg2WlmQawobo/GN1lLM7FvjMVkMu0fzyz2sUYhdDPHdQxe7EhIgL0jhB5d3RppENMKyRM2Qw4wWusKisVJ5n0aqSgNZPDmuR5wXyKbeKof/aeZaSf2Pp7yRaj1ZWm0XXq7mgpy8KGbXMt30X/zyq1Sru8Uab3vzDQe871gec6soctcvCdxW6K5dVGxiafrnCqY0I94XKmz7PtqPxdKsPiovRAxjfXRduxcm2U2a/YoI2YNmVOBbEiX2qIclOe6bodz+wCjD6HxgcxyX7r1BXtpK2b1HnzJLe9O2bftPAAAA//8kyNqv/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:23Z"
    generation: 3
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: coredns
    namespace: kube-system
    resourceVersion: "11100003"
    uid: f768239a-b4bd-4651-ac2a-0359e7773df3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-26T21:00:08+01:00"
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-08T22:04:28Z"
      lastUpdateTime: "2025-02-08T22:04:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-08T22:04:28Z"
      lastUpdateTime: "2025-02-26T20:00:10Z"
      message: ReplicaSet "coredns-554b67d6c4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUXWvrRhD9K2WeJdmuE2MEfTBJSksTxySkL8GU8Wpkb7zaXXbHaoTRf7+MJeeDGyf3wn3SfsycPTPnjPaAXv9LIWpnIQf0Pg7qESSw1baAHC7JG9dUZBkSqIixQEbI94DWOkbWzkbZutUTKY7EWdAuU8hsKNNuoAUEkpP37n9LIV3XW8hhO45vbupR8ts/2hZ/zIrC2S8hLFYEORin0KSRXcA1/VBS9Kgkc7tbURqbyFRBm4DBFZlPS9tg3EAOo+m4HJ+ryXlZrtR4ODmbDMfl2bgcnU+HxVRNpvh7gaviTEDfkfTIm9QHV2tpPgXo7k/wiZ6UsAnUxf+lpcjmWleaIR8mEMmQYhckqEJWm+uXCtD706+2As4BmdbN4QFnjLbrB18gUwf2/GCxRm1wZQjyUZsAN1443r2LlXOqvDnmvXGL+QkufaHKWUZtKUTIH2VbVSiWfDzdvsgYxKdpqpwt9RoSGBCrQbfrP9lTdBaWCZCtD8i9KIvby//ms5ur+8Xs4goSqNHs6M/gKiFTajLFHZUv6wWyiH+sMXtVrm3bZQK6Ev/lENCqDYXBx5zzepgNs/EQ+oTFzpiFM1o1kMPf5dzxIlDshu8r79TO7Cq6cTvLXccqWfY837bhFas7SLtMaJdC3AftgubmwmCM8y6uc2FqXUGpCpq1QiPtplBrRTOl5KX5Z/zSPjbFLhgSYGcoHH8gj3vYkhR90cMfhj7eWtPIEHuJFGvD1bOOHKFN9kBlSYohh7m7VxsqdkYGvoM5UA3OUCZjFCwxRZlZMVVwJvUGLf1S5AojH3T4AHJ5VOdoZWn7DXpx0/ey9t5tT8vUtu23AAAA///s6eu+uAUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:23Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4
    name: local-path-provisioner
    namespace: kube-system
    resourceVersion: "5852196"
    uid: d5cc4dec-c4b7-4b71-9416-99b1cfd356bd
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.30
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-08T22:04:28Z"
      lastUpdateTime: "2025-02-08T22:04:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-08T22:04:28Z"
      lastUpdateTime: "2025-02-08T22:04:36Z"
      message: ReplicaSet "local-path-provisioner-5cf85fd84d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoHQrsQHRPiV6tIUFXqnUxUh4x3Ah9f2eWZpuIj//TS7SUovpelV9wUt9vPzmzfjmXtQwfyKkYx3UIAKgc53OSSwNa6EAq4wWL+v0DEkUCGrUrGC4h6Uc54VG+9I/vrlH6iZkM+i8WdaMVs8M/7cCAkkJ/f9Z4cxXe+2UMD2go52dnnyv1+MKwfDsvTuRQqnKoRCJEajKSWMO4xpeSz/ZQIKSgvLtl5iSntirOCQgFVLtE2Y21eUqhCeXfQd9o2iDRSAeYadbg/zC+xeqn6np1+XZVl2V69x2bl4vcq7/X6/uyrlvm/GAu36CYkUUIvAiDsjuZwYYh/316YyDEWWAKFFzT4KqFKsN9cvB3UQYo6Kcb1vyL21xq0/hlIxtkR3H53aKWPV0iIU+SEB3gfR9+ErrKxjFezjuaNC+gFzT1pyFLj2jpVxGAmKT/eg4lo+IE01Rk5LEwfnXAVIIE0JdR0xDT7yIM86vaxZFUMtchoirjBGLFNVlhGJUomIBm8dY3TKvp0m47unz4knbrQdU9SEqfMlpsSKa2puagCt/DQieVvL2xnkPWp22FKqTdhgTKk2jDSYX88W49HVZCy/s+Hit7fzyWI4ni06vcvFm9G7xWwyvHjVTb7gPvwQ6h9seefVI67TuzzFdhJ1xDaaDEeTYSdbTN9f/55fZL1vkT0DwW0CplJryW5UTm8wnlcmRi8Z+DrdxS476591IAFrduiQaBr9simolTK2jjjfRKSNtyUUFwlsmMMbZNkPiuURnsvBvyCBJiNFgxD/SW+wqa/JfD6dSVkZZ9goe4VW7WeovSsJisssgYDR+PJpKZenVWuNREeX5wmwqdDX/AX4nXctatqyfariaSOwqc6nc49qQ/TstbdQwHw0hcNtAhFVaX7KETm5/3lLnjvS+ReGyEOoo0ZqW9efNRI33zrUUECeZVUzdiof91BAP3tn2qYkL9jwfuQd410Tj7LWf55GszMW1zgmrWwznaBYKUvYWvTe2f0H7/n/xuJD7yw41rJbuyHdeCe7X619JIySiCw7JLDztq7wna/dQ74q+Zw+WNn2l4dkcRWk68DhVvITovGNYKuIblpEK6BtFDoaNlpZMR7jzmgcai3cNydKhr3F+Dh+P93DFsWg0QNNMzJJopXBFAQpnR/Gd0YMPiT3gKsVakn4jZ/pDZa1lR7W0jSSord4Jh0tOmQkGWVSndHbNFjl8D9lrhRxO0WfU94++t5GilXg/ZWRQXb4ltuHw+HvAAAA//9PFN5y1QgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:23Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      objectset.rio.cattle.io/hash: e10e245e13e46a725c9dddd4f9eb239f147774fd
    name: metrics-server
    namespace: kube-system
    resourceVersion: "5851985"
    uid: 199e1eae-0360-4075-90d2-fad0a9806117
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-08T22:04:28Z"
      lastUpdateTime: "2025-02-08T22:04:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-08T22:04:28Z"
      lastUpdateTime: "2025-02-08T22:04:58Z"
      message: ReplicaSet "metrics-server-5985cbc9d7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: pushprox-kube-proxy
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-04-02T20:58:18Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
      component: kube-proxy
      helm.toolkit.fluxcd.io/name: pushprox-kube-proxy
      helm.toolkit.fluxcd.io/namespace: kube-system
      k8s-app: pushprox-kube-proxy-proxy
      provider: kubernetes
      pushprox-exporter: proxy
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-proxy
    namespace: kube-system
    resourceVersion: "11096441"
    uid: 7cf93552-bf66-4102-9e06-af1fe833d1b0
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        component: kube-proxy
        k8s-app: pushprox-kube-proxy-proxy
        provider: kubernetes
        release: pushprox-kube-proxy
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          component: kube-proxy
          k8s-app: pushprox-kube-proxy-proxy
          provider: kubernetes
          release: pushprox-kube-proxy
      spec:
        containers:
        - command:
          - pushprox-proxy
          image: rancher/pushprox-proxy:v0.1.0-rancher2-proxy
          imagePullPolicy: IfNotPresent
          name: pushprox-proxy
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: pushprox-kube-proxy-proxy
        serviceAccountName: pushprox-kube-proxy-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-04-02T20:58:18Z"
      lastUpdateTime: "2025-04-02T20:58:54Z"
      message: ReplicaSet "pushprox-kube-proxy-proxy-59874cc749" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T11:38:26Z"
      lastUpdateTime: "2025-04-05T11:38:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "7"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:41Z"
    generation: 7
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
    name: traefik
    namespace: kube-system
    resourceVersion: "11100076"
    uid: 5cc7e51a-21a9-4168-8d6d-cabf2c0d9390
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-11T20:16:10+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-08T22:04:41Z"
      lastUpdateTime: "2025-02-11T19:16:21Z"
      message: ReplicaSet "traefik-686b6b64c4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:41Z"
      lastUpdateTime: "2025-04-05T12:03:41Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
    creationTimestamp: "2025-02-26T18:45:23Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: front
      helm.sh/chart: kubeshark-52.4
      helm.toolkit.fluxcd.io/name: kubeshark
      helm.toolkit.fluxcd.io/namespace: kubeshark
    name: kubeshark-front
    namespace: kubeshark
    resourceVersion: "11099965"
    uid: 454f503b-3cf8-4599-bb0b-79e9c55d96a3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kubeshark
        app.kubernetes.io/name: kubeshark
        app.kubeshark.co/app: front
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kubeshark
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kubeshark
          app.kubernetes.io/version: "52.4"
          app.kubeshark.co/app: front
          helm.sh/chart: kubeshark-52.4
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux
        containers:
        - env:
          - name: REACT_APP_AUTH_ENABLED
            value: "true"
          - name: REACT_APP_AUTH_TYPE
            value: oidc
          - name: REACT_APP_AUTH_SAML_IDP_METADATA_URL
            value: ' '
          - name: REACT_APP_TIMEZONE
            value: ' '
          - name: REACT_APP_SCRIPTING_DISABLED
            value: "false"
          - name: REACT_APP_TARGETED_PODS_UPDATE_DISABLED
            value: "false"
          - name: REACT_APP_PRESET_FILTERS_CHANGING_ENABLED
            value: "true"
          - name: REACT_APP_BPF_OVERRIDE_DISABLED
            value: "false"
          - name: REACT_APP_RECORDING_DISABLED
            value: "false"
          - name: REACT_APP_STOP_TRAFFIC_CAPTURING_DISABLED
            value: "false"
          - name: REACT_APP_CLOUD_LICENSE_ENABLED
            value: "true"
          - name: REACT_APP_SUPPORT_CHAT_ENABLED
            value: "true"
          - name: REACT_APP_DISSECTORS_UPDATING_ENABLED
            value: "true"
          - name: REACT_APP_SENTRY_ENABLED
            value: "false"
          - name: REACT_APP_SENTRY_ENVIRONMENT
            value: production
          image: docker.io/kubeshark/front:v52.4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 3
            periodSeconds: 1
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          name: kubeshark-front
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 3
            periodSeconds: 1
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 750m
              memory: 1Gi
            requests:
              cpu: 50m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx/conf.d/default.conf
            name: nginx-config
            readOnly: true
            subPath: default.conf
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubeshark-service-account
        serviceAccountName: kubeshark-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubeshark-nginx-config-map
          name: nginx-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-26T18:45:23Z"
      lastUpdateTime: "2025-02-26T18:45:44Z"
      message: ReplicaSet "kubeshark-front-6dd457cff6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:37Z"
      lastUpdateTime: "2025-04-05T12:03:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
    creationTimestamp: "2025-02-26T18:45:23Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: hub
      helm.sh/chart: kubeshark-52.4
      helm.toolkit.fluxcd.io/name: kubeshark
      helm.toolkit.fluxcd.io/namespace: kubeshark
    name: kubeshark-hub
    namespace: kubeshark
    resourceVersion: "11100112"
    uid: e1722460-f304-4c42-ac79-ca9676a4e479
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kubeshark
        app.kubernetes.io/name: kubeshark
        app.kubeshark.co/app: hub
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kubeshark
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kubeshark
          app.kubernetes.io/version: "52.4"
          app.kubeshark.co/app: hub
          helm.sh/chart: kubeshark-52.4
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux
        containers:
        - command:
          - ./hub
          - -port
          - "8080"
          - -loglevel
          - warning
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: SENTRY_ENABLED
            value: "false"
          - name: SENTRY_ENVIRONMENT
            value: production
          - name: KUBESHARK_CLOUD_API_URL
            value: https://api.kubeshark.co
          - name: PROFILING_ENABLED
            value: "false"
          image: docker.io/kubeshark/hub:v52.4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          name: hub
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          resources:
            limits:
              memory: 5Gi
            requests:
              cpu: 50m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/saml/x509
            name: saml-x509-volume
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubeshark-service-account
        serviceAccountName: kubeshark-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - name: saml-x509-volume
          projected:
            defaultMode: 420
            sources:
            - secret:
                items:
                - key: AUTH_SAML_X509_CRT
                  path: kubeshark.crt
                name: kubeshark-saml-x509-crt-secret
            - secret:
                items:
                - key: AUTH_SAML_X509_KEY
                  path: kubeshark.key
                name: kubeshark-saml-x509-key-secret
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-26T18:45:23Z"
      lastUpdateTime: "2025-02-26T18:45:54Z"
      message: ReplicaSet "kubeshark-hub-7c9f6c7f7f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:44Z"
      lastUpdateTime: "2025-04-05T12:03:44Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "16"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:45Z"
    generation: 16
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.2.2
      helm.sh/chart: grafana-7.0.17
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    resourceVersion: "11100119"
    uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: c7b16e62bdf0a9cd5b1d531b40884dc4e40e401c55b4ace6e201920579bc6c38
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-03-21T22:54:39+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - k3s-w-3
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 800m
              memory: 256Mi
            requests:
              cpu: 300m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-30T07:43:57Z"
      lastUpdateTime: "2025-04-02T19:38:21Z"
      message: ReplicaSet "kube-prometheus-stack-grafana-56cc479d4b" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:45Z"
      lastUpdateTime: "2025-04-05T12:03:45Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 16
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:45Z"
    generation: 4
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics
    namespace: monitoring
    resourceVersion: "11084649"
    uid: 667ce338-4f99-4046-bb70-5e3681d5a078
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.10.1
          helm.sh/chart: kube-state-metrics-5.15.2
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 300m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-kube-state-metrics
        serviceAccountName: kube-prometheus-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-23T21:46:55Z"
      lastUpdateTime: "2025-04-02T19:38:21Z"
      message: ReplicaSet "kube-prometheus-stack-kube-state-metrics-668b4bf9ff" has
        successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T10:05:50Z"
      lastUpdateTime: "2025-04-05T10:05:50Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:45Z"
    generation: 5
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator
    namespace: monitoring
    resourceVersion: "11117608"
    uid: 2898c080-8b5f-4fd8-bf99-45efa86fed7d
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: kube-prometheus-stack
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 55.5.0
          chart: kube-prometheus-stack-55.5.0
          heritage: Helm
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prometheus-stack-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.32.5
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 256Mi
            requests:
              cpu: 200m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-operator
        serviceAccountName: kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prometheus-stack-admission
  status:
    conditions:
    - lastTransitionTime: "2025-03-09T01:33:29Z"
      lastUpdateTime: "2025-03-09T01:33:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-23T21:46:55Z"
      lastUpdateTime: "2025-04-02T19:38:33Z"
      message: ReplicaSet "kube-prometheus-stack-operator-5d9db74bcd" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 5
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsWF1v27gS/S9EH3oBUbGdj/YK0ENQu7jG3SRGsuhLERQMObK5pkh2SKnxBv7vC5JyLCdOmyyyu33YhyAWdWY4POeQHvqOMCs/ATppNCkIs9YdtEOSkaXUghRkDFaZVQ3ak4zU4JlgnpHijmhWAykId5JysIvKWdXMpaYWTStDNkCSRZSzjAcoGrOMWJIR800DXkIFCJqDI8XnJwu5UYYvLwJ+DAp8fF0x5SAj3GiPRilAUnhsYH/VXaX301NjAZk3ob5GBvghHFfHJ4fHtDoZnNCjm5GgbMQPaXV0Mjr+L38/rN4BWV+vM+Is8LB6BKskZ44Uo4w4UMBDvuKO1MzzxS/sBpQLj8zaH5G0Dmk9Mg/zVQjxKxvKvQSOwDyQdUY81FaFz2GCngbqJfMkupjUbh+yBo+Su1RNt0hWVVJLH6vSRsDp9nmdEWvEqfayN0gQvjYSQYwblHp+xRcgGiX1fDrX5n54cgu8STJ+7pZw9ZDAya1FcKHwZI0lrJIngnU26hVkqklGWqaa6KDvL/866ueNNcrMV/+PCZfNDaAGDy6X5mBhnI9eWV8HFjq2ADt34jxOQmlbDkhGKA3TMSFCoeWbt6fj8eXk6uo/8ZUCJgBpXJY0ugzmjC+8rME0vhzVhwO3D0rvd0zZ3y+PcQqYAyoaZHGGUT18tz8hgoZvVAATSmooh/XRk0CPK2oBpRHl6MSR64yAbuPquz3ULXLDOinIAXcy/PW5zp3hS7K+zois2TzuPZhL53GVL99Hqp2cU+cNsjnEYOY94wvAoj3Kj/Ow62PkrFFqZpTkQaxpdW78DMHt7Op+NMkIgjMNxhMleLQ1qqnhzDTaJxXr8HHG/KIrfZso1AyeColxp/cF/47QGy/8UNetBX5iwQMIbj0yms4eujlsUvXP8UOj5W1xcPA6tnCaWbcw3gdnvMsH+eiFzugl+Nccr2COBdNCAU3MUakbBxQQDZbxGzliKmC+QaBz5sGVl8BNC/gpRkxuLdPBDB+ZVA3CP2QrBCd/D5Ya5sPBi0+bLvqn8lMSS2oP2DJFnWfoy+PBoH4lu+1q+j+jDc4+XQJXTNaJsm1Oi9CC9huT1EYA5Ua3qbV7eu6/ycI/0/nWCw7ffC/34m5/95f4MTR+UpRv3p5fjCdfpuNkxdCklqnXSqxqYY3Uvnzz9sPV9MvkfDy7mJ7/uuvbbbvuAFvArRcEyhYwFLM1Xp7S59zJPD2bOhlMCiVr6Us6TOY0yCEglqGZU1xJ0P4pRWcX4y/T2UbQj2jq0HRWEpS4hOr+c8eS88w3LrdGTGdkvQ7MdHk6Np6dyALPA5XnscHsZwoVnZ+eTa5mpx8mz823MW6+vWDtJO2L8Gfd+7Vhq+DaQG0MSP+L9jAfDvPBC63ab8xfzavZA6BbuS0w9PM0jDyCKXlzUBvRKOjBlbyh20EEJi60WqWL5aMMAtoHE4WRRzDwPNIWT3VudCXnBz1adt88Dve1jVItob+u8ER9bTfb9TrcQLGVHE45D+Hn/QvvPfV9saljZEN64htq61djma5gIGRTk4KcQW1wFW6g+7kPC0+13hHbk2C9X4N98L4U671aPBWZCN+rQeLzjNmAlx7q/i0yMqIa5wE74ulvzgRPdpnTYB4HA7n64e8HO5J9T8znkfpQz/SDQDh4wt5Y/xEAAP//UEsHCBmy5nPnBAAAohEAAFBLAQIUABQACAAIAAAAAAAZsuZz5wQAAKIRAAAIAAAAAAAAAAAAAAAAAAAAAABvcmlnaW5hbFBLBQYAAAAAAQABADYAAAAdBQAAAAA=
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-08T14:56:56Z"
    generation: 22
    name: csi-cephfsplugin-provisioner
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 3e5f5635-f606-4b2d-a2c3-f46259c81f7e
    resourceVersion: "11099972"
    uid: 8ace202a-f51c-4461-8ce1-22193923d0cd
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-cephfsplugin-provisioner
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin-provisioner
          contains: csi-cephfsplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-cephfsplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --v=0
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --timeout=2m30s
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --feature-gates=HonorPVReclaimPolicy=true
          - --prevent-volume-mode-conversion=true
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --controllerserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --forcecephkernelclient=true
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-provisioner-sa
        serviceAccountName: rook-csi-cephfs-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: socket-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2025-03-08T14:56:57Z"
      lastUpdateTime: "2025-03-09T07:51:03Z"
      message: ReplicaSet "csi-cephfsplugin-provisioner-87fd75775" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:37Z"
      lastUpdateTime: "2025-04-05T12:03:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 22
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzkmE1v2zgTx78L0UMfQJRf8tI+AnwIahcb7CYxkqKXIihocmSzpkh2SKnxBv7uC1KyrdhKmuy2WBR7MCyRw+Fw/j+OSd8TZuVHQCeNJhlh1rpeNSAJWUotSEbGYJVZFaA9SUgBngnmGcnuiWYFkIxwJynOhFXlXGpq0VQyuAIkSTRxlvFgh8YsKQe7IAkx3zTgNeSAoDk4kn16NIqZMnx5FezHoMDH7pwpBwnhRns0SgGSzGMJ3SE3YW6np8YCMm9CfKUM5kdwkp+cHp3Q/LR/So9nQ0HZkB/R/Ph0ePJ//naQvwGyvl0nxFngYekIVknOHMmGCXGggAd/2T0pmOeLP9gMlAuvzNonM7QOPj0yD/NVsPcrG2K9Bo7APJB1QjwUVoXn4L2VffXsSepEMandgVkBHiV3dRzN2lieSy19jEcbAWe793VCrBFn2stWI0H4WkoEMS5R6vkNX4AoldTz87k22+bJHfCyVu9TE/zNft4mdxbBhahrIpawqlEIxGxEy8i5JgmpmCojOE8s/DZq5o01ysxXv0dvy3IGqMGDS6XpLYzzkY/1bUhBkyfAhkicxxkoDXMwIUJ0o1evz8bj68nNzf9IQiitRv347WUBpvSjYXHUd7EFweOKSu0BK6ao8wz96KTfL+puBUwA0pgEafQoENzVQbebaNTeQod2CpgDKkpk0d+wGLzpnIkiaPhGBTChpIbRoDh+1DCswAJKI0bD09pIQM5K5WnuAq0juPPHsR3uPDJag0s3pO5WZREq0J5WRpUF0MIIoNzoqt7yO7scmC8R6Jx5cKPfjDY4/XgNXDFZTI2SfFXb3iYEdBV1anZ4I8sGDpKRUsu7rNfrcSfDpw1H6gxfkvVtQmTB5rFAwFw6j6t0+Tay4eScOm+QzWF/cFYdp/00VKc4eFoqVYcW6MwvjZ8iuAfVZ88BSQiCMyXG4hf2VZ2WC1NqX8NXhMcp8wuSkTD/zleIHDwVEmNR+sec/qogLpgWCjZASV06oIBocBR/HzpougZuKsCPccTkzjId5HjPpCoR/iWsEJz8MyA1SAcvZ6oZ/bN4ehyaJ0j7VXnqKmzPweHHcMC8Z3xR15aTF3OwGf2zQPhPFZYnfsmew8MPLQ9OM+sWxvtAxpu0nw5fSEbLwc+CI5wSpRi9en15NZ58Ph/XXIAW1kjtR69ev7s5/zy5HE+vzi8/7EETdhrORF1Ttud5B1gB7oARKCvAEMmOkBRnIuVOpvGFm6I+ZkihZCH9iA46xJpejT+fTzdavUdThMNnLkGJa8i3z00CnGe+dKk14nxK1uuw6MZPs9BnO7LA05Cly3jWbHsKEV2eXUxupmfvJs/1t2Ey3d2vHjht5/vvgvm1ZKsAZMhuHFB/Z9VROhik/RdSuD2g/zAGkz1DAdXOMJzraWg5MHMrt2cWWg7MlJz1CiNKBS1zJWd014jAxJVWq/rueeABPI85i6WTG53Lea+Vk4c9HRH4wkahltCOOLxRX9juHGCpey7cHb3rebME3RpppOA0Nh7GfhuED3tOcjjjPLi8bN+aawHbvFDHyEa6WrWQyzqOe2Jbmqy7Rekyr5XoFKfLvK3RulOkMBIK61djWd8yQciyIBm5gMLgqjVqj6zHVEqIRfMFuAcR/G05/nRPapMLZkOH9FC077DRhyqdB2x80S/OBCWaxdSNaWwMYmi296fFLgoSd/p3pyuYtVLPH5muiaUx+s68D12RdbwsPzezB8R28fhoVh9C+SGaZ/eElUKC5g80WsZ7NdxZWR8PboAbLRzJjk77/e3KW9PGddyu6/9gQqUP9Wj9VwAAAP//UEsHCM6mYEtTBQAADxMAAFBLAQIUABQACAAIAAAAAADOpmBLUwUAAA8TAAAIAAAAAAAAAAAAAAAAAAAAAABvcmlnaW5hbFBLBQYAAAAAAQABADYAAACJBQAAAAA=
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-03-08T14:56:56Z"
    generation: 23
    name: csi-rbdplugin-provisioner
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 3e5f5635-f606-4b2d-a2c3-f46259c81f7e
    resourceVersion: "10606022"
    uid: c6a92e8a-3a3f-4243-8b6f-0cf7f311de79
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-rbdplugin-provisioner
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin-provisioner
          contains: csi-rbdplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-rbdplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          - --extra-create-metadata=true
          - --prevent-volume-mode-conversion=true
          - --feature-gates=HonorPVReclaimPolicy=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --v=0
          - --timeout=2m30s
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --controllerserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-configs
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/secrets/tokens
            name: oidc-token
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-provisioner-sa
        serviceAccountName: rook-csi-rbd-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - emptyDir:
            medium: Memory
          name: socket-dir
        - name: ceph-csi-configs
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: csi-cluster-config-json
                  path: config.json
                name: rook-ceph-csi-config
            - configMap:
                items:
                - key: csi-mapping-config-json
                  path: cluster-mapping.json
                name: rook-ceph-csi-mapping-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - name: oidc-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: ceph-csi-kms
                expirationSeconds: 3600
                path: oidc-token
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2025-03-08T14:56:56Z"
      lastUpdateTime: "2025-03-16T20:30:56Z"
      message: ReplicaSet "csi-rbdplugin-provisioner-8648497f4c" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T16:27:17Z"
      lastUpdateTime: "2025-04-02T16:27:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 23
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-08T15:08:01Z"
    generation: 5
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-m-1
      node_name: k3s-m-1
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-m-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "9750113"
    uid: 57628580-09cb-4690-a090-ad7327808bea
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-exporter
        kubernetes.io/hostname: k3s-m-1
        node_name: k3s-m-1
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9926"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-exporter
          ceph-version: 18.2.2-0
          ceph_daemon_id: exporter
          kubernetes.io/hostname: k3s-m-1
          node_name: k3s-m-1
          rook-version: v1.14.8
          rook_cluster: rook-ceph
      spec:
        containers:
        - args:
          - --sock-dir
          - /run/ceph
          - --port
          - "9926"
          - --prio-limit
          - "5"
          - --stats-period
          - "5"
          command:
          - ceph-exporter
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: ceph-exporter
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/exporter-keyring-store/
            name: rook-ceph-exporter-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-exporter-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-exporter-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-08T15:08:02Z"
      lastUpdateTime: "2025-03-08T15:08:04Z"
      message: ReplicaSet "rook-ceph-exporter-k3s-m-1-df76dc8d8" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-28T19:39:24Z"
      lastUpdateTime: "2025-03-28T19:39:24Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 5
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-16T19:59:57Z"
    generation: 1
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-1
      node_name: k3s-w-1
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "11099873"
    uid: f9966238-eabd-4627-99af-ac834c100aaa
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-exporter
        kubernetes.io/hostname: k3s-w-1
        node_name: k3s-w-1
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9926"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-exporter
          ceph-version: 18.2.2-0
          ceph_daemon_id: exporter
          kubernetes.io/hostname: k3s-w-1
          node_name: k3s-w-1
          rook-version: v1.14.8
          rook_cluster: rook-ceph
      spec:
        containers:
        - args:
          - --sock-dir
          - /run/ceph
          - --port
          - "9926"
          - --prio-limit
          - "5"
          - --stats-period
          - "5"
          command:
          - ceph-exporter
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: ceph-exporter
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/exporter-keyring-store/
            name: rook-ceph-exporter-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-exporter-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-exporter-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-16T19:59:57Z"
      lastUpdateTime: "2025-03-16T20:00:07Z"
      message: ReplicaSet "rook-ceph-exporter-k3s-w-1-58fdfc478" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:36Z"
      lastUpdateTime: "2025-04-05T12:03:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-27T03:55:07Z"
    generation: 1
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-2
      node_name: k3s-w-2
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-2
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "9964637"
    uid: 63c20aea-2986-422c-914c-736836f71bac
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-exporter
        kubernetes.io/hostname: k3s-w-2
        node_name: k3s-w-2
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9926"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-exporter
          ceph-version: 18.2.2-0
          ceph_daemon_id: exporter
          kubernetes.io/hostname: k3s-w-2
          node_name: k3s-w-2
          rook-version: v1.14.8
          rook_cluster: rook-ceph
      spec:
        containers:
        - args:
          - --sock-dir
          - /run/ceph
          - --port
          - "9926"
          - --prio-limit
          - "5"
          - --stats-period
          - "5"
          command:
          - ceph-exporter
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: ceph-exporter
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/exporter-keyring-store/
            name: rook-ceph-exporter-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-exporter-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-exporter-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-27T03:55:07Z"
      lastUpdateTime: "2025-03-27T03:57:51Z"
      message: ReplicaSet "rook-ceph-exporter-k3s-w-2-ccc459589" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-30T00:46:52Z"
      lastUpdateTime: "2025-03-30T00:46:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-17T20:18:49Z"
    generation: 1
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-3
      node_name: k3s-w-3
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-3
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "9322192"
    uid: 69a75536-c529-4cd2-953d-409c72e58e37
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-exporter
        kubernetes.io/hostname: k3s-w-3
        node_name: k3s-w-3
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9926"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-exporter
          ceph-version: 18.2.2-0
          ceph_daemon_id: exporter
          kubernetes.io/hostname: k3s-w-3
          node_name: k3s-w-3
          rook-version: v1.14.8
          rook_cluster: rook-ceph
      spec:
        containers:
        - args:
          - --sock-dir
          - /run/ceph
          - --port
          - "9926"
          - --prio-limit
          - "5"
          - --stats-period
          - "5"
          command:
          - ceph-exporter
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: ceph-exporter
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/exporter-keyring-store/
            name: rook-ceph-exporter-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: k3s-w-3
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-exporter-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-exporter-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-17T20:18:49Z"
      lastUpdateTime: "2025-03-17T20:18:51Z"
      message: ReplicaSet "rook-ceph-exporter-k3s-w-3-8576f74844" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-26T19:24:31Z"
      lastUpdateTime: "2025-03-26T19:24:31Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "11"
    creationTimestamp: "2025-03-08T15:02:27Z"
    generation: 11
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 18.2.2-0
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      rook-version: v1.14.8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "10640795"
    uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - k3s-m-1
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 7000
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "3"
              memory: 1Gi
            requests:
              cpu: 800m
              memory: 500Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 1Gi
            requests:
              cpu: 800m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-08T15:02:27Z"
      lastUpdateTime: "2025-03-30T07:45:04Z"
      message: ReplicaSet "rook-ceph-mgr-a-dcf48548" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T21:04:28Z"
      lastUpdateTime: "2025-04-02T21:04:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 11
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "14"
    creationTimestamp: "2025-03-08T14:57:31Z"
    generation: 2397
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 18.2.2-0
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      rook-version: v1.14.8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "11100140"
    uid: e20b9613-7745-4e36-8511-c9c1e877c05a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-08T14:57:31Z"
      lastUpdateTime: "2025-03-29T17:12:28Z"
      message: ReplicaSet "rook-ceph-mon-a-6d68789b6b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:03:49Z"
      lastUpdateTime: "2025-04-05T12:03:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2397
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-03-11T18:40:25Z"
    generation: 319
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 18.2.2-0
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      rook-version: v1.14.8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "10640769"
    uid: febcd12a-97fc-4f2e-913d-d17837ace9ee
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: e
        mon: e
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: e
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: e
          ceph_daemon_type: mon
          mon: e
          mon_cluster: rook-ceph
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-e
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.219.20
          - --setuser-match-path=/var/lib/ceph/mon/ceph-e/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-e
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.219.20
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-e/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-11T18:40:25Z"
      lastUpdateTime: "2025-03-29T17:12:01Z"
      message: ReplicaSet "rook-ceph-mon-e-78588fff8b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T21:04:22Z"
      lastUpdateTime: "2025-04-02T21:04:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 319
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-26T15:01:31Z"
    generation: 20
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: h
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph-version: 18.2.2-0
      ceph_daemon_id: h
      ceph_daemon_type: mon
      mon: h
      mon_cluster: rook-ceph
      rook-version: v1.14.8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-h
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "11084668"
    uid: 06cc8142-995b-40db-85b8-80effaea6d40
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: h
        mon: h
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: h
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: h
          ceph_daemon_type: mon
          mon: h
          mon_cluster: rook-ceph
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-h
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=h
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.167.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-h/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.h.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.h.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-h
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=h
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.167.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-h/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-26T15:01:31Z"
      lastUpdateTime: "2025-03-29T17:12:51Z"
      message: ReplicaSet "rook-ceph-mon-h-9bc6b5db7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T10:05:55Z"
      lastUpdateTime: "2025-04-05T10:05:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 20
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"rook-ceph-operator","app.kubernetes.io/instance":"rook-ceph","app.kubernetes.io/name":"rook-ceph","app.kubernetes.io/part-of":"rook-ceph-operator","operator":"rook","storage-backend":"ceph"},"name":"rook-ceph-operator","namespace":"rook-ceph"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-operator"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-operator"}},"spec":{"containers":[{"args":["ceph","operator"],"env":[{"name":"ROOK_CURRENT_NAMESPACE_ONLY","value":"false"},{"name":"ROOK_HOSTPATH_REQUIRES_PRIVILEGED","value":"false"},{"name":"DISCOVER_DAEMON_UDEV_BLACKLIST","value":"(?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+"},{"name":"ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS","value":"5"},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"rook/ceph:v1.14.8","name":"rook-ceph-operator","securityContext":{"capabilities":{"drop":["ALL"]},"runAsGroup":2016,"runAsNonRoot":true,"runAsUser":2016},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-config"},{"mountPath":"/etc/ceph","name":"default-config-dir"}]}],"serviceAccountName":"rook-ceph-system","tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":5}],"volumes":[{"emptyDir":{},"name":"rook-config"},{"emptyDir":{},"name":"default-config-dir"}]}}}}
      kustomize.toolkit.fluxcd.io/prune: disabled
    creationTimestamp: "2025-03-08T13:17:48Z"
    generation: 2
    labels:
      app.kubernetes.io/component: rook-ceph-operator
      app.kubernetes.io/instance: rook-ceph
      app.kubernetes.io/name: rook-ceph
      app.kubernetes.io/part-of: rook-ceph-operator
      operator: rook
      storage-backend: ceph
    name: rook-ceph-operator
    namespace: rook-ceph
    resourceVersion: "9309700"
    uid: 3e5f5635-f606-4b2d-a2c3-f46259c81f7e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-operator
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-operator
      spec:
        containers:
        - args:
          - ceph
          - operator
          env:
          - name: ROOK_CURRENT_NAMESPACE_ONLY
            value: "false"
          - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
            value: "false"
          - name: DISCOVER_DAEMON_UDEV_BLACKLIST
            value: (?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+
          - name: ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS
            value: "5"
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: rook-ceph-operator
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-config
          - mountPath: /etc/ceph
            name: default-config-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-system
        serviceAccountName: rook-ceph-system
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - emptyDir: {}
          name: rook-config
        - emptyDir: {}
          name: default-config-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-08T13:17:48Z"
      lastUpdateTime: "2025-03-08T13:17:50Z"
      message: ReplicaSet "rook-ceph-operator-fc4766689" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-26T17:50:14Z"
      lastUpdateTime: "2025-03-26T17:50:14Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2025-03-08T15:08:01Z"
    generation: 8
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph-version: 18.2.2-0
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      portable: "false"
      rook-version: v1.14.8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "10640777"
    uid: fed060e3-4b07-4c07-8720-734d979bdd12
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-m-1
          osd: "0"
          osd-store: bluestore
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-m-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-m-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: e319b7b8-8843-4ff4-a053-f07520dbe330
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-09T22:35:20Z"
      lastUpdateTime: "2025-03-29T17:14:20Z"
      message: ReplicaSet "rook-ceph-osd-0-cddcdb46d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-02T21:04:23Z"
      lastUpdateTime: "2025-04-02T21:04:23Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 8
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2025-03-08T15:03:40Z"
    generation: 8
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph-version: 18.2.2-0
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      portable: "false"
      rook-version: v1.14.8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "11100321"
    uid: 4c42924e-03e7-45bb-a6d2-679081f0ffbb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-w-1
          osd: "1"
          osd-store: bluestore
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-09T22:35:20Z"
      lastUpdateTime: "2025-03-29T17:15:58Z"
      message: ReplicaSet "rook-ceph-osd-1-756ccd44c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:04:43Z"
      lastUpdateTime: "2025-04-05T12:04:43Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 8
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsOmtvIrmWf8XjbolkloJAHp1mhFZMoHvQJCEXkl6tUllkqk6BJ1V2je2CZnvz31fHroLi1Z25M6vbWs0XBPbxeT9tvtAEDAuZYbT1hcZsArHGbyxNaYsqKZ+9ANKZJ3VIq7hae84moAQY0DUu64FMUilAGNqiCBjEmTagdA1/1PB8jcv9JxUwA6E3WW4SSkExI9XeM1xow0QAtEWbewESJtj0DyIVLIGc+4NipkwZT0ZlpLS6OuLxMOfIrsxBaS4FbdHGZa1Za3on+c44ZJBIMd4ALxbNMkU2HAchzHkAXhAzrWmLarsYMR5nCrxQJowj+udT7S08RISnHEpkRxupENckzsB9r9JUKsMmMS5HLNa4ZEVZMztv1Bpntct8A6Uu1OahinTKrOLLCsDv49zkW1tGpjKW06UXy4AZLoU3k9psML0LoqREkBAilsWGvlRpbpwNR7SHD7EkFwLUECJQIALQtPWIzsw/rcQsO2Z93qBVOoll8DzAc12IwVgwozKo0kAKo2Qco3Ru5ZkL1PQVpLOrXO5dJmmVZtbGF8FZEEyid97ZCXvvnUXNc28SwKkXssvmyfn7xuTs4oS+PL1UqU4hwLhTkMY8YJq2GlWqIYYA3bb1hSbMBLPrb8TnrkMetNALEjWKGZguEWHufkNwcYm6N5CkMX5v/Z0l/vIs8R3mgu845GkpRFgUccENeu2LC1HGBag81NUUv1DPi6SCqZKZQHV5qPyqVYrnRdr+gCa8P7tgF97JBVx6Z5eNS+/y9P25B6xxFsHlWePizB3VYDJtAz0X1C4h7rS8FqhMz1aCtVGwdi4WQUW012rwvHzDi+XUM9LTJgSl2phjNrZBqa9tJ1J4uS2+hQm33Z6XKoj453YIk2xK9nET8RjahWd8ldoWZKK9GJgSHgtD5UVKJl4KoHKIJ7RWkjDMoI/rSHqqUhBza73c9MPB4Nfx7aDbG992bnq0SucszqDkSC/VTdir64fRfW847ndLwK9LvgcwbREu5c2tA3fD/qfOfW/cvyvAPyiZoJ9GHOJwCNHq+x0zMwxgw0yma6kM+3f05WUH4cPP1/2rvwTf3aBrBRndda5eJ83N4Hbcu+3eDfq396NtBgIpIj69YemvsMz5eAZMobYw7AlcdBgQYSq5MHqPqFeD2w/9j+Nuf1hirj5nqh7zSR3x7Nqnd/dLcW7wqTcc9rtlyepgAnuw7pityzkoxUOo4e8yth33+qaaUwhqQoZwiwj2CDN8GP1y07kbDweD+xJL68x2AP6Xwej+235uBf80uH646Y27vZ8fPpbAG4cAR7/278bD3uh+MOxdDW4PHenejLv9Uefn6974odv7dAjMsj0YdUfju95w3O196m941SYXg9v7Tv8WY/Km87EM9nvGlrayQzqzH625a5PpHs99rXGK9qQmdoyDqG56N4Phf46v+zf9+22UCrTMVAAfSqhDPuca+y6KrXsBQVs05gk3upZAItXyIJ1h7x8PvdGfo6TgdyzYh2ld3T38MwI19goUpNl+Cn+pKDtUrK8+jHrjYee2O7gZ3w5uN1zKVrE9Hmh9ujO8H/dv73vDT53r0pndnI4HHh42akN4EjTfBRcT73JyduGdvT899yYXFxOvMWk2o8vzCEJ2vhfPBpY9hQgFwiSKQb2tNI29tdlOntjv2T5pTwJ1SWyPzjrDj6MSI15C3h7tMnC8w9/P14OrX8d3nftfykkzhHldh2xXmk/jm8FGflVssVctLhmMr647ozJf2nZuW/Vt0P0Txc01C+7UY6kk5Ud3NIgtMIi5V9QB7IpT7M9Y7EY6i5InbPr13JTD3GVxfCdjHqDl+tGtNHcKNA42VRrzOQjQ+k7JiZ2Z4LNrWEtNDzY6VepxWqXado0BrVJfyMykbZ++PUKaxPNYmHDhuWGA1FUm1hyhRLVmjWn5TJxySNPPTk5OAT+bF41jn/pCBe23/+4LHpFH8lYFxBNATsgTcTDukzySH4gXkbpJ0rodFWKAlDz9RMwMhC98A8FMEt92aiRnZgYsNjMSzCB4JjiEQEgW3MzwDIlkHMsFF1OCAmWmhays0LzFRZ+S/yEaQuIBqej6f9Ud76Q+rVjQz9wgw76IuC8oTsb5pHM/U6BnMg5p67xKcQDgLO5CzJYjCKQIcWI+qdIUFJfhaunipEp1FgSgdQlBo0oNT0BmZgV4erKePNzYVSQwO+i6NGmNmWa0RU9pleaJuUXPPnKcT4o0t4ZqlKGaCIVjDASZ4mZ5JYWBzwahU8XnPIYphMU1gwIWDkS8HEppPvAY9FIbSGjLNtJVqjLR0Q84k7RO7BzPlMnSvx3v/9jx3jVPXu96uLTtZecvVTqXcZbAjcywJ8YcluDXPN1tNr+bFcF22ZhNNw5gw5uPoBvJzybGctorXCpPe9t4Cluv8VijO+1rT8vg2Qu52mXAciyne5lADLGcHjjEJ7l3Kab3HnUbO4dDmK+h3e2I3oVCgbINUJUJz658nR2pQ+fwzfVRFhg+ZwbcXcRTlS6keuZi2uVqRwe2oAhurjZvJkrBWJ9wUZ+wciBqMMSTBJRCT1z9TnkK6IerBSEz+/WNjQHCIzJnirNJDJowBcTurqA/G8UC8IUvXO+CoV5qZTBKbMPwYdTvtl9zH+IQYTPVfk0L5cDt+DH+cN352Pap561uopC8bR469x0cAtsH7ODTtyV+XU/SVmzhC9d2rKRadzc2/sWbPIfY4c+nJMm0ITM2B5s0fBppHvqUTGM5YTFxIZMpe31DjCSF0QmIQC1TAyEZjLoa8bLIgNpKPVeQziqa3A0J1yQBNYWwhrAzY1Ldqten3MyySS2QyTqp1tEruKk3zy/Oz+GcXV4238PJSYOF0dnFu/NGg11MQrhsnDUnJ+z83Tniu59xTWxBcpwueByTCRAFiZxDSGJmQNVIJ2eRmSpZAAmYyCGImXErBIpPuPgNAosnkCHUCBYRNomXhJGplCHBFIYCLWZglZLj+EfGRbAkOktTqcwhOe0kbj/SLI7rjZPT09M3IddBpjWXYqwu3zcvG+8u373zRZASzxPSS7GhUnNoJzIEYvObgST1Vjqz1iSrxLde80W6NDMpTokXEPQAniBzuWFTpjQodAv3m7Q3NmpX9sed/XF0XEDVMG0eVfZQqxwjLh4RIQsStRnTY+3UeVRxblU5bvmCEJKDPBbLT6RNvrzsR+E61BWGagUtdRDRo9tGhJW3q3DGqiZsgZQpiP0SVEllUTkmTOc4Ix7DBpHaQnEDR+tdlDkPLXczTwQsyDMsFRdTX7guQpAg5iBMzXYThGVmRqZgPKm8/BC2EqWoJljgKwwDiaRKIiEEqZBkqvauSx0W6z9WiPdc8gbXweQcudvv+oo/G7oY1S4DIBqrIptmXGF2vcsjtg15qvEpabeJT+N54lPyVGpT7m/ubN56e5Q8o48SL7RO4Zs36+QhdYgrJRIknifrfev0rr8LSa6Udcb0KSkrKv+OCdjZAUkFMl1aeUySRpqEXNlXo6XNYQQZk4qp5XrDHcM0gJEtAEIIyQQClmnIA30GBGsYWV2vE6xLulqiM5VYcxZsSZgICTfWiwwI43SKVWxFxuYoBTqLDeEOPSSpWYZcIZeT/Kcjzmy/jiw5sRLGhbXVihmrUMwXc1ATqXMlhqCKd79CUUVp8Wn9R1zLLebTeqG8TNhOoCSX5ohgASSUomKsegg3hIllgkVL+CZzR3ZobBhkwoJni7XQioxKRLgw0vmg4lMuWGwF3LDQNwVcC/PjYV4UGKu+BTYiM54iGxhImuQM2JjNNKi6fdGwlGdyITaIKxxaNMYNgrfsma+QzMsMHHI+lWzgtI81WzL5AmINDmGlHDyKLUjMtamQo8WMB7MikrFwpNgC/SYnKI921cpWACzbRAoiZAj62CFFzWRKaCu7FykOIoyXxPWTBPs+LHpIAEtnMGNimgeHC1ZNFEykNDVyLx3GZ5xiBqMuSWWoSaRkQmwHi82BgtQ+ZGJhtYWZ5cdtcbY+ZmQ+3XDrKA6nfZsumFqA6164JtpgRAVSoVarLgIjdFOu0WlXG2vkDp9GYVgcW62FXD9bNUVchHZF8enMECmghuBo3uv+6N5OiC7FHRdmjjLh2gY8O3YMHh2TL7iHhH77nSxkFmNeIal1W2xSq2SSGZJYKqWWJZKKzJgKQUBo+yhib1t0gayo63pmUcJnrm0eYVYHRZuNyikaMXdyfz+gl7pKftNS+AIJj6uEi0giPlysxZKFR3qpa9qEXBzXOBr7CKuvb1DHIpKPFanDsSu6bZKn51ZOU3FhjhyUU0vlqUpAhO2KbRlWIJVIZiLMbdsiFfJvZPuUfV7LWQGljskbEsvpynZoXYfESOJAHAE8gvn66OSYkDd5IrPumSqJydWOyphGVqi4dqh8sTpcEdLtORo2q6N797uFyJWiHzAvzit4RH4g+6IVY9t161jF3OidJxD0MJ+uqyoa3IUAyWcmO+vktccylLClcx8M09AdssN+pbIXuXVYblMbMztbBd8lT3ZYgsMs7pfxAOmDZH2znmCOvkX+uCzH4yPx/ntDqU9blyxfSH79gVa0WH1KfrJlnDTIT+SlSNf9iDDyDEpAXM5+RcqbsTQFoW2KYZv5CN3T4chrOnrHuv2YQCAxiybcOlzVtQIIY+3pnJapvE/AOOYik5mOlzU35KRKTmJIbAK25tYynru+wLakKnFdne0ygMxZzENulkWlLXFpuVfuvy3o+5goIyIAY4Gp5SrbrSfIQ8No7vl1q4jcdR6Jd+1CYn2eeAzNc4TzQ8zF8/b+sU/JD22S2488kbJ3qWQbfGV5p6st77OjkhTxaiDTZDVlEy23mhkjScq0Jow4gChmVovbWEstauGWJX/b6FvtD9vZFBdoO4/638sD5v+vJ5qb3n3HdWDuOXZ7/z8614e29j1nfcfPK1s3cf/yG/rNK/jX3eoGr7tf/OcuPDduSv7sdfBX5+l9cbABt4v/CSkUf87C3BRpbxLC3IPPKcv/opUi7ephbe35/9Aqy3lGypj+xb7mePMcu3873IYNV9OhNd1qlsv/ENfKfXD7dWD/xf/68WHLxDiH/tVGtUi91dRgn1Xsy8a/2r5/6Mnvdeb/+xXpe35FwtIoZAij0t+8N/9QjE1Hfnb9lzB0U2ybV37eiRdsiflJg0IxOkGAbN3uKda08BznNIjfcf+Fpvt85+WA83zdSVIlf4PAoG9/oauQKvcTuGHnarvsuqy8k6pS7GZpq/m+WS2YCoora9uhfI34y8uT7cS+JVkdPmOfbP/Vm//bulsMLgN1tfoP/De9+JuEVvqvOx8+7Nt/AFXh01/z9n3onN/u8/p90Cvnf9nv/X+A4dc8Lo5f86T4OnvtlLdv9iuu81/PADvRs3nA+pn764XJMP+/vPxvAAAA//9QSwcIcZwPL9sPAABLNQAAUEsBAhQAFAAIAAgAAAAAAHGcDy/bDwAASzUAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAABEQAAAAAA==
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-03-08T15:04:06Z"
    generation: 7
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph-version: 18.2.2-0
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k3s-w-2
      osd: "2"
      osd-store: bluestore
      portable: "false"
      rook-version: v1.14.8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-2
      topology-location-root: default
    name: rook-ceph-osd-2
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "11084691"
    uid: 25c07072-1e4b-44d0-bc38-26a56f686927
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: k3s-w-2
          osd: "2"
          osd-store: bluestore
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-2
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: d0c27c6b-8b46-4935-b66b-1b22f85feda5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=d0c27c6b-8b46-4935-b66b-1b22f85feda5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_d0c27c6b-8b46-4935-b66b-1b22f85feda5
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-08T15:04:06Z"
      lastUpdateTime: "2025-03-29T17:19:08Z"
      message: ReplicaSet "rook-ceph-osd-2-56c7cdd86f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T10:06:01Z"
      lastUpdateTime: "2025-04-05T10:06:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-17T20:18:49Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "3"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "3"
      ceph-version: 18.2.2-0
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      device-class: nvme
      failure-domain: k3s-w-3
      osd: "3"
      osd-store: bluestore
      portable: "false"
      rook-version: v1.14.8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-3
      topology-location-root: default
    name: rook-ceph-osd-3
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "11084870"
    uid: 527b36fa-aab5-417c-857e-1d8e59d2b011
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "3"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "3"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "3"
          ceph_daemon_id: "3"
          ceph_daemon_type: osd
          device-class: nvme
          failure-domain: k3s-w-3
          osd: "3"
          osd-store: bluestore
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-3
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "3"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-3
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-3
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-3
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 9c8eabc6-e7b1-4837-8d10-9d1b9e107222
          - name: ROOK_OSD_ID
            value: "3"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/nvme0n1p3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: nvme
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=9c8eabc6-e7b1-4837-8d10-9d1b9e107222\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/nvme0n1p3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "3"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-3
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-3
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_9c8eabc6-e7b1-4837-8d10-9d1b9e107222
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-17T20:18:49Z"
      lastUpdateTime: "2025-03-29T17:16:46Z"
      message: ReplicaSet "rook-ceph-osd-3-c984f9545" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T10:06:53Z"
      lastUpdateTime: "2025-04-05T10:06:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "10"
    creationTimestamp: "2025-02-08T22:20:42Z"
    generation: 13
    labels:
      app: rook-ceph-tools
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: rook-ceph-tools
    namespace: rook-ceph
    resourceVersion: "5852148"
    uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-tools
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
      spec:
        containers:
        - args:
          - -c
          - "# Wait for the Ceph configuration to be available\nwhile [ ! -f /etc/ceph/ceph.conf
            ]; do\n  echo 'Waiting for Ceph configuration to be available...'\n  \n
            \ # Get monitor endpoints\n  MONS=$(grep -v '^#' /etc/rook/mon-endpoints
            | tr ',' ' ' | sed 's/=/ /g' | awk '{print $2}' | tr '\\n' ',' | sed 's/,$//')\n
            \ \n  # Create initial Ceph configuration\n  if [ ! -z \"$MONS\" ]; then\n
            \   echo '[global]' > /etc/ceph/ceph.conf\n    echo 'mon_host = '$MONS
            >> /etc/ceph/ceph.conf\n    echo 'auth_cluster_required = cephx' >> /etc/ceph/ceph.conf\n
            \   echo 'auth_service_required = cephx' >> /etc/ceph/ceph.conf\n    echo
            'auth_client_required = cephx' >> /etc/ceph/ceph.conf\n    \n    # Create
            keyring file\n    SECRET=$(cat /etc/ceph/secret)\n    echo '[client.admin]'
            > /etc/ceph/ceph.client.admin.keyring\n    echo '  key = '$SECRET >> /etc/ceph/ceph.client.admin.keyring\n
            \ fi\n  \n  sleep 5\ndone\n\necho 'Ceph configuration is available.'\n\n#
            Keep the container running\nwhile true; do\n  sleep 5\ndone"
          command:
          - /bin/bash
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /etc/ceph/secret
            name: ceph-secret
            subPath: secret
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
        - name: ceph-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret
            secretName: rook-ceph-mon
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-08T22:20:42Z"
      lastUpdateTime: "2025-03-08T14:01:28Z"
      message: ReplicaSet "rook-ceph-tools-95c4b57b6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-09T01:33:31Z"
      lastUpdateTime: "2025-03-09T01:33:31Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 13
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-03-29T14:40:31Z"
    generation: 7
    labels:
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: uptime-kuma
    namespace: uptime-kuma
    resourceVersion: "11100224"
    uid: b105299d-92dd-4ef7-a948-86bd5978b397
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: uptime-kuma
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: uptime-kuma
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        containers:
        - env:
          - name: TZ
            value: Europe/Stockholm
          image: louislam/uptime-kuma:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: uptime-kuma
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 384Mi
            requests:
              cpu: 400m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: uptime-kuma-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-30T16:20:39Z"
      lastUpdateTime: "2025-04-02T19:38:14Z"
      message: ReplicaSet "uptime-kuma-844847fb6b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-04-05T12:04:15Z"
      lastUpdateTime: "2025-04-05T12:04:15Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T20:36:45Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: b6fd485d9
    name: cert-manager-b6fd485d9
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: 16a7cd95-1255-47e9-a762-59eb356a4be6
    resourceVersion: "9409220"
    uid: 79dcdd4d-9251-4ef6-8af3-9e252dd1dbb6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: b6fd485d9
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
          pod-template-hash: b6fd485d9
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.16.2
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T19:38:22Z"
    generation: 2
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.13.3
      helm.sh/chart: cert-manager-v1.13.3
      pod-template-hash: 77ddd846b9
    name: cert-manager-cainjector-77ddd846b9
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 9e918745-7e79-4a8b-9263-8a5356d7b9dd
    resourceVersion: "467299"
    uid: 9460ceb5-4601-4669-b19b-7a900ee4a94c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 77ddd846b9
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.13.3
          helm.sh/chart: cert-manager-v1.13.3
          pod-template-hash: 77ddd846b9
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.13.3
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T20:36:45Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: dcc5966bc
    name: cert-manager-cainjector-dcc5966bc
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 9e918745-7e79-4a8b-9263-8a5356d7b9dd
    resourceVersion: "5852050"
    uid: 4fc0038f-bc31-4fc8-be51-dbf57252ab23
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: dcc5966bc
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
          pod-template-hash: dcc5966bc
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.16.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T19:38:22Z"
    generation: 2
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.3
      helm.sh/chart: cert-manager-v1.13.3
      pod-template-hash: fc6d97cdf
    name: cert-manager-fc6d97cdf
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: 16a7cd95-1255-47e9-a762-59eb356a4be6
    resourceVersion: "467241"
    uid: 5768af51-4558-4cd8-889d-f3458cb29ffe
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: fc6d97cdf
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.13.3
          helm.sh/chart: cert-manager-v1.13.3
          pod-template-hash: fc6d97cdf
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.3
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.13.3
          imagePullPolicy: IfNotPresent
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T19:38:22Z"
    generation: 2
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.3
      helm.sh/chart: cert-manager-v1.13.3
      pod-template-hash: 54c86d9759
    name: cert-manager-webhook-54c86d9759
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: 723a9bd6-a5cc-4d2b-aa13-bba5bb5f133f
    resourceVersion: "467280"
    uid: ff8c8da6-ce95-4793-8809-cf06fea3401f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 54c86d9759
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.13.3
          helm.sh/chart: cert-manager-v1.13.3
          pod-template-hash: 54c86d9759
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.13.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-12T20:36:45Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: dfb76c7bd
    name: cert-manager-webhook-dfb76c7bd
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: 723a9bd6-a5cc-4d2b-aa13-bba5bb5f133f
    resourceVersion: "10640700"
    uid: b1c06a3b-85ac-4e62-807a-aee5fb822bbd
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: dfb76c7bd
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
          pod-template-hash: dfb76c7bd
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "14"
    creationTimestamp: "2025-02-16T09:50:17Z"
    generation: 2
    labels:
      app: cloudflared
      pod-template-hash: 57ffb966f7
    name: cloudflared-57ffb966f7
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "1144989"
    uid: 0187981d-3a38-4f5d-bd68-1d86a39f443f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: 57ffb966f7
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-16T10:50:16+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: 57ffb966f7
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "12"
    creationTimestamp: "2025-02-15T18:49:24Z"
    generation: 2
    labels:
      app: cloudflared
      pod-template-hash: 66c4b6f6bc
    name: cloudflared-66c4b6f6bc
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "974892"
    uid: 056a1093-e96c-42c4-82f8-e9cd6fb71000
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: 66c4b6f6bc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-15T19:49:23+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: 66c4b6f6bc
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-02-12T17:34:56Z"
    generation: 2
    labels:
      app: cloudflared
      pod-template-hash: 69f7cb8955
    name: cloudflared-69f7cb8955
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "948026"
    uid: 22071cb6-c6cd-49c4-937a-c1fac7c6d4c7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: 69f7cb8955
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T18:34:56+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: 69f7cb8955
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-02-12T17:33:52Z"
    generation: 2
    labels:
      app: cloudflared
      pod-template-hash: 6f989889f8
    name: cloudflared-6f989889f8
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "446203"
    uid: f6a64989-2654-4fde-81d3-99913d655f78
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: 6f989889f8
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T18:33:52+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: 6f989889f8
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-02-15T16:45:09Z"
    generation: 2
    labels:
      app: cloudflared
      pod-template-hash: 7d7f4b7b9c
    name: cloudflared-7d7f4b7b9c
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "950229"
    uid: 4db476b8-9af9-4e94-873e-d927814150cd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: 7d7f4b7b9c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: 7d7f4b7b9c
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config-cg42tbdgk5
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
    creationTimestamp: "2025-02-15T18:46:09Z"
    generation: 2
    labels:
      app: cloudflared
      pod-template-hash: 7f5697c46c
    name: cloudflared-7f5697c46c
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "974256"
    uid: ad18c12e-7e47-4eac-9ecd-ada896fe65df
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: 7f5697c46c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-15T19:46:08+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: 7f5697c46c
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-02-12T17:31:04Z"
    generation: 4
    labels:
      app: cloudflared
      pod-template-hash: 85f79966f
    name: cloudflared-85f79966f
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "446003"
    uid: 7039b84d-009c-47e5-a925-c39fcbb30393
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: 85f79966f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T18:31:04+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: 85f79966f
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-02-12T17:26:16Z"
    generation: 4
    labels:
      app: cloudflared
      pod-template-hash: 9dc8c894d
    name: cloudflared-9dc8c894d
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "445648"
    uid: 66fb52bd-ecd9-40c1-a64c-4a7867a25d11
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: 9dc8c894d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T18:26:16+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: 9dc8c894d
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "17"
      deployment.kubernetes.io/revision-history: 1,7,9,11,13,15
    creationTimestamp: "2025-02-12T17:19:11Z"
    generation: 14
    labels:
      app: cloudflared
      pod-template-hash: c8b747df5
    name: cloudflared-c8b747df5
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "11099887"
    uid: 51f1d663-a1ca-42ca-92d6-4772ecd64901
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: c8b747df5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: c8b747df5
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 14
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2025-02-15T18:37:30Z"
    generation: 2
    labels:
      app: cloudflared
      pod-template-hash: d44f9dc7b
    name: cloudflared-d44f9dc7b
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "973171"
    uid: ab995ad8-02e4-45e4-a69b-34a6b892231e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: d44f9dc7b
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-15T19:37:29+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: d44f9dc7b
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "16"
    creationTimestamp: "2025-02-16T11:12:48Z"
    generation: 2
    labels:
      app: cloudflared
      pod-template-hash: fdf4cd595
    name: cloudflared-fdf4cd595
    namespace: cloudflare
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cloudflared
      uid: 032f35f4-f00f-4647-b725-b3a5330aad1a
    resourceVersion: "1158797"
    uid: 3ad39b5b-efae-4631-875d-622c3ec5a037
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cloudflared
        pod-template-hash: fdf4cd595
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-16T12:12:47+01:00"
        creationTimestamp: null
        labels:
          app: cloudflared
          pod-template-hash: fdf4cd595
      spec:
        containers:
        - args:
          - tunnel
          - --config
          - /etc/cloudflared/config.yaml
          - run
          image: cloudflare/cloudflared:latest
          imagePullPolicy: Always
          name: cloudflared
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloudflared/config.yaml
            name: config
            subPath: config.yaml
          - mountPath: /etc/cloudflared/credentials.json
            name: creds
            subPath: credentials.json
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cloudflared-config
          name: config
        - name: creds
          secret:
            defaultMode: 420
            secretName: tunnel-credentials
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-10T19:24:00Z"
    generation: 1
    labels:
      app: helm-controller
      pod-template-hash: 7f788c795c
    name: helm-controller-7f788c795c
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: helm-controller
      uid: 7f10ca95-4c9a-4215-9345-ab2b58e6fe6d
    resourceVersion: "10640658"
    uid: 0d8538e9-84ad-4df8-9789-6829cc8b5ea5
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: helm-controller
        pod-template-hash: 7f788c795c
    template:
      metadata:
        annotations:
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: helm-controller
          pod-template-hash: 7f788c795c
      spec:
        containers:
        - args:
          - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
          - --watch-all-namespaces=true
          - --log-level=info
          - --log-encoding=json
          - --enable-leader-election
          env:
          - name: RUNTIME_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.cpu
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.memory
          image: ghcr.io/fluxcd/helm-controller:v1.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 8080
            name: http-prom
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: temp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1337
        serviceAccount: helm-controller
        serviceAccountName: helm-controller
        terminationGracePeriodSeconds: 600
        volumes:
        - emptyDir: {}
          name: temp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-10T19:24:00Z"
    generation: 1
    labels:
      app: kustomize-controller
      pod-template-hash: b4f45fff6
    name: kustomize-controller-b4f45fff6
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kustomize-controller
      uid: 94bbf3e6-d673-4bcd-98a4-b6e4dca7386f
    resourceVersion: "10640666"
    uid: 0b6928a5-2466-48dc-88ad-681d41736161
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kustomize-controller
        pod-template-hash: b4f45fff6
    template:
      metadata:
        annotations:
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: kustomize-controller
          pod-template-hash: b4f45fff6
      spec:
        containers:
        - args:
          - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
          - --watch-all-namespaces=true
          - --log-level=info
          - --log-encoding=json
          - --enable-leader-election
          env:
          - name: RUNTIME_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.cpu
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.memory
          image: ghcr.io/fluxcd/kustomize-controller:v1.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 8080
            name: http-prom
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: temp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1337
        serviceAccount: kustomize-controller
        serviceAccountName: kustomize-controller
        terminationGracePeriodSeconds: 60
        volumes:
        - emptyDir: {}
          name: temp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-10T19:24:01Z"
    generation: 1
    labels:
      app: notification-controller
      pod-template-hash: 556b8867f8
    name: notification-controller-556b8867f8
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: notification-controller
      uid: ddf2940b-c850-47e1-8124-297516d39341
    resourceVersion: "10640713"
    uid: 1366df8e-b946-4e13-8789-72a6016552e4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: notification-controller
        pod-template-hash: 556b8867f8
    template:
      metadata:
        annotations:
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: notification-controller
          pod-template-hash: 556b8867f8
      spec:
        containers:
        - args:
          - --watch-all-namespaces=true
          - --log-level=info
          - --log-encoding=json
          - --enable-leader-election
          env:
          - name: RUNTIME_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.cpu
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.memory
          image: ghcr.io/fluxcd/notification-controller:v1.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          - containerPort: 9292
            name: http-webhook
            protocol: TCP
          - containerPort: 8080
            name: http-prom
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: temp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1337
        serviceAccount: notification-controller
        serviceAccountName: notification-controller
        terminationGracePeriodSeconds: 10
        volumes:
        - emptyDir: {}
          name: temp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-10T19:24:01Z"
    generation: 1
    labels:
      app: source-controller
      pod-template-hash: 77d6cd56c9
    name: source-controller-77d6cd56c9
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: source-controller
      uid: b4f9f035-fe6b-46e2-8083-3df2ba5ba478
    resourceVersion: "10640742"
    uid: 56673d73-1911-471b-9826-b002a1862a9e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: source-controller
        pod-template-hash: 77d6cd56c9
    template:
      metadata:
        annotations:
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: source-controller
          pod-template-hash: 77d6cd56c9
      spec:
        containers:
        - args:
          - --events-addr=http://notification-controller.flux-system.svc.cluster.local./
          - --watch-all-namespaces=true
          - --log-level=info
          - --log-encoding=json
          - --enable-leader-election
          - --storage-path=/data
          - --storage-adv-addr=source-controller.$(RUNTIME_NAMESPACE).svc.cluster.local.
          env:
          - name: RUNTIME_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: TUF_ROOT
            value: /tmp/.sigstore
          - name: GOMAXPROCS
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.cpu
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                containerName: manager
                divisor: "0"
                resource: limits.memory
          image: ghcr.io/fluxcd/source-controller:v1.4.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          - containerPort: 8080
            name: http-prom
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1337
        serviceAccount: source-controller
        serviceAccountName: source-controller
        terminationGracePeriodSeconds: 10
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ww-gitops
      meta.helm.sh/release-namespace: flux-system
    creationTimestamp: "2025-02-12T19:33:03Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: ww-gitops
      app.kubernetes.io/name: weave-gitops
      app.kubernetes.io/part-of: weave-gitops
      pod-template-hash: 6bd7fbd8c4
      weave.works/app: weave-gitops-oss
    name: ww-gitops-weave-gitops-6bd7fbd8c4
    namespace: flux-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ww-gitops-weave-gitops
      uid: 65121c9d-1e50-4604-b577-1d4cc50ef892
    resourceVersion: "5852037"
    uid: 54165ed3-3a05-4346-8311-e09d067886b2
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: ww-gitops
        app.kubernetes.io/name: weave-gitops
        pod-template-hash: 6bd7fbd8c4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: ww-gitops
          app.kubernetes.io/name: weave-gitops
          app.kubernetes.io/part-of: weave-gitops
          pod-template-hash: 6bd7fbd8c4
          weave.works/app: weave-gitops-oss
      spec:
        containers:
        - args:
          - --log-level
          - info
          - --insecure
          env:
          - name: WEAVE_GITOPS_FEATURE_TENANCY
            value: "true"
          - name: WEAVE_GITOPS_FEATURE_CLUSTER
            value: "false"
          image: ghcr.io/weaveworks/wego-app:v0.38.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: weave-gitops
          ports:
          - containerPort: 9001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ww-gitops-weave-gitops
        serviceAccountName: ww-gitops-weave-gitops
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-25T20:57:35Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 56dd5c8f66
    name: headlamp-56dd5c8f66
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "3326589"
    uid: f74eb9f6-6769-40ca-a603-37e6ab9b6167
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 56dd5c8f66
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 56dd5c8f66
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - --in-cluster
          - --plugins-dir=/headlamp/plugins
          env:
          - name: KUBECONFIG
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-25T19:59:55Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 5b9bc84b88
    name: headlamp-5b9bc84b88
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "3322079"
    uid: d81034f0-844d-4fff-a7ad-4a8d3e8461d7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 5b9bc84b88
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 5b9bc84b88
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - -base-url=/
          env:
          - name: KUBERNETES_SERVICE_HOST
            value: 10.43.0.1
          - name: KUBERNETES_SERVICE_PORT
            value: "443"
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: //
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: //
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-25T21:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 66557f8596
    name: headlamp-66557f8596
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "11100012"
    uid: 89700d92-e9ae-4939-bfd9-72acdeb85008
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 66557f8596
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 66557f8596
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - --plugins-dir=/headlamp/plugins
          - --kubeconfig=/home/headlamp/.config/Headlamp/kubeconfigs/config
          env:
          - name: KUBECONFIG
            value: /home/headlamp/.config/Headlamp/kubeconfigs/config
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/headlamp
            name: config-dir
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            mkdir -p /home/headlamp/.config/Headlamp/kubeconfigs/
            cat > /home/headlamp/.config/Headlamp/kubeconfigs/config << EOF
            apiVersion: v1
            kind: Config
            current-context: default
            clusters:
            - name: default
              cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: https://10.43.0.1:443
            contexts:
            - name: default
              context:
                cluster: default
                user: default
            users:
            - name: default
              user:
                tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
            EOF
            chown -R 100:101 /home/headlamp
          command:
          - /bin/sh
          - -c
          image: busybox
          imagePullPolicy: Always
          name: init-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/headlamp
            name: config-dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: config-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-25T20:55:26Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 67d4fb846
    name: headlamp-67d4fb846
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "3326150"
    uid: 6f666087-7461-48f4-bb37-30f8b6f6b0e7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 67d4fb846
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 67d4fb846
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - --in-cluster
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "12"
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-25T20:58:52Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 69f9d5465f
    name: headlamp-69f9d5465f
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "3326972"
    uid: 39bb0343-4830-49a6-9bcf-f1c5c7dc2ee6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 69f9d5465f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 69f9d5465f
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - --plugins-dir=/headlamp/plugins
          env:
          - name: KUBECONFIG
            value: /home/headlamp/.config/Headlamp/kubeconfigs/config
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/headlamp
            name: config-dir
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            mkdir -p /home/headlamp/.config/Headlamp/kubeconfigs/
            cat > /home/headlamp/.config/Headlamp/kubeconfigs/config << EOF
            apiVersion: v1
            kind: Config
            current-context: default
            clusters:
            - name: default
              cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: https://10.43.0.1:443
            contexts:
            - name: default
              context:
                cluster: default
                user: default
            users:
            - name: default
              user:
                tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
            EOF
            chown -R 100:101 /home/headlamp
          command:
          - /bin/sh
          - -c
          image: busybox
          imagePullPolicy: Always
          name: init-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/headlamp
            name: config-dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: config-dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-25T19:43:13Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 77cf55b8c
    name: headlamp-77cf55b8c
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "3306849"
    uid: 2023f7c1-e432-444a-ba4d-60b4f0bc3a88
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 77cf55b8c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 77cf55b8c
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - -base-url=http://headlamp.local/
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: http://headlamp.local//
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: http://headlamp.local//
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
      deployment.kubernetes.io/revision-history: 3,6
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-23T20:07:08Z"
    generation: 6
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 79d8759cb8
    name: headlamp-79d8759cb8
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "3324821"
    uid: 79b6374f-74c9-48da-9318-8cb1e3aee29a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 79d8759cb8
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 79d8759cb8
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - -base-url=/
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: //
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: //
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
      deployment.kubernetes.io/revision-history: 2,5
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-23T19:33:21Z"
    generation: 6
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 7f4f6b686d
    name: headlamp-7f4f6b686d
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "3325509"
    uid: 13138460-641d-4d40-bee8-b782dbed5d60
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 7f4f6b686d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 7f4f6b686d
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: headlamp
      meta.helm.sh/release-namespace: headlamp
    creationTimestamp: "2025-02-23T19:27:17Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: headlamp
      app.kubernetes.io/name: headlamp
      pod-template-hash: 85d6d757fb
    name: headlamp-85d6d757fb
    namespace: headlamp
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: headlamp
      uid: aa274b9e-79cb-405d-a788-e3477a9cd460
    resourceVersion: "2596656"
    uid: 81f6d4e2-8b23-4550-aec9-4854ffd83968
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: headlamp
        app.kubernetes.io/name: headlamp
        pod-template-hash: 85d6d757fb
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: headlamp
          app.kubernetes.io/name: headlamp
          pod-template-hash: 85d6d757fb
      spec:
        containers:
        - args:
          - -in-cluster
          - -plugins-dir=/headlamp/plugins
          - -base-url=/
          env:
          - name: HEADLAMP_PLUGINS_INSTALL_ENABLED
            value: "true"
          - name: HEADLAMP_CONFIG_ENABLE_PLUGIN_INSTALL
            value: "true"
          - name: HOME
            value: /headlamp/config
          - name: XDG_CONFIG_HOME
            value: /headlamp/config
          image: ghcr.io/headlamp-k8s/headlamp:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: //
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: headlamp
          ports:
          - containerPort: 4466
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: //
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 101
            runAsNonRoot: false
            runAsUser: 100
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /headlamp/plugins
            name: plugins
          - mountPath: /headlamp/config
            name: config
          - mountPath: /headlamp/frontend
            name: frontend
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: false
        serviceAccount: headlamp
        serviceAccountName: headlamp
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: config
        - emptyDir: {}
          name: frontend
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QjbPboq3XqJNeiqCgqZHFNcXhkiMnRqD/vhhJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HG0ySGBtXAE5TNFb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfkk2DoRCtmiyeGRkaSQPLDfbp1GNLVZg05rM/iwc4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilpPYqq830H6vPIZHDJGyTYce0EBp7PFE8dWKlaQw1Lj6eTs9PVkkmXnL87U+GzySi1fZuPytHx1juX5i9MXY/3yXIh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5OIKJFzRQEVCvW1YenymwlJQfFuNp2acla41bXvlCMfYq7a6c2yli1tAh51ibAWy/MPh1hZR1rb3dxBy30pNDtQVGaHCvjMETIv9yDCiv5gFSTKyGBEbIeDSqN5CZKYxFuEjC1WgmjoJyuMIxqE4LA0gG8+82zk+z0ZAxDxLyxdk7W6C3k8K6cEc8Dxt4C1mzQYYzzQMuuoFIZ2wS8qgLGimwB+VkCFbP/A1n2vWK591GFynIFCXgKDPlkPJFL0RV2d/z26mouUhln2Cg7Rau2C9Tkigj5q3ECHoOhYr+USXCjNcZ4cHKWAJsaqeEH4GN9JBR6KffKzjtWL8/26AEZiEmThRyup8LwmZCUtT8Ou7p4NOx1dhBYIwej4yOBNwkEVIX5V5JL5PZB8WyS/azi3wt++gt6B4zUBI1da1txYOxbv6YgLZWdjz8a6IB/Nxj7Xe0b2RqP627QDtAeKVZA3QTD2wtyjHddmcpaup0HszEWV3gZtbLdPIa8VDZiAlp5tTTWsOmpqKIQ28wur77+9m42/bq4/PT53cWlOKUI5GVPWQs3bS/6n85uPxHx78biMGhyDg22CWzINjV+pMYNfVTL53zQ/cCOcNB9rjSrtI+EhxN2OX+cY6SbyFQfpOr+p89kvJHmKVzcO3mKpWqsmNhRgYuDeXg80ilCDta45k7uyAdDnfBWxTjrCfRqpNo2kTGkOhg2WlmQawobo/GN1lLM7FvjMVkMu0fzyz2sUYhdDPHdQxe7EhIgL0jhB5d3RppENMKyRM2Qw4wWusKisVJ5n0aqSgNZPDmuR5wXyKbeKof/aeZaSf2Pp7yRaj1ZWm0XXq7mgpy8KGbXMt30X/zyq1Sru8Uab3vzDQe871gec6soctcvCdxW6K5dVGxiafrnCqY0I94XKmz7PtqPxdKsPiovRAxjfXRduxcm2U2a/YoI2YNmVOBbEiX2qIclOe6bodz+wCjD6HxgcxyX7r1BXtpK2b1HnzJLe9O2bftPAAAA//8kyNqv/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-26T20:00:08Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 554b67d6c4
    name: coredns-554b67d6c4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: f768239a-b4bd-4651-ac2a-0359e7773df3
    resourceVersion: "11100002"
    uid: 4cb8d08d-fe8c-42a7-a6d6-0806f66894dc
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 554b67d6c4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-26T21:00:08+01:00"
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 554b67d6c4
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUXWvrRhD9K2WeJdmuE2MEfTBJSksTxySkL8GU8Wpkb7zaXXbHaoTRf7+MJeeDGyf3wn3SfsycPTPnjPaAXv9LIWpnIQf0Pg7qESSw1baAHC7JG9dUZBkSqIixQEbI94DWOkbWzkbZutUTKY7EWdAuU8hsKNNuoAUEkpP37n9LIV3XW8hhO45vbupR8ts/2hZ/zIrC2S8hLFYEORin0KSRXcA1/VBS9Kgkc7tbURqbyFRBm4DBFZlPS9tg3EAOo+m4HJ+ryXlZrtR4ODmbDMfl2bgcnU+HxVRNpvh7gaviTEDfkfTIm9QHV2tpPgXo7k/wiZ6UsAnUxf+lpcjmWleaIR8mEMmQYhckqEJWm+uXCtD706+2As4BmdbN4QFnjLbrB18gUwf2/GCxRm1wZQjyUZsAN1443r2LlXOqvDnmvXGL+QkufaHKWUZtKUTIH2VbVSiWfDzdvsgYxKdpqpwt9RoSGBCrQbfrP9lTdBaWCZCtD8i9KIvby//ms5ur+8Xs4goSqNHs6M/gKiFTajLFHZUv6wWyiH+sMXtVrm3bZQK6Ev/lENCqDYXBx5zzepgNs/EQ+oTFzpiFM1o1kMPf5dzxIlDshu8r79TO7Cq6cTvLXccqWfY837bhFas7SLtMaJdC3AftgubmwmCM8y6uc2FqXUGpCpq1QiPtplBrRTOl5KX5Z/zSPjbFLhgSYGcoHH8gj3vYkhR90cMfhj7eWtPIEHuJFGvD1bOOHKFN9kBlSYohh7m7VxsqdkYGvoM5UA3OUCZjFCwxRZlZMVVwJvUGLf1S5AojH3T4AHJ5VOdoZWn7DXpx0/ey9t5tT8vUtu23AAAA///s6eu+uAUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:28Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 5cf85fd84d
    name: local-path-provisioner-5cf85fd84d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: d5cc4dec-c4b7-4b71-9416-99b1cfd356bd
    resourceVersion: "5852191"
    uid: 2092e1f8-4754-43a4-aa6f-28d7daf6b021
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 5cf85fd84d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 5cf85fd84d
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.30
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoHQrsQHRPiV6tIUFXqnUxUh4x3Ah9f2eWZpuIj//TS7SUovpelV9wUt9vPzmzfjmXtQwfyKkYx3UIAKgc53OSSwNa6EAq4wWL+v0DEkUCGrUrGC4h6Uc54VG+9I/vrlH6iZkM+i8WdaMVs8M/7cCAkkJ/f9Z4cxXe+2UMD2go52dnnyv1+MKwfDsvTuRQqnKoRCJEajKSWMO4xpeSz/ZQIKSgvLtl5iSntirOCQgFVLtE2Y21eUqhCeXfQd9o2iDRSAeYadbg/zC+xeqn6np1+XZVl2V69x2bl4vcq7/X6/uyrlvm/GAu36CYkUUIvAiDsjuZwYYh/316YyDEWWAKFFzT4KqFKsN9cvB3UQYo6Kcb1vyL21xq0/hlIxtkR3H53aKWPV0iIU+SEB3gfR9+ErrKxjFezjuaNC+gFzT1pyFLj2jpVxGAmKT/eg4lo+IE01Rk5LEwfnXAVIIE0JdR0xDT7yIM86vaxZFUMtchoirjBGLFNVlhGJUomIBm8dY3TKvp0m47unz4knbrQdU9SEqfMlpsSKa2puagCt/DQieVvL2xnkPWp22FKqTdhgTKk2jDSYX88W49HVZCy/s+Hit7fzyWI4ni06vcvFm9G7xWwyvHjVTb7gPvwQ6h9seefVI67TuzzFdhJ1xDaaDEeTYSdbTN9f/55fZL1vkT0DwW0CplJryW5UTm8wnlcmRi8Z+DrdxS476591IAFrduiQaBr9simolTK2jjjfRKSNtyUUFwlsmMMbZNkPiuURnsvBvyCBJiNFgxD/SW+wqa/JfD6dSVkZZ9goe4VW7WeovSsJisssgYDR+PJpKZenVWuNREeX5wmwqdDX/AX4nXctatqyfariaSOwqc6nc49qQ/TstbdQwHw0hcNtAhFVaX7KETm5/3lLnjvS+ReGyEOoo0ZqW9efNRI33zrUUECeZVUzdiof91BAP3tn2qYkL9jwfuQd410Tj7LWf55GszMW1zgmrWwznaBYKUvYWvTe2f0H7/n/xuJD7yw41rJbuyHdeCe7X619JIySiCw7JLDztq7wna/dQ74q+Zw+WNn2l4dkcRWk68DhVvITovGNYKuIblpEK6BtFDoaNlpZMR7jzmgcai3cNydKhr3F+Dh+P93DFsWg0QNNMzJJopXBFAQpnR/Gd0YMPiT3gKsVakn4jZ/pDZa1lR7W0jSSord4Jh0tOmQkGWVSndHbNFjl8D9lrhRxO0WfU94++t5GilXg/ZWRQXb4ltuHw+HvAAAA//9PFN5y1QgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:28Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5985cbc9d7
    name: metrics-server-5985cbc9d7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 199e1eae-0360-4075-90d2-fad0a9806117
    resourceVersion: "5851976"
    uid: 23e60b8d-b20d-4ef4-82c5-e054dcc5e60f
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 5985cbc9d7
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 5985cbc9d7
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: pushprox-kube-proxy
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-04-02T20:58:18Z"
    generation: 1
    labels:
      component: kube-proxy
      k8s-app: pushprox-kube-proxy-proxy
      pod-template-hash: 59874cc749
      provider: kubernetes
      release: pushprox-kube-proxy
    name: pushprox-kube-proxy-proxy-59874cc749
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: pushprox-kube-proxy-proxy
      uid: 7cf93552-bf66-4102-9e06-af1fe833d1b0
    resourceVersion: "11096440"
    uid: 104a2ebf-a6b7-4537-8536-ba50d977d587
  spec:
    replicas: 1
    selector:
      matchLabels:
        component: kube-proxy
        k8s-app: pushprox-kube-proxy-proxy
        pod-template-hash: 59874cc749
        provider: kubernetes
        release: pushprox-kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          component: kube-proxy
          k8s-app: pushprox-kube-proxy-proxy
          pod-template-hash: 59874cc749
          provider: kubernetes
          release: pushprox-kube-proxy
      spec:
        containers:
        - command:
          - pushprox-proxy
          image: rancher/pushprox-proxy:v0.1.0-rancher2-proxy
          imagePullPolicy: IfNotPresent
          name: pushprox-proxy
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: pushprox-kube-proxy-proxy
        serviceAccountName: pushprox-kube-proxy-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-11T19:01:08Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 5b6b4f8b7f
    name: traefik-5b6b4f8b7f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 5cc7e51a-21a9-4168-8d6d-cabf2c0d9390
    resourceVersion: "295442"
    uid: 941063ec-59bf-4fa0-a6e8-846ae2cd71c1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 5b6b4f8b7f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-11T20:01:08+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 5b6b4f8b7f
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:41Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 5d45fc8cc9
    name: traefik-5d45fc8cc9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 5cc7e51a-21a9-4168-8d6d-cabf2c0d9390
    resourceVersion: "295110"
    uid: 0d448541-4d81-44a1-be76-f2c6a22b8ead
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 5d45fc8cc9
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 5d45fc8cc9
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-11T19:16:10Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 686b6b64c4
    name: traefik-686b6b64c4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 5cc7e51a-21a9-4168-8d6d-cabf2c0d9390
    resourceVersion: "11100075"
    uid: 521ebdac-67a2-469a-883a-f7c1098dd34b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 686b6b64c4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-11T20:16:10+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 686b6b64c4
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-11T19:05:51Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 758c8897
    name: traefik-758c8897
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 5cc7e51a-21a9-4168-8d6d-cabf2c0d9390
    resourceVersion: "296581"
    uid: 1fa122bb-9575-46d1-8120-5bad368719fc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 758c8897
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-11T20:05:51+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 758c8897
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-11T19:14:09Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 7b5cdc966
    name: traefik-7b5cdc966
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 5cc7e51a-21a9-4168-8d6d-cabf2c0d9390
    resourceVersion: "297003"
    uid: 12f9c443-df3f-4034-ac4b-9665fb7e2d1e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 7b5cdc966
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-11T20:14:09+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 7b5cdc966
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-11T19:04:06Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 7dbcf89464
    name: traefik-7dbcf89464
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 5cc7e51a-21a9-4168-8d6d-cabf2c0d9390
    resourceVersion: "295695"
    uid: 1e9ac161-6417-4039-bdfb-f7cad8e03a9a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 7dbcf89464
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-11T20:04:06+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 7dbcf89464
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-11T19:13:14Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 7fdcc4b8f9
    name: traefik-7fdcc4b8f9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 5cc7e51a-21a9-4168-8d6d-cabf2c0d9390
    resourceVersion: "296737"
    uid: 311cc4cd-46c1-4fc9-aaea-370604e97429
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 7fdcc4b8f9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-11T20:13:14+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 7fdcc4b8f9
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
    creationTimestamp: "2025-02-26T18:45:23Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: front
      helm.sh/chart: kubeshark-52.4
      pod-template-hash: 6dd457cff6
    name: kubeshark-front-6dd457cff6
    namespace: kubeshark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubeshark-front
      uid: 454f503b-3cf8-4599-bb0b-79e9c55d96a3
    resourceVersion: "11099964"
    uid: bc73433d-cd02-4726-ac25-5e1201989d87
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: kubeshark
        app.kubernetes.io/name: kubeshark
        app.kubeshark.co/app: front
        pod-template-hash: 6dd457cff6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kubeshark
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kubeshark
          app.kubernetes.io/version: "52.4"
          app.kubeshark.co/app: front
          helm.sh/chart: kubeshark-52.4
          pod-template-hash: 6dd457cff6
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux
        containers:
        - env:
          - name: REACT_APP_AUTH_ENABLED
            value: "true"
          - name: REACT_APP_AUTH_TYPE
            value: oidc
          - name: REACT_APP_AUTH_SAML_IDP_METADATA_URL
            value: ' '
          - name: REACT_APP_TIMEZONE
            value: ' '
          - name: REACT_APP_SCRIPTING_DISABLED
            value: "false"
          - name: REACT_APP_TARGETED_PODS_UPDATE_DISABLED
            value: "false"
          - name: REACT_APP_PRESET_FILTERS_CHANGING_ENABLED
            value: "true"
          - name: REACT_APP_BPF_OVERRIDE_DISABLED
            value: "false"
          - name: REACT_APP_RECORDING_DISABLED
            value: "false"
          - name: REACT_APP_STOP_TRAFFIC_CAPTURING_DISABLED
            value: "false"
          - name: REACT_APP_CLOUD_LICENSE_ENABLED
            value: "true"
          - name: REACT_APP_SUPPORT_CHAT_ENABLED
            value: "true"
          - name: REACT_APP_DISSECTORS_UPDATING_ENABLED
            value: "true"
          - name: REACT_APP_SENTRY_ENABLED
            value: "false"
          - name: REACT_APP_SENTRY_ENVIRONMENT
            value: production
          image: docker.io/kubeshark/front:v52.4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 3
            periodSeconds: 1
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          name: kubeshark-front
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 3
            periodSeconds: 1
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 750m
              memory: 1Gi
            requests:
              cpu: 50m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx/conf.d/default.conf
            name: nginx-config
            readOnly: true
            subPath: default.conf
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubeshark-service-account
        serviceAccountName: kubeshark-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubeshark-nginx-config-map
          name: nginx-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kubeshark
      meta.helm.sh/release-namespace: kubeshark
    creationTimestamp: "2025-02-26T18:45:23Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: kubeshark
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubeshark
      app.kubernetes.io/version: "52.4"
      app.kubeshark.co/app: hub
      helm.sh/chart: kubeshark-52.4
      pod-template-hash: 7c9f6c7f7f
    name: kubeshark-hub-7c9f6c7f7f
    namespace: kubeshark
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubeshark-hub
      uid: e1722460-f304-4c42-ac79-ca9676a4e479
    resourceVersion: "11100111"
    uid: 99a139fe-81d7-4fc6-942f-bb0f3074ad01
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: kubeshark
        app.kubernetes.io/name: kubeshark
        app.kubeshark.co/app: hub
        pod-template-hash: 7c9f6c7f7f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kubeshark
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kubeshark
          app.kubernetes.io/version: "52.4"
          app.kubeshark.co/app: hub
          helm.sh/chart: kubeshark-52.4
          pod-template-hash: 7c9f6c7f7f
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux
        containers:
        - command:
          - ./hub
          - -port
          - "8080"
          - -loglevel
          - warning
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: SENTRY_ENABLED
            value: "false"
          - name: SENTRY_ENVIRONMENT
            value: production
          - name: KUBESHARK_CLOUD_API_URL
            value: https://api.kubeshark.co
          - name: PROFILING_ENABLED
            value: "false"
          image: docker.io/kubeshark/hub:v52.4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          name: hub
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
          resources:
            limits:
              memory: 5Gi
            requests:
              cpu: 50m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/saml/x509
            name: saml-x509-volume
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubeshark-service-account
        serviceAccountName: kubeshark-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - name: saml-x509-volume
          projected:
            defaultMode: 420
            sources:
            - secret:
                items:
                - key: AUTH_SAML_X509_CRT
                  path: kubeshark.crt
                name: kubeshark-saml-x509-crt-secret
            - secret:
                items:
                - key: AUTH_SAML_X509_KEY
                  path: kubeshark.key
                name: kubeshark-saml-x509-key-secret
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-28T19:40:03Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 55c46dd6f6
    name: kube-prometheus-stack-grafana-55c46dd6f6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "4381671"
    uid: a048d25d-49b2-43c4-a371-24493744c8c7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 55c46dd6f6
    template:
      metadata:
        annotations:
          checksum/config: 3d08a4d2bbc80813fdcfbd8b71f56c0f9f2509d535399da79c655e2c5f31c449
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-02-28T20:40:03+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 55c46dd6f6
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "true"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "16"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-02T19:38:10Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 56cc479d4b
    name: kube-prometheus-stack-grafana-56cc479d4b
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "11100118"
    uid: fb120f87-dd99-47c2-b0d6-6ed1c54c9afc
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 56cc479d4b
    template:
      metadata:
        annotations:
          checksum/config: c7b16e62bdf0a9cd5b1d531b40884dc4e40e401c55b4ace6e201920579bc6c38
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-03-21T22:54:39+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 56cc479d4b
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - k3s-w-3
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 800m
              memory: 256Mi
            requests:
              cpu: 300m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-29T15:51:20Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 66849b7d7f
    name: kube-prometheus-stack-grafana-66849b7d7f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "10015741"
    uid: d604730a-6d81-4647-bd55-581cb4624d01
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 66849b7d7f
    template:
      metadata:
        annotations:
          checksum/config: c7b16e62bdf0a9cd5b1d531b40884dc4e40e401c55b4ace6e201920579bc6c38
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-03-21T22:54:39+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 66849b7d7f
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 400m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-28T19:38:27Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 688cb9d7b4
    name: kube-prometheus-stack-grafana-688cb9d7b4
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "4381053"
    uid: dc9df950-a45c-4a9e-b774-d36c45fb86c8
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 688cb9d7b4
    template:
      metadata:
        annotations:
          checksum/config: 3d08a4d2bbc80813fdcfbd8b71f56c0f9f2509d535399da79c655e2c5f31c449
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-02-28T20:35:25+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 688cb9d7b4
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "true"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-04T19:22:53Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 7985fd4bc5
    name: kube-prometheus-stack-grafana-7985fd4bc5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "5618749"
    uid: ec2f5fa8-4917-4e2d-85cc-ea7236be0995
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 7985fd4bc5
    template:
      metadata:
        annotations:
          checksum/config: 3d08a4d2bbc80813fdcfbd8b71f56c0f9f2509d535399da79c655e2c5f31c449
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-02-28T20:40:03+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 7985fd4bc5
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "14"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-29T20:03:02Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 7996c5445
    name: kube-prometheus-stack-grafana-7996c5445
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "10015584"
    uid: 49084013-1ccd-4ff5-81dc-39b8f593403a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 7996c5445
    template:
      metadata:
        annotations:
          checksum/config: c7b16e62bdf0a9cd5b1d531b40884dc4e40e401c55b4ace6e201920579bc6c38
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-03-21T22:54:39+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 7996c5445
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 600m
              memory: 256Mi
            requests:
              cpu: 150m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-28T19:44:17Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 7c9c654c85
    name: kube-prometheus-stack-grafana-7c9c654c85
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "5129751"
    uid: dd9981e3-c17c-4994-9ebe-8f8551588e78
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 7c9c654c85
    template:
      metadata:
        annotations:
          checksum/config: 3d08a4d2bbc80813fdcfbd8b71f56c0f9f2509d535399da79c655e2c5f31c449
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-02-28T20:40:03+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 7c9c654c85
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "12"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-21T21:54:39Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 85d745f849
    name: kube-prometheus-stack-grafana-85d745f849
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "9897296"
    uid: c36f6aa9-bfad-4e65-9b47-e6105cdc1b47
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 85d745f849
    template:
      metadata:
        annotations:
          checksum/config: c7b16e62bdf0a9cd5b1d531b40884dc4e40e401c55b4ace6e201920579bc6c38
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-03-21T22:54:39+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 85d745f849
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-05T18:37:09Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: 86958c4b44
    name: kube-prometheus-stack-grafana-86958c4b44
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "6232704"
    uid: 68535922-cf91-4f40-95a7-88836b15e102
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 86958c4b44
    template:
      metadata:
        annotations:
          checksum/config: 3d08a4d2bbc80813fdcfbd8b71f56c0f9f2509d535399da79c655e2c5f31c449
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-02-28T20:40:03+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: 86958c4b44
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-10T19:13:01Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: d98dfb6c9
    name: kube-prometheus-stack-grafana-d98dfb6c9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "9322830"
    uid: b86ad585-5dce-4c05-bb1d-1ef2f259b664
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: d98dfb6c9
    template:
      metadata:
        annotations:
          checksum/config: c7b16e62bdf0a9cd5b1d531b40884dc4e40e401c55b4ace6e201920579bc6c38
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-02-28T20:40:03+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: d98dfb6c9
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "15"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-30T07:43:57Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/name: grafana
      pod-template-hash: fbbd85ccb
    name: kube-prometheus-stack-grafana-fbbd85ccb
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-grafana
      uid: 5ab7f378-165c-44f1-b14d-dc908beb8dcd
    resourceVersion: "10629409"
    uid: e3ad6f9f-6e1c-4da9-b0fd-546ac342c61c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: fbbd85ccb
    template:
      metadata:
        annotations:
          checksum/config: c7b16e62bdf0a9cd5b1d531b40884dc4e40e401c55b4ace6e201920579bc6c38
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-03-21T22:54:39+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/name: grafana
          pod-template-hash: fbbd85ccb
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - k3s-w-3
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: monitoring
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.25.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prometheus-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prometheus-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 600m
              memory: 256Mi
            requests:
              cpu: 150m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prometheus-stack-grafana
        serviceAccountName: kube-prometheus-stack-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prometheus-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prometheus-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:53Z"
    generation: 2
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      pod-template-hash: 57bcb98f6
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics-57bcb98f6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-kube-state-metrics
      uid: 667ce338-4f99-4046-bb70-5e3681d5a078
    resourceVersion: "4377382"
    uid: 98748f9c-c592-47e6-b00f-2611c04301b2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 57bcb98f6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.10.1
          helm.sh/chart: kube-state-metrics-5.15.2
          pod-template-hash: 57bcb98f6
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-kube-state-metrics
        serviceAccountName: kube-prometheus-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-28T19:11:43Z"
    generation: 2
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      pod-template-hash: 5979b9f5b7
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics-5979b9f5b7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-kube-state-metrics
      uid: 667ce338-4f99-4046-bb70-5e3681d5a078
    resourceVersion: "4579324"
    uid: 7742a169-127e-4303-9ca9-cd5edb71cf6a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 5979b9f5b7
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.10.1
          helm.sh/chart: kube-state-metrics-5.15.2
          pod-template-hash: 5979b9f5b7
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 50m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-kube-state-metrics
        serviceAccountName: kube-prometheus-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-02T19:38:10Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      pod-template-hash: 668b4bf9ff
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics-668b4bf9ff
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-kube-state-metrics
      uid: 667ce338-4f99-4046-bb70-5e3681d5a078
    resourceVersion: "11084648"
    uid: ca01b861-68ad-48e0-bb76-49ffe60ea181
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 668b4bf9ff
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.10.1
          helm.sh/chart: kube-state-metrics-5.15.2
          pod-template-hash: 668b4bf9ff
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 300m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-kube-state-metrics
        serviceAccountName: kube-prometheus-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-01T22:35:03Z"
    generation: 2
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.10.1
      helm.sh/chart: kube-state-metrics-5.15.2
      pod-template-hash: 857f9b4f47
      release: kube-prometheus-stack
    name: kube-prometheus-stack-kube-state-metrics-857f9b4f47
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-kube-state-metrics
      uid: 667ce338-4f99-4046-bb70-5e3681d5a078
    resourceVersion: "10629426"
    uid: c23f2f6d-7eca-40f4-a83f-3ba75d82a436
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 857f9b4f47
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.10.1
          helm.sh/chart: kube-state-metrics-5.15.2
          pod-template-hash: 857f9b4f47
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-kube-state-metrics
        serviceAccountName: kube-prometheus-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-28T19:11:43Z"
    generation: 2
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      heritage: Helm
      pod-template-hash: "5579778498"
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator-5579778498
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-operator
      uid: 2898c080-8b5f-4fd8-bf99-45efa86fed7d
    resourceVersion: "4582889"
    uid: 76021c9e-c394-4197-934e-6b1c4776818a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: "5579778498"
        release: kube-prometheus-stack
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 55.5.0
          chart: kube-prometheus-stack-55.5.0
          heritage: Helm
          pod-template-hash: "5579778498"
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prometheus-stack-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.32.5
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-operator
        serviceAccountName: kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prometheus-stack-admission
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-02T19:38:10Z"
    generation: 2
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      heritage: Helm
      pod-template-hash: 5d9db74bcd
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator-5d9db74bcd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-operator
      uid: 2898c080-8b5f-4fd8-bf99-45efa86fed7d
    resourceVersion: "11117607"
    uid: a5859097-c6b6-417f-b28f-db56490f6467
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 5d9db74bcd
        release: kube-prometheus-stack
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 55.5.0
          chart: kube-prometheus-stack-55.5.0
          heritage: Helm
          pod-template-hash: 5d9db74bcd
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prometheus-stack-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.32.5
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 256Mi
            requests:
              cpu: 200m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-operator
        serviceAccountName: kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prometheus-stack-admission
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-03-01T22:59:19Z"
    generation: 2
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      heritage: Helm
      pod-template-hash: 75b94cbbb4
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator-75b94cbbb4
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-operator
      uid: 2898c080-8b5f-4fd8-bf99-45efa86fed7d
    resourceVersion: "10629481"
    uid: 9b5c774f-1c3b-4a4f-90f0-95420247545f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 75b94cbbb4
        release: kube-prometheus-stack
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 55.5.0
          chart: kube-prometheus-stack-55.5.0
          heritage: Helm
          pod-template-hash: 75b94cbbb4
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prometheus-stack-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.32.5
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 300m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-operator
        serviceAccountName: kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prometheus-stack-admission
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-23T21:46:53Z"
    generation: 2
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      heritage: Helm
      pod-template-hash: 7bc97ff7d6
      release: kube-prometheus-stack
    name: kube-prometheus-stack-operator-7bc97ff7d6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prometheus-stack-operator
      uid: 2898c080-8b5f-4fd8-bf99-45efa86fed7d
    resourceVersion: "4377201"
    uid: 885d423a-2eaa-48f7-b9db-459800194d1f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 7bc97ff7d6
        release: kube-prometheus-stack
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/instance: kube-prometheus-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 55.5.0
          chart: kube-prometheus-stack-55.5.0
          heritage: Helm
          pod-template-hash: 7bc97ff7d6
          release: kube-prometheus-stack
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prometheus-stack-kubelet
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.32.5
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.70.0
          imagePullPolicy: IfNotPresent
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-operator
        serviceAccountName: kube-prometheus-stack-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prometheus-stack-admission
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsWF1v27gS/S9EH3oBUf7IR3sF6CGoXVzjbhwjWfSlCAqGGtlcUyQ7pNRoA//3BUk5lhOnTRbZ3T7sQxCLOjMcnnNID31HmBGfAK3QimSEGWMHzYgkZC1UQTIyASN1W4FyJCEVOFYwx0h2RxSrgGSEW0E5mFVpjayXQlGDuhE+GyBJAsoaxj0UtV4HLEmI/qYAL6EEBMXBkuzzk4XcSM3XFx4/AQkuvC6ZtJAQrpVDLSUgyRzWcLjqrtL76ak2gMxpX18tPPwITsqT06MTWp4OT+nxzbigbMyPaHl8Oj75L38/Kt8B2VxvEmINcL96BCMFZ5Zk44RYkMB9vuyOVMzx1S/sBqT1j8yYH5G08WkdMgfL1oe41vhyL4EjMAdkkxAHlZH+s5+gp4F8yTyRLiaUPYSswKHgNlbTLZKVpVDChaqULuBs97xJiNHFmXKiN0gQvtYCoZjUKNTyiq+gqKVQy9lS6fvh6S3wOsr4uVvC1UMCp7cGwfrCozXW0EZPeOts1cvITJGENEzWwUHfX/510M9po6Vetv8PCdf1DaACBzYVerDS1gWvbK49Cx1bgJ07cRkmobTJhyQhlPrpWFH4QvM3b88mk8vp1dV/wisJrACkYVlCq9ybM7xwogJdu3xcHQ3tISi93zF5f788xklgFmhRIwszjKvRu8MJERR8owWwQgoF+ag6fhLosKUGUOgiH59acp0QUE1YfbeHukVuWScZGXAr/F+f69Rqviab64SIii3D3oOlsA7bdP0+UG3FklqnkS0hBDPnGF8BZs1xepL6XR8iF7WUCy0F92LNyrl2CwS7t6v70SQhCFbXGE4U79FGy7qCc10rF1Ws/McFc6uu9F0iXzM4WggMO70v+HeE3nrhh7ruLPATC+5BcOuQ0Xj20O1hE6t/jh9qJW6zweB1bGEVM3alnfPOeJcO0/ELndFL8K85XsEcK6YKCTQyR4WqLVBA1JiHb+SAKYG5GoEumQObXwLXDeCnEDG9NUx5M3xkQtYI/5CtEKz43VtqlI6GLz5tuuifyk9RLKEcYMMktY6hy0+Gw+qV7Lav6f+00rj4dAlcMlFFynY5DUIDym1NUukCKNeqia3d03P/TRb+mc63XrD/5nu5F/f7u7/Ej77xE0X+5u38YjL9MptEK/omNY+9VmRVFUYL5fI3bz9czb5M55PFxWz+675vd+26BWwAd14oUDSAvpid8dKYPuVWpPFZV9FgopCiEi6no2hOjRw8Yu2bOcmlAOWeUnRxMfkyW2wF/Yi68k1nKUAWl1Def+5Yso652qZGF7MF2Ww8M12ejo1nJzLAU0/lPDSY/Uy+ovnZ+fRqcfZh+tx8W+OmuwvWXtK+CH/WvV9r1nrXempDQPyfNUfpaJQOX2jVfmP+al5NHgBta3dA389TP/IIJsXNoNJFLaEHl+KG7gYRWHGhZBsvlo8yFNA8mMiPPIKB44G2cKpzrUqxHPRo2X/zONxVJki1hv66/BN1ldlu1+uEGBQahWs/SGbtvOOptQ4qymVtHSDlKJzgTBJ/X8VGcDjj3E8271+P74XqW4NaRrYSRXWgMq6diHhhg0LUFcnIOVQaW39fPayUpymu7I6YnmCbw4odgveF2xxU7qnIKM9BxSL758x4vHBQ9e+cgZEthQFIf7PaO7jLHAfTMOilUA9/bdgT+HvSP4/Uh+rHnw/8MeV30uaPAAAA//9QSwcIBtD1QAEFAADQEQAAUEsBAhQAFAAIAAgAAAAAAAbQ9UABBQAA0BEAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAADcFAAAAAA==
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-08T14:56:56Z"
    generation: 2
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 866c4df6fc
    name: csi-cephfsplugin-provisioner-866c4df6fc
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-cephfsplugin-provisioner
      uid: 8ace202a-f51c-4461-8ce1-22193923d0cd
    resourceVersion: "5912011"
    uid: 74e5096b-1aeb-468c-88d3-8fa6f288d243
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: csi-cephfsplugin-provisioner
        pod-template-hash: 866c4df6fc
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin-provisioner
          contains: csi-cephfsplugin-metrics
          pod-template-hash: 866c4df6fc
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-cephfsplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --v=0
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --timeout=2m30s
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --feature-gates=HonorPVReclaimPolicy=true
          - --prevent-volume-mode-conversion=true
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --controllerserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --forcecephkernelclient=true
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-provisioner-sa
        serviceAccountName: rook-csi-cephfs-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: socket-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsWF1v27gS/S9EH3oBUbGdj/YK0ENQu7jG3SRGsuhLERQMObK5pkh2SKnxBv7vC5JyLCdOmyyyu33YhyAWdWY4POeQHvqOMCs/ATppNCkIs9YdtEOSkaXUghRkDFaZVQ3ak4zU4JlgnpHijmhWAykId5JysIvKWdXMpaYWTStDNkCSRZSzjAcoGrOMWJIR800DXkIFCJqDI8XnJwu5UYYvLwJ+DAp8fF0x5SAj3GiPRilAUnhsYH/VXaX301NjAZk3ob5GBvghHFfHJ4fHtDoZnNCjm5GgbMQPaXV0Mjr+L38/rN4BWV+vM+Is8LB6BKskZ44Uo4w4UMBDvuKO1MzzxS/sBpQLj8zaH5G0Dmk9Mg/zVQjxKxvKvQSOwDyQdUY81FaFz2GCngbqJfMkupjUbh+yBo+Su1RNt0hWVVJLH6vSRsDp9nmdEWvEqfayN0gQvjYSQYwblHp+xRcgGiX1fDrX5n54cgu8STJ+7pZw9ZDAya1FcKHwZI0lrJIngnU26hVkqklGWqaa6KDvL/866ueNNcrMV/+PCZfNDaAGDy6X5mBhnI9eWV8HFjq2ADt34jxOQmlbDkhGKA3TMSFCoeWbt6fj8eXk6uo/8ZUCJgBpXJY0ugzmjC+8rME0vhzVhwO3D0rvd0zZ3y+PcQqYAyoaZHGGUT18tz8hgoZvVAATSmooh/XRk0CPK2oBpRHl6MSR64yAbuPquz3ULXLDOinIAXcy/PW5zp3hS7K+zois2TzuPZhL53GVL99Hqp2cU+cNsjnEYOY94wvAoj3Kj/Ow62PkrFFqZpTkQaxpdW78DMHt7Op+NMkIgjMNxhMleLQ1qqnhzDTaJxXr8HHG/KIrfZso1AyeColxp/cF/47QGy/8UNetBX5iwQMIbj0yms4eujlsUvXP8UOj5W1xcPA6tnCaWbcw3gdnvMsH+eiFzugl+Nccr2COBdNCAU3MUakbBxQQDZbxGzliKmC+QaBz5sGVl8BNC/gpRkxuLdPBDB+ZVA3CP2QrBCd/D5Ya5sPBi0+bLvqn8lMSS2oP2DJFnWfoy+PBoH4lu+1q+j+jDc4+XQJXTNaJsm1Oi9CC9huT1EYA5Ua3qbV7eu6/ycI/0/nWCw7ffC/34m5/95f4MTR+UpRv3p5fjCdfpuNkxdCklqnXSqxqYY3Uvnzz9sPV9MvkfDy7mJ7/uuvbbbvuAFvArRcEyhYwFLM1Xp7S59zJPD2bOhlMCiVr6Us6TOY0yCEglqGZU1xJ0P4pRWcX4y/T2UbQj2jq0HRWEpS4hOr+c8eS88w3LrdGTGdkvQ7MdHk6Np6dyALPA5XnscHsZwoVnZ+eTa5mpx8mz823MW6+vWDtJO2L8Gfd+7Vhq+DaQG0MSP+L9jAfDvPBC63ab8xfzavZA6BbuS0w9PM0jDyCKXlzUBvRKOjBlbyh20EEJi60WqWL5aMMAtoHE4WRRzDwPNIWT3VudCXnBz1adt88Dve1jVItob+u8ER9bTfb9TrcQLGVHE45D+Hn/QvvPfV9saljZEN64htq61djma5gIGRTk4KcQW1wFW6g+7kPC0+13hHbk2C9X4N98L4U671aPBWZCN+rQeLzjNmAlx7q/i0yMqIa5wE74ulvzgRPdpnTYB4HA7n64e8HO5J9T8znkfpQz/SDQDh4wt5Y/xEAAP//UEsHCBmy5nPnBAAAohEAAFBLAQIUABQACAAIAAAAAAAZsuZz5wQAAKIRAAAIAAAAAAAAAAAAAAAAAAAAAABvcmlnaW5hbFBLBQYAAAAAAQABADYAAAAdBQAAAAA=
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-09T07:50:58Z"
    generation: 1
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 87fd75775
    name: csi-cephfsplugin-provisioner-87fd75775
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-cephfsplugin-provisioner
      uid: 8ace202a-f51c-4461-8ce1-22193923d0cd
    resourceVersion: "11099971"
    uid: 7556307a-91a5-4946-91d2-a15bca6d3dee
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: csi-cephfsplugin-provisioner
        pod-template-hash: 87fd75775
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin-provisioner
          contains: csi-cephfsplugin-metrics
          pod-template-hash: 87fd75775
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-cephfsplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --v=0
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --timeout=2m30s
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --feature-gates=HonorPVReclaimPolicy=true
          - --prevent-volume-mode-conversion=true
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --controllerserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --forcecephkernelclient=true
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-provisioner-sa
        serviceAccountName: rook-csi-cephfs-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: socket-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-08T14:56:56Z"
    generation: 2
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 54b4855f96
    name: csi-rbdplugin-provisioner-54b4855f96
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-rbdplugin-provisioner
      uid: c6a92e8a-3a3f-4243-8b6f-0cf7f311de79
    resourceVersion: "5911960"
    uid: 38952679-1e66-48ed-82db-e544c3b5ca8d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: csi-rbdplugin-provisioner
        pod-template-hash: 54b4855f96
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin-provisioner
          contains: csi-rbdplugin-metrics
          pod-template-hash: 54b4855f96
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-rbdplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          - --extra-create-metadata=true
          - --prevent-volume-mode-conversion=true
          - --feature-gates=HonorPVReclaimPolicy=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --v=0
          - --timeout=2m30s
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --controllerserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-configs
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/secrets/tokens
            name: oidc-token
            readOnly: true
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-provisioner-sa
        serviceAccountName: rook-csi-rbd-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - emptyDir:
            medium: Memory
          name: socket-dir
        - name: ceph-csi-configs
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: csi-cluster-config-json
                  path: config.json
                name: rook-ceph-csi-config
            - configMap:
                items:
                - key: csi-mapping-config-json
                  path: cluster-mapping.json
                name: rook-ceph-csi-mapping-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - name: oidc-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: ceph-csi-kms
                expirationSeconds: 3600
                path: oidc-token
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzkmE1v2zgTx78L0UMfQJRf8tI+AnwIahcb7CYxkqKXIihocmSzpkh2SKnxBv7uC1KyrdhKmuy2WBR7MCyRw+Fw/j+OSd8TZuVHQCeNJhlh1rpeNSAJWUotSEbGYJVZFaA9SUgBngnmGcnuiWYFkIxwJynOhFXlXGpq0VQyuAIkSTRxlvFgh8YsKQe7IAkx3zTgNeSAoDk4kn16NIqZMnx5FezHoMDH7pwpBwnhRns0SgGSzGMJ3SE3YW6np8YCMm9CfKUM5kdwkp+cHp3Q/LR/So9nQ0HZkB/R/Ph0ePJ//naQvwGyvl0nxFngYekIVknOHMmGCXGggAd/2T0pmOeLP9gMlAuvzNonM7QOPj0yD/NVsPcrG2K9Bo7APJB1QjwUVoXn4L2VffXsSepEMandgVkBHiV3dRzN2lieSy19jEcbAWe793VCrBFn2stWI0H4WkoEMS5R6vkNX4AoldTz87k22+bJHfCyVu9TE/zNft4mdxbBhahrIpawqlEIxGxEy8i5JgmpmCojOE8s/DZq5o01ysxXv0dvy3IGqMGDS6XpLYzzkY/1bUhBkyfAhkicxxkoDXMwIUJ0o1evz8bj68nNzf9IQiitRv347WUBpvSjYXHUd7EFweOKSu0BK6ao8wz96KTfL+puBUwA0pgEafQoENzVQbebaNTeQod2CpgDKkpk0d+wGLzpnIkiaPhGBTChpIbRoDh+1DCswAJKI0bD09pIQM5K5WnuAq0juPPHsR3uPDJag0s3pO5WZREq0J5WRpUF0MIIoNzoqt7yO7scmC8R6Jx5cKPfjDY4/XgNXDFZTI2SfFXb3iYEdBV1anZ4I8sGDpKRUsu7rNfrcSfDpw1H6gxfkvVtQmTB5rFAwFw6j6t0+Tay4eScOm+QzWF/cFYdp/00VKc4eFoqVYcW6MwvjZ8iuAfVZ88BSQiCMyXG4hf2VZ2WC1NqX8NXhMcp8wuSkTD/zleIHDwVEmNR+sec/qogLpgWCjZASV06oIBocBR/HzpougZuKsCPccTkzjId5HjPpCoR/iWsEJz8MyA1SAcvZ6oZ/bN4ehyaJ0j7VXnqKmzPweHHcMC8Z3xR15aTF3OwGf2zQPhPFZYnfsmew8MPLQ9OM+sWxvtAxpu0nw5fSEbLwc+CI5wSpRi9en15NZ58Ph/XXIAW1kjtR69ev7s5/zy5HE+vzi8/7EETdhrORF1Ttud5B1gB7oARKCvAEMmOkBRnIuVOpvGFm6I+ZkihZCH9iA46xJpejT+fTzdavUdThMNnLkGJa8i3z00CnGe+dKk14nxK1uuw6MZPs9BnO7LA05Cly3jWbHsKEV2eXUxupmfvJs/1t2Ey3d2vHjht5/vvgvm1ZKsAZMhuHFB/Z9VROhik/RdSuD2g/zAGkz1DAdXOMJzraWg5MHMrt2cWWg7MlJz1CiNKBS1zJWd014jAxJVWq/rueeABPI85i6WTG53Lea+Vk4c9HRH4wkahltCOOLxRX9juHGCpey7cHb3rebME3RpppOA0Nh7GfhuED3tOcjjjPLi8bN+aawHbvFDHyEa6WrWQyzqOe2Jbmqy7Rekyr5XoFKfLvK3RulOkMBIK61djWd8yQciyIBm5gMLgqjVqj6zHVEqIRfMFuAcR/G05/nRPapMLZkOH9FC077DRhyqdB2x80S/OBCWaxdSNaWwMYmi296fFLgoSd/p3pyuYtVLPH5muiaUx+s68D12RdbwsPzezB8R28fhoVh9C+SGaZ/eElUKC5g80WsZ7NdxZWR8PboAbLRzJjk77/e3KW9PGddyu6/9gQqUP9Wj9VwAAAP//UEsHCM6mYEtTBQAADxMAAFBLAQIUABQACAAIAAAAAADOpmBLUwUAAA8TAAAIAAAAAAAAAAAAAAAAAAAAAABvcmlnaW5hbFBLBQYAAAAAAQABADYAAACJBQAAAAA=
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-03-16T20:30:53Z"
    generation: 2
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 8648497f4c
    name: csi-rbdplugin-provisioner-8648497f4c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-rbdplugin-provisioner
      uid: c6a92e8a-3a3f-4243-8b6f-0cf7f311de79
    resourceVersion: "7669689"
    uid: ca59d6dc-1c1b-4ade-afbb-d15af072e001
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: csi-rbdplugin-provisioner
        pod-template-hash: 8648497f4c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-03-16T21:30:45+01:00"
        creationTimestamp: null
        labels:
          app: csi-rbdplugin-provisioner
          contains: csi-rbdplugin-metrics
          pod-template-hash: 8648497f4c
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-rbdplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          - --extra-create-metadata=true
          - --prevent-volume-mode-conversion=true
          - --feature-gates=HonorPVReclaimPolicy=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --v=0
          - --timeout=2m30s
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --controllerserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-configs
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/secrets/tokens
            name: oidc-token
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-provisioner-sa
        serviceAccountName: rook-csi-rbd-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - emptyDir:
            medium: Memory
          name: socket-dir
        - name: ceph-csi-configs
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: csi-cluster-config-json
                  path: config.json
                name: rook-ceph-csi-config
            - configMap:
                items:
                - key: csi-mapping-config-json
                  path: cluster-mapping.json
                name: rook-ceph-csi-mapping-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - name: oidc-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: ceph-csi-kms
                expirationSeconds: 3600
                path: oidc-token
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzkmE1v2zgTx78L0UMfQJRf8tI+AnwIahcb7CYxkqKXIihocmSzpkh2SKnxBv7uC1KyrdhKmuy2WBR7MCyRw+Fw/j+OSd8TZuVHQCeNJhlh1rpeNSAJWUotSEbGYJVZFaA9SUgBngnmGcnuiWYFkIxwJynOhFXlXGpq0VQyuAIkSTRxlvFgh8YsKQe7IAkx3zTgNeSAoDk4kn16NIqZMnx5FezHoMDH7pwpBwnhRns0SgGSzGMJ3SE3YW6np8YCMm9CfKUM5kdwkp+cHp3Q/LR/So9nQ0HZkB/R/Ph0ePJ//naQvwGyvl0nxFngYekIVknOHMmGCXGggAd/2T0pmOeLP9gMlAuvzNonM7QOPj0yD/NVsPcrG2K9Bo7APJB1QjwUVoXn4L2VffXsSepEMandgVkBHiV3dRzN2lieSy19jEcbAWe793VCrBFn2stWI0H4WkoEMS5R6vkNX4AoldTz87k22+bJHfCyVu9TE/zNft4mdxbBhahrIpawqlEIxGxEy8i5JgmpmCojOE8s/DZq5o01ysxXv0dvy3IGqMGDS6XpLYzzkY/1bUhBkyfAhkicxxkoDXMwIUJ0o1evz8bj68nNzf9IQiitRv347WUBpvSjYXHUd7EFweOKSu0BK6ao8wz96KTfL+puBUwA0pgEafQoENzVQbebaNTeQod2CpgDKkpk0d+wGLzpnIkiaPhGBTChpIbRoDh+1DCswAJKI0bD09pIQM5K5WnuAq0juPPHsR3uPDJag0s3pO5WZREq0J5WRpUF0MIIoNzoqt7yO7scmC8R6Jx5cKPfjDY4/XgNXDFZTI2SfFXb3iYEdBV1anZ4I8sGDpKRUsu7rNfrcSfDpw1H6gxfkvVtQmTB5rFAwFw6j6t0+Tay4eScOm+QzWF/cFYdp/00VKc4eFoqVYcW6MwvjZ8iuAfVZ88BSQiCMyXG4hf2VZ2WC1NqX8NXhMcp8wuSkTD/zleIHDwVEmNR+sec/qogLpgWCjZASV06oIBocBR/HzpougZuKsCPccTkzjId5HjPpCoR/iWsEJz8MyA1SAcvZ6oZ/bN4ehyaJ0j7VXnqKmzPweHHcMC8Z3xR15aTF3OwGf2zQPhPFZYnfsmew8MPLQ9OM+sWxvtAxpu0nw5fSEbLwc+CI5wSpRi9en15NZ58Ph/XXIAW1kjtR69ev7s5/zy5HE+vzi8/7EETdhrORF1Ttud5B1gB7oARKCvAEMmOkBRnIuVOpvGFm6I+ZkihZCH9iA46xJpejT+fTzdavUdThMNnLkGJa8i3z00CnGe+dKk14nxK1uuw6MZPs9BnO7LA05Cly3jWbHsKEV2eXUxupmfvJs/1t2Ey3d2vHjht5/vvgvm1ZKsAZMhuHFB/Z9VROhik/RdSuD2g/zAGkz1DAdXOMJzraWg5MHMrt2cWWg7MlJz1CiNKBS1zJWd014jAxJVWq/rueeABPI85i6WTG53Lea+Vk4c9HRH4wkahltCOOLxRX9juHGCpey7cHb3rebME3RpppOA0Nh7GfhuED3tOcjjjPLi8bN+aawHbvFDHyEa6WrWQyzqOe2Jbmqy7Rekyr5XoFKfLvK3RulOkMBIK61djWd8yQciyIBm5gMLgqjVqj6zHVEqIRfMFuAcR/G05/nRPapMLZkOH9FC077DRhyqdB2x80S/OBCWaxdSNaWwMYmi296fFLgoSd/p3pyuYtVLPH5muiaUx+s68D12RdbwsPzezB8R28fhoVh9C+SGaZ/eElUKC5g80WsZ7NdxZWR8PboAbLRzJjk77/e3KW9PGddyu6/9gQqUP9Wj9VwAAAP//UEsHCM6mYEtTBQAADxMAAFBLAQIUABQACAAIAAAAAADOpmBLUwUAAA8TAAAIAAAAAAAAAAAAAAAAAAAAAABvcmlnaW5hbFBLBQYAAAAAAQABADYAAACJBQAAAAA=
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      deployment.kubernetes.io/revision-history: "2"
    creationTimestamp: "2025-03-09T07:52:15Z"
    generation: 3
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 86b55ccbdd
    name: csi-rbdplugin-provisioner-86b55ccbdd
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-rbdplugin-provisioner
      uid: c6a92e8a-3a3f-4243-8b6f-0cf7f311de79
    resourceVersion: "10606021"
    uid: c1254a58-30c5-460c-b45b-383d726c10c2
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: csi-rbdplugin-provisioner
        pod-template-hash: 86b55ccbdd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin-provisioner
          contains: csi-rbdplugin-metrics
          pod-template-hash: 86b55ccbdd
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-rbdplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          - --extra-create-metadata=true
          - --prevent-volume-mode-conversion=true
          - --feature-gates=HonorPVReclaimPolicy=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --v=0
          - --timeout=2m30s
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --default-fstype=ext4
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=2m30s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --leader-election-lease-duration=2m17s
          - --leader-election-renew-deadline=1m47s
          - --leader-election-retry-period=26s
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --controllerserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.11.0
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-configs
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/secrets/tokens
            name: oidc-token
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-provisioner-sa
        serviceAccountName: rook-csi-rbd-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - emptyDir:
            medium: Memory
          name: socket-dir
        - name: ceph-csi-configs
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: csi-cluster-config-json
                  path: config.json
                name: rook-ceph-csi-config
            - configMap:
                items:
                - key: csi-mapping-config-json
                  path: cluster-mapping.json
                name: rook-ceph-csi-mapping-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - name: oidc-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: ceph-csi-kms
                expirationSeconds: 3600
                path: oidc-token
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 3
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-08T15:08:01Z"
    generation: 5
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-m-1
      node_name: k3s-m-1
      pod-template-hash: df76dc8d8
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-m-1-df76dc8d8
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-exporter-k3s-m-1
      uid: 57628580-09cb-4690-a090-ad7327808bea
    resourceVersion: "9750112"
    uid: accbabc7-d2c5-4641-ba55-ff988b0cdbdc
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-exporter
        kubernetes.io/hostname: k3s-m-1
        node_name: k3s-m-1
        pod-template-hash: df76dc8d8
    template:
      metadata:
        annotations:
          prometheus.io/port: "9926"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-exporter
          ceph-version: 18.2.2-0
          ceph_daemon_id: exporter
          kubernetes.io/hostname: k3s-m-1
          node_name: k3s-m-1
          pod-template-hash: df76dc8d8
          rook-version: v1.14.8
          rook_cluster: rook-ceph
      spec:
        containers:
        - args:
          - --sock-dir
          - /run/ceph
          - --port
          - "9926"
          - --prio-limit
          - "5"
          - --stats-period
          - "5"
          command:
          - ceph-exporter
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: ceph-exporter
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/exporter-keyring-store/
            name: rook-ceph-exporter-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-exporter-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-exporter-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 5
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-16T19:59:57Z"
    generation: 1
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-1
      node_name: k3s-w-1
      pod-template-hash: 58fdfc478
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-1-58fdfc478
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-exporter-k3s-w-1
      uid: f9966238-eabd-4627-99af-ac834c100aaa
    resourceVersion: "11099872"
    uid: 67a434c0-fe5b-4a13-85e8-bea28bc8b919
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-exporter
        kubernetes.io/hostname: k3s-w-1
        node_name: k3s-w-1
        pod-template-hash: 58fdfc478
    template:
      metadata:
        annotations:
          prometheus.io/port: "9926"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-exporter
          ceph-version: 18.2.2-0
          ceph_daemon_id: exporter
          kubernetes.io/hostname: k3s-w-1
          node_name: k3s-w-1
          pod-template-hash: 58fdfc478
          rook-version: v1.14.8
          rook_cluster: rook-ceph
      spec:
        containers:
        - args:
          - --sock-dir
          - /run/ceph
          - --port
          - "9926"
          - --prio-limit
          - "5"
          - --stats-period
          - "5"
          command:
          - ceph-exporter
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: ceph-exporter
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/exporter-keyring-store/
            name: rook-ceph-exporter-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-exporter-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-exporter-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-27T03:55:07Z"
    generation: 1
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-2
      node_name: k3s-w-2
      pod-template-hash: ccc459589
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-2-ccc459589
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-exporter-k3s-w-2
      uid: 63c20aea-2986-422c-914c-736836f71bac
    resourceVersion: "9964636"
    uid: 1a610de5-2d34-44d5-a9b4-8b5156657dd9
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-exporter
        kubernetes.io/hostname: k3s-w-2
        node_name: k3s-w-2
        pod-template-hash: ccc459589
    template:
      metadata:
        annotations:
          prometheus.io/port: "9926"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-exporter
          ceph-version: 18.2.2-0
          ceph_daemon_id: exporter
          kubernetes.io/hostname: k3s-w-2
          node_name: k3s-w-2
          pod-template-hash: ccc459589
          rook-version: v1.14.8
          rook_cluster: rook-ceph
      spec:
        containers:
        - args:
          - --sock-dir
          - /run/ceph
          - --port
          - "9926"
          - --prio-limit
          - "5"
          - --stats-period
          - "5"
          command:
          - ceph-exporter
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: ceph-exporter
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/exporter-keyring-store/
            name: rook-ceph-exporter-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-exporter-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-exporter-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-17T20:18:49Z"
    generation: 1
    labels:
      app: rook-ceph-exporter
      ceph-version: 18.2.2-0
      ceph_daemon_id: exporter
      kubernetes.io/hostname: k3s-w-3
      node_name: k3s-w-3
      pod-template-hash: 8576f74844
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-exporter-k3s-w-3-8576f74844
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-exporter-k3s-w-3
      uid: 69a75536-c529-4cd2-953d-409c72e58e37
    resourceVersion: "9322191"
    uid: de1cc63f-42f1-4b58-8ecd-c925b6ed1518
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-exporter
        kubernetes.io/hostname: k3s-w-3
        node_name: k3s-w-3
        pod-template-hash: 8576f74844
    template:
      metadata:
        annotations:
          prometheus.io/port: "9926"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-exporter
          ceph-version: 18.2.2-0
          ceph_daemon_id: exporter
          kubernetes.io/hostname: k3s-w-3
          node_name: k3s-w-3
          pod-template-hash: 8576f74844
          rook-version: v1.14.8
          rook_cluster: rook-ceph
      spec:
        containers:
        - args:
          - --sock-dir
          - /run/ceph
          - --port
          - "9926"
          - --prio-limit
          - "5"
          - --stats-period
          - "5"
          command:
          - ceph-exporter
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -n client.ceph-exporter -k /etc/ceph/exporter-keyring-store/keyring
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: ceph-exporter
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/exporter-keyring-store/
            name: rook-ceph-exporter-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: k3s-w-3
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-exporter-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-exporter-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-08T15:02:27Z"
    generation: 2
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 545cb79549
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-545cb79549
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "5803807"
    uid: 9aed3cb6-88b3-4614-b122-de96c9abb823
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 545cb79549
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 545cb79549
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-03-10T18:51:12Z"
    generation: 2
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 575cb47ff4
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-575cb47ff4
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "7679556"
    uid: 85168a3b-7ce8-4443-aef1-a3e4139e9b21
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 575cb47ff4
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 575cb47ff4
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 800Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 800Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-03-09T22:09:39Z"
    generation: 2
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 5845f894d5
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-5845f894d5
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "6216570"
    uid: 6ee5f2c6-7672-47e6-ad99-61eb2b0046c2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 5845f894d5
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 5845f894d5
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2025-03-17T21:10:04Z"
    generation: 2
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 67c9cc66b4
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-67c9cc66b4
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "9908071"
    uid: 1039db1d-1cc0-4b03-b47d-8475240d15ff
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 67c9cc66b4
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 67c9cc66b4
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 7000
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 800Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 800Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "9"
    creationTimestamp: "2025-03-29T17:13:05Z"
    generation: 2
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 6895f8b68b
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-6895f8b68b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "9930045"
    uid: 7f744bd3-59a7-4ea8-a49f-bb2f2468496e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 6895f8b68b
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 6895f8b68b
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 7000
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "2"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "4"
      deployment.kubernetes.io/revision-history: "2"
    creationTimestamp: "2025-03-08T20:32:19Z"
    generation: 4
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 6cdd5f965c
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-6cdd5f965c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "6027930"
    uid: 2f9364ed-ce54-4077-8a05-97f02ce490b0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 6cdd5f965c
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 6cdd5f965c
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              memory: 1Gi
            requests:
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              memory: 1Gi
            requests:
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsWHlvIjkW/yolq6WZlsocaZIlrNAqS5gZNM2xJD3Sqski43oF3rjsGttFgrJ899VzFQUhRdI9hzR/9D+osN/l59+7/EQScCxijpHOE5FsAdLiF0tT0iFG63vKIV3RZGlIiKu1+2wBRoEDWxO6znWSagXKkQ5BQi4z68DYGv6pIX9N6GpOA8xBRBebZ4p0CoY5Xa1NKOuY4kA6hFUSJEyx5VcKVSyBwvqTx0yZcVTHh0JJmLOswVihFemQZrt2VjujjWJnHjFItJqLqDD3cNFtUlSa6zs6Fi6WX3Ojpd/gTqyBhLkBe6XrZq3ZqrWLDbR1d1iKB7Mp83IPzcbveXFRz7a2ISmc8eziKZpySph+UGCmEIMBxcGSzmcEj/ilNPAQCPV1k4RkITW/HyPfNUhwnsyZDELCtXJGS4l25Sv3QqH7epCueoXFL40kIcm8ly94i/NF/DfaarBL2orPzumCwwcasfZZ4/yyuWhdNMj2bhsSmwJHnBtIpeDMkk4zJBYkcIRJ54kkzPHVxzfiofKaT97mab+jQc4wB8sNKivAMYU8RvBeHCSpxO/O84hlSmnH0IfezNToBNwKshy12mBcXp61P5DwaM9yw7wW9DNq+Bb7Xx37f2KE/1mBfAB9FsdCCYeI2+ahx4QCU4SwWeIHoTS2IurCGVy2LtgFbVxAm7bazTZtf7g8p8CarRjareZFKyIhofQeNkaoZbcOjtdRb71YodZpA7t/njaCmGXSUamX1GlqXQTGdD0eD7fBmNe2E61o4YO3JOF2vkdTA7F47EawyJZBlTWxkNCNmbRvajuiRIqVtq777vvpePzzvNef/DQfjkfzn8Y3t+9LEnS9YJImkCzA2BfUg9HgdnD1cT7sD//Zn97kjCLqMv9hwWUWTLcAgl9YGp2l+xUuBSg0OFOOZiLqNl4uL8vlWBtAASq/xjRbSMEpiyKzs2wyvp4PJu/JHWIlSRjm5c/7yLkLCai1x04Bvd54dHs1GPWn88Hw6sc+CcmayQx3fs3YxqcBxAf+dNZ57STbsGRHfaOrYcn3g9EJwjYWIKMpxOX3hLkVBlqRFWuef7utEHUzuer9Nnl59D0TOhpf97/KQIy7mtIRjCoNHPaH4+m/5x8Hw8HtsUgDVmeGww8HoiOxFhZrFcEb3FGQDpEiEc7WEki02ZzUM+3/61P/5vdpMvBrBvYVXb3Jp99yoGblgXiaVWv4Q4/yQouPyE83/fn0anQ9Hs5H41HvEMxF/SzpXwb9sWEW67r7GTaFUfeAFQtrBuaNiv6Gcq1isXxu16vp4stVFnloXuShr9I+nvSnV7fj6cvgOqpGFUb3Pn66ue1P573p9fyX/vRmMB4dMK+bb3Mdxt4pfZ7pavrjzQHhPkUHlSk6KKtY8FYVO7Yxz5FfnBAcc5mtpToaTNC7dyERCVu+niELmkkm5URLwfEeB/FIu4kBi71YSKRYgwJrJ0YvfMMIj3nFP8jbmKtDQgUJifX1gpOQzJTOXNqdkXffo86AUhYlQtG8ownqJlN7izDr11iNWX0f5CcJzmZZo/EB8Pfsovl+RmbK8O67f8yUiIPPwTvDA6ogaAR3fw/cCtRMzRzwlQ5mvo4EhZ4VMOlWAV8Bvw9iJiREwYNwK+QJYi2lfsDLQVsz10EtpZh3uDgjwf8CC1FAIfjO1v9Tz80K6svvPOmjcGjLTMVipghOA0UQXINkmxvgWkU4EDRC4kQCOnPl2vm+s8obO2yx846pbKAmvuu+aDcax7RGO821JB1y25t48BwxYadeMq2cS2kCzghuv4i73WrtuSNmVwvNTFTBerdPfb7hzxOsR0iaFfm3SOkd0m40hgL7xl2K3NOdNxrJIel5Too9JvDMCLfpaeXg0eWjiVgLCUuISMd3S37oYcZl6TegfilQUUlm4HZlwK60jEjn4hX0pmCEjt4E9FrLLIEhdoQ5lH1zWCSpMgUeVwZfFKhegzEi8mMLsGis5CYf3RGfz+Ts7mQvx19O7kpLreb3NBKGvGA8kYOrKpWfceh+xnjDpDUzdamXlcdDcVIvX5rjmcSiwJdhtpI133idOVmaHKCs0ifUz/gYrA/a3Au1vBbmhdW+aCjheqfGtzWYhS5GE4NRafMh0xeV4uDHjqg+4+ENnjzI0XSw0g+K/FF1beciFErL3Ofd5JHzV8lq3+LpLx1PRfnD0lMi7Uo+sA1WWQtmLThccY5qRhUOISFxWoLZPbp9fiIQx8AdzoS6/wg8c3h9eYuN897Re1KmDDC+YguJZOXzVIf0HwUC8VD+QZ6+2wHLHg7ZJ7CTGv1f4A5R+UTKmMi7lFgshyzFDeEgyZdzY4tGPySJjoB0zi7PQpLmnsbT13DfZ5zXlG+3d4eDwmkw5TPJfjqpcvYBgxeKXXt+9087y0oMIGMdHrEj86/ExQPctTD+UXczNr3yOfXNiHlTUWlmPUf1abR/hagdyl/DPySp2/hS8FR5kBLm+asy9jtIuv1/AAAA//9QSwcIRgEtUFAHAADqGQAAUEsBAhQAFAAIAAgAAAAAAEYBLVBQBwAA6hkAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAAIYHAAAAAA==
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-03-10T17:39:04Z"
    generation: 2
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 7975cf55c5
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-7975cf55c5
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "6228475"
    uid: 3f974bb9-6602-43fe-b3e2-43813687c944
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 7975cf55c5
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 7975cf55c5
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 800Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 800Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsWH1v4jga/yqRNdLuSDEvnbZLOaETR9lZtMPL0c5Kp6GHjPMEfCR21nZoUY/vfnqcEFIa2uncrrR/zD8o2M+bH/+eNz+SGCwLmGWk/UgitoDI4BdLEtImWqk15ZCsaLzUxMfV2jpdgJZgwdSEqnMVJ0qCtKRNkJBHqbGgTQ3/1JC/JlQ1pwZmIaCL7RNFKgHNrKrWJqSxTHIgbcIqCWIm2fKNQiWLIbf+5DETpi1VYVko8TOWDWgjlCRt0mzVzmpntJHvzAMGsZJzEeTmlhftNkGlmb6jY+Fi8TXXKnIb3IoNED8z4KB006w1z2utfANt3R+W4sFMwpzcstn4Pc8v6snWzie5M55cPEVTTglT9xL0FELQIDkY0v6C4BG/FQaWgVDfNIlPFpHi6zHyXUME1pFZnYJPuJJWqyhCu7KVtZDovh4kq15u8XMjiU9S5+VLfs75IvyJnjfYFT0Pzy7ogsMHGrDWWePiqrk4v2yQ3d3OJyYBjjjXkESCM0PaTZ8YiIAjTNqPJGaWrz69Eg+V13zyNk/7HQ2ymllYblFZDo4pZDGC92IhTiL8bj+NWCalsgx96MxMtIrBriDNUKs0xuXVWesD8Y/2DNfMaUE/o4bvsf/m2P8TI/zPCuQS9FkYCiksIm6XhR4TEnQewnqJH4TS0IigA2dwdX7JLmnjElr0vNVs0daHqwsKrHkeQuu8eXkeEJ9QuoatFnLZqYPlddRbz1eosUrD/p+jDSBkaWRppJbUKmpsAFp3HB7L26D1S9uxkjT3wWuScDvbo4mGUDx0AlikS6/KmlBE0AlZZF7VdkSJFCtlbOfdj9Px+Nd5rz/5ZT4cj+a/jG9u3xck6HrBIhpDvABtnlEPRoPbQffTfNgf/qM/vckYRdBh7sOATQ3oTg4Et7DUKk0OKzwSINHgVFqaiqDTeL68LJZDpQEFyOwak3QRCU5ZEOi9ZZPx9XwweU/uECtxzDAvfzlEzp1PQG4cdnLo9caj2+5g1J/OB8Puxz7xyYZFKe78nrKtSwOID/xpb7LaSXZ+wY76Rt1hwfezVjHCNhQQBVMIi+8JsysMtDwr1hz/blch6mbS7X2bvCz6nggdja/7bzIQ464mVQCjSgOH/eF4+q/5p8FwcHssUoNRqebwc0l0IDbCYK0ieIN7CtImkYiFNbUYYqW3J/VM+//83L/5/zRp+D0F84Ku3uTztxyoWXkgnqTVGv7QozzT4iLy801/Pu2OrsfD+Wg86pXBnNfPgv550B8bZrCu219hmxu1BqxYWDMwb1T0N5QrGYrlU7teTBdfrzLPQ/M8D71J+3jSn3Zvx9PnwXVUjSqM7n36fHPbn8570+v5b/3pzWA8KjFvmq9zlWPvlD7H1J1+vCkRHlK0V5mivaKKea9VsWMbsxz51QnBMpuaWqKCwQS9e+cTEbPlyxkyp5mkUTRRkeB4j4NwpOxEg8FezCeR2IAEYyZaLVzDCA9ZxS/lbczVPqGC+MS4esGJT2ZSpTbpzMi7H1GnRykLYiFp1tF4dZ3Kg0WY9Wusxoxae9lJvLNZ2mh8APw9u2y+n5GZ1Lzz7u8zKULvi/dOc49K8Bre3d88uwI5kzMLfKW8masjXq5nBSyyK4+vgK+9kIkIAu9e2BXyeKGKInWPl4O2praNWgox73BxRrz/egYCj4L3g6n/u56Z5dWXPzjSB2HRlpkMxUwSnAZQSarhdqXBrFQUkPYF9mwuMq4hYtsb4EoGOCU0fJKAFiooli4bPjEp52BMSUDTJ1bEoFJbEH5oHPqyrC3EBj3rt4r2a+J69stWo3FMq5VVXEWkTW57Ewe9Iybs8wumlbUJjcFqwc1Xcf/UKKkMmFktFNNBBevdIXG6cSFLzw5fSUraBGeNvCC0SfOjwJ5zn14PVK1GIy4TXjQaQ5FNQsBTLey2p6SFB5uNNWIjIlhCQNqu03IDE9M2Tb6D/NtBfvn1IMelYzxf7HyyUVEawxC7yQzIrrHME1yRPo+riisoVG1AaxG4kQdYMJbRNhv7EZ1P5Ozv5CDHXU7mSkON4msaCE2eMZ7I31VVzs1H9DCfvGLShul6pJaVx0NxkVo+N8cxiUWOL81MJWu28TJzvNQZQFmlT6h7H8BQvVd6LeTyWuhnVruCI4XtnRr9NqAXKh9rNEalyQZUV5Dygx87ovqM5Rs8eZCjyWKl7iX5o2ri3kUolBaZz7nJIeevkdO+R9NfOpry0oeFp8BZN7pnW6ywBvRGcOhyjmpGFQ4hPrEqAr1/rvvySCAMgVucJlX/AXhq8fqy5hwnxaOXqFRqYHzFFhGSFQ9bbdJ/EAjEsvxSlr7bA8uUx/MT2Em0+g9wi6h8JEVEZB1KKJZDluCGsBBny5mx+Yjgk1gFQNpnV2c+STJP4+lruO/yzUvKd7u78ohxGkzZNHOYa6qcXWJwQrHfz+7+cW9ZgQFkrMMDdmPufTl/ursW2j0Hb8e6VzzEvhoxryoqzKxnqD6N9jeI2qP8JfxDnNitKwSPlQcpYJ69R2O3g6S7/wUAAP//UEsHCBkyuLZpBwAAJBoAAFBLAQIUABQACAAIAAAAAAAZMri2aQcAACQaAAAIAAAAAAAAAAAAAAAAAAAAAABvcmlnaW5hbFBLBQYAAAAAAQABADYAAACfBwAAAAA=
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "10"
    creationTimestamp: "2025-03-29T20:04:26Z"
    generation: 2
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 84754d9cb6
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-84754d9cb6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "10015723"
    uid: b193b6a0-cfb6-4944-901f-7ece7f58ca56
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 84754d9cb6
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 84754d9cb6
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 7000
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "3"
              memory: 1Gi
            requests:
              cpu: 800m
              memory: 500Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 1Gi
            requests:
              cpu: 800m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-03-09T20:49:29Z"
    generation: 2
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: 865db5b7d
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-865db5b7d
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "6022454"
    uid: aaf8950e-c43b-4cc2-a3c0-cecb4b8b4d47
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 865db5b7d
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: 865db5b7d
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources:
            limits:
              memory: 1Gi
            requests:
              memory: 512Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              memory: 1Gi
            requests:
              memory: 512Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "11"
    creationTimestamp: "2025-03-30T07:44:43Z"
    generation: 1
    labels:
      app: rook-ceph-mgr
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mgr
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      mgr_role: active
      pod-template-hash: dcf48548
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-dcf48548
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: bcf9fd8f-a714-485d-8294-6b7cf6a0985c
    resourceVersion: "10640793"
    uid: d437df49-ec79-4cd5-bdd9-d7234a847336
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: dcf48548
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mgr
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          mgr_role: active
          pod-template-hash: dcf48548
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - k3s-m-1
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: CEPH_ARGS
            value: --mon-host $(ROOK_CEPH_MON_HOST) --keyring /etc/ceph/keyring-store/keyring
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 7000
            name: dashboard
            protocol: TCP
          resources:
            limits:
              cpu: "3"
              memory: 1Gi
            requests:
              cpu: 800m
              memory: 500Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mgr.a.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ]; then\n\techo \"ceph daemon health check failed with
                the following output:\"\n\techo \"$outp\" | sed -e 's/^/> /g'\n\texit
                $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 1Gi
            requests:
              cpu: 800m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "12"
    creationTimestamp: "2025-03-10T17:36:05Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 57765dcbd
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-57765dcbd
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "7884971"
    uid: c3a8c47c-f87d-4d68-a604-7a242132f4a0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 57765dcbd
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 57765dcbd
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 500m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-08T21:21:15Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 5bd5c67dd8
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-5bd5c67dd8
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "5812180"
    uid: e194218f-b661-4202-baf6-f95d31ebb8ab
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 5bd5c67dd8
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 5bd5c67dd8
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 60
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-03-09T20:58:20Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 67f6c76f44
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-67f6c76f44
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "6017204"
    uid: 73f3e296-9035-499c-92cd-a0ddd2f65608
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 67f6c76f44
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 67f6c76f44
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-03-09T20:59:30Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 6c54ff469f
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-6c54ff469f
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "6019812"
    uid: 2d0b3cea-b219-4f2a-8e61-562bb07217bc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 6c54ff469f
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 6c54ff469f
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "14"
    creationTimestamp: "2025-03-29T17:12:06Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 6d68789b6b
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-6d68789b6b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "11100139"
    uid: e7da8e60-740e-4156-863a-68153530ded5
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 6d68789b6b
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 6d68789b6b
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsWW1vIjkS/istK9LuSO0GEiabcEKnHGFv0Q4vl2ROOg05ZNzV4MNt99puEpTjv5/K3QFCOmQyOzrdS74gY7uqHlc9VS6rH0gKjsXMMdJ6IJJNQVocsSwjLWK0XlAO2ZymWpEQZ6NFPgWjwIGNhK5xnWZagXKkRXAjl7l1YGyEfyKUj4SuljTAHMR0unpiSGdgmNOmUkYo65jiQFqEVW5ImWKzNypVLIUS/YvHzJhxVCe7SklYiCzBWKEVaZHGWXQcHdN6uTKJGaRaTURcwt2ddKsMjRb2Ui/OitGkdOGeLT/e2lo2okYzOisXEOLjGSmex2bMe2lfQ6XydUhKHzyJN0VALynTdwrMFSRgQHGwpPUFOSP+ugG4G//askFCMpWaL4YodwkSnN/mTA4h4Vo5o6VEXMXMQij0WgeyeadE/BwkCUnunXvKm5xPk59os87OaTM5/kinHE5ozM6O6x/PG9PmaZ2sb9chsRlwpLeBTArOLGk1QmJBAkd2tB5Iyhyff3olDSqj+7VBrA4BYnOGOZit0G5JjysosgRD5CDNJI5b7zn7O3P2+2fmvzsBd6jMkkQo4ZA26yKVmFBgypQ0MxwQShMr4jYcw3nzlJ3S+imc0eZZ44yenZx/pMAazQTOmo3TZkxCQukCVkaoWbsGjtfQYq2codZpA4///N4YEpZLR6WeUaepdTEY08Y8frIMxhxaxvOWvnlNEy4XazQzkIj7dgzTfBZUoUmEhHbCpH3V2t5O3DHX1rWPfrwaDn+ddLqjXyb94WDyy/D65sNmC7peMElTSKdg7LPdvUHvpnfxadLv9v/UvbouBEXcZn5gweUWTLskiJ+YGZ1n25lEG8ApVQQmy6dScMri2LQb9ah5EjVO6lHjvLmrj/oqRjPm5u3akpmaFNMiiqlWfkBZzQcyiqe7aqdCxYXu8hyj4eWkN/pAbpFZacqwKn/Zpt1tSEAtPdNKAneGg5uL3qB7Nen1L/7cJSFZMpnjym85W/kagjjwp7UsLkyyDjfiaG9w0d/I/Wx0iiRPBMj4CpLNeMTcHLO0LISRl1+vK1Rdjy4636avSLsnSgfDy+6bAGKWRkrHMKgE2O/2h1d/m3zq9Xs3+yoNWJ0bDj/vqI7FUli8qQh2GY87SItIkQpnoxRSbVYv2rnq/uVz9/r3WTLwWw72gK3O6PO3HKhReSCe5dUWvutRnlnx+fv5uju5uhhcDvuTwXDQ2SWzr0k7+5+XiH1gFq9y9yusSlALWBW3zASrTEV3Q7lWiZg9xXWwuHy9ybJqTcqq9SbrRUn4av475nIbZTrujVDZbUhEymaHC0K5Z5RLOdJScITdSwbajQxY7FtCIsUSFFg7MnrqWyK4L67DnTKFpSkkVJCQWF9MOQnJWOncZe0xOfoRbQaUsjgViha3f1AzudoiwiIXsYhZvQjQb8VpguNxXq+fAP4enzY+jMlYGd4++uNYiST4EhwZHlAFQT24/UPg5qDGauyAz3Uw9qUzKG3NgUk3D/gc+CJImJAQB3fCzVEmSLSU+k6oWYB4c9dCKxs1Rzg5JsE/AwtxQCH4wdb+XitgBbXZD37rvXCIZawSMVYE298y7pcg2eoauFYxdsD1kDiRgs7dZu7jtiUpGqFMG1e0FJsOY6SNI62Tk3p9s9fxjKZ2ZjCEmdFOcy1Ji9x0Rj5Z9kRPfzo7fybaqBC93WatLdocCzw3wq06Wjm4dxj4zIilkDCDmLT8Ne5bamZcnr2T5C0kQSO5gZu5ATvXMiat0wPMycAIHb9KpqWWeQp9nauSRikOyyKx6TL3C5GvQVQvwRgRgy/eLB4quSreicipJ3oe47LV4wNUuNJSq/mCxsKQZ4IvtLlVhdFr2va/ryDy3ZeeVZ4OtUk9e47mScvGDbOVosXCYeFtv1fpEuofkZhfd9oshJpdCvMMta/ZSrjOS0+LJZipLttmg4lpxRLKV1WrPPi+I6rPuBvAFw+y14vO9Z1vRL/LtfLoIlRKN+XKu8kT51sL0Tv9/5Ppj0reX8r/7S/lw0/jdJHY92fs+zP2/Rn7f/qM/c4tAtr2dS2x713B/2ZXcBsSLHnXO5+Hnn5hQMqX8osTS+9og6w9F/DZueHPhbxjKySJBbMUHC44R2iDfejljUseeWF3L6QXQp8Z/Q/gDkn1QDYULF7piZj1WYYLwkFaTBe5UyZFSFIdA2kdnx+HJCs8hUgiXPdt/yHj6/Xtbkq/yIUifbeJ/OzcT/Z7lejXInIPj7g2EUS5Gtxn2hQfBcvPNZfC+CCthqaz+WT2Kt1fNbRBWSs4+TJX36DqkaOH2PuqOv99qFZQ9SCLi0+LzOW+Nq3/FQAA//9QSwcImEHK2d4GAADxHwAAUEsBAhQAFAAIAAgAAAAAAJhBytneBgAA8R8AAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAABQHAAAAAA==
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "8"
      deployment.kubernetes.io/revision-history: 1,3,5
    creationTimestamp: "2025-03-08T14:57:31Z"
    generation: 8
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 7c4f5c6bc9
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-7c4f5c6bc9
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "6027017"
    uid: 609a34ba-c640-4ad5-af4f-f6c102db7f22
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 7c4f5c6bc9
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 7c4f5c6bc9
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 8
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-03-09T20:49:22Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 84b7674b74
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-84b7674b74
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "6016530"
    uid: 7f0a90d5-ad09-4874-983f-9ac56f6a0e16
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 84b7674b74
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 84b7674b74
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "10"
    creationTimestamp: "2025-03-09T22:17:35Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: c9479977b
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-c9479977b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "6030860"
    uid: c0ab837c-26d6-430f-835b-b9076e8ddf15
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: c9479977b
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: c9479977b
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 300m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 300m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 300m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "13"
    creationTimestamp: "2025-03-18T18:52:10Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: f9bf67dd5
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-f9bf67dd5
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "9907823"
    uid: e9020112-8ef3-478e-b74b-2dc3009bfbe4
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: f9bf67dd5
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: f9bf67dd5
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "11"
      deployment.kubernetes.io/revision-history: "9"
    creationTimestamp: "2025-03-09T22:03:52Z"
    generation: 4
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: a
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: ff899565c
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-ff899565c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: e20b9613-7745-4e36-8511-c9c1e877c05a
    resourceVersion: "6216162"
    uid: 8b720a78-e158-416d-aac9-d2da25e746ef
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: ff899565c
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: a
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: ff899565c
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.130.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 300m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 300m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.130.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 300m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-11T18:40:25Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      pod-template-hash: 56d4d9dd56
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e-56d4d9dd56
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-e
      uid: febcd12a-97fc-4f2e-913d-d17837ace9ee
    resourceVersion: "7884755"
    uid: e3489ce9-905a-40fa-a874-ab727b015e79
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: e
        mon: e
        mon_cluster: rook-ceph
        pod-template-hash: 56d4d9dd56
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: e
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: e
          ceph_daemon_type: mon
          mon: e
          mon_cluster: rook-ceph
          pod-template-hash: 56d4d9dd56
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-e
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.219.20
          - --setuser-match-path=/var/lib/ceph/mon/ceph-e/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-e
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.219.20
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 500m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-e/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-18T18:51:13Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      pod-template-hash: 699cd7ff55
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e-699cd7ff55
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-e
      uid: febcd12a-97fc-4f2e-913d-d17837ace9ee
    resourceVersion: "9907695"
    uid: 0db8356f-49b5-40f6-8b20-c6e83c4562b1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: e
        mon: e
        mon_cluster: rook-ceph
        pod-template-hash: 699cd7ff55
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: e
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: e
          ceph_daemon_type: mon
          mon: e
          mon_cluster: rook-ceph
          pod-template-hash: 699cd7ff55
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-e
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.219.20
          - --setuser-match-path=/var/lib/ceph/mon/ceph-e/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-e
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.219.20
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-e/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-03-29T17:11:40Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: e
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: e
      ceph_daemon_type: mon
      mon: e
      mon_cluster: rook-ceph
      pod-template-hash: 78588fff8b
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-e-78588fff8b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-e
      uid: febcd12a-97fc-4f2e-913d-d17837ace9ee
    resourceVersion: "10640768"
    uid: 13e9aff6-3342-4ae9-bcb4-13ba217e4464
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: e
        mon: e
        mon_cluster: rook-ceph
        pod-template-hash: 78588fff8b
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: e
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: e
          ceph_daemon_type: mon
          mon: e
          mon_cluster: rook-ceph
          pod-template-hash: 78588fff8b
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-e
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.219.20
          - --setuser-match-path=/var/lib/ceph/mon/ceph-e/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.e.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-e
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=e
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.219.20
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-e
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-e/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-29T17:12:30Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: h
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: h
      ceph_daemon_type: mon
      mon: h
      mon_cluster: rook-ceph
      pod-template-hash: 9bc6b5db7
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-h-9bc6b5db7
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-h
      uid: 06cc8142-995b-40db-85b8-80effaea6d40
    resourceVersion: "11084667"
    uid: b4efcf29-33b5-49a4-b937-7f6431ef2c62
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: h
        mon: h
        mon_cluster: rook-ceph
        pod-template-hash: 9bc6b5db7
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: h
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: h
          ceph_daemon_type: mon
          mon: h
          mon_cluster: rook-ceph
          pod-template-hash: 9bc6b5db7
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-h
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=h
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.167.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-h/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.h.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.h.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-h
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=h
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.167.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: 1500m
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-h/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-26T15:01:31Z"
    generation: 2
    labels:
      app: rook-ceph-mon
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: h
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-mon
      app.kubernetes.io/part-of: rook-ceph
      ceph_daemon_id: h
      ceph_daemon_type: mon
      mon: h
      mon_cluster: rook-ceph
      pod-template-hash: dccc74f48
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-h-dccc74f48
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-h
      uid: 06cc8142-995b-40db-85b8-80effaea6d40
    resourceVersion: "9907928"
    uid: a02c5c99-dbc4-4be1-9f21-28b6b7b10967
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: h
        mon: h
        mon_cluster: rook-ceph
        pod-template-hash: dccc74f48
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: h
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-mon
          app.kubernetes.io/part-of: rook-ceph
          ceph_daemon_id: h
          ceph_daemon_type: mon
          mon: h
          mon_cluster: rook-ceph
          pod-template-hash: dccc74f48
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-h
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=h
          - --setuser=ceph
          - --setgroup=ceph
          - --foreground
          - --public-addr=10.43.167.194
          - --setuser-match-path=/var/lib/ceph/mon/ceph-h/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.h.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: mon
          ports:
          - containerPort: 3300
            name: tcp-msgr2
            protocol: TCP
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-mon.h.asok mon_status
                2>&1)\"\nrc=$?\nif [ $rc -ne 0 ]; then\n\techo \"ceph daemon health
                check failed with the following output:\"\n\techo \"$outp\" | sed
                -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          - /var/lib/ceph/mon/ceph-h
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
        - args:
          - --fsid=e2e946a6-06e8-4818-8395-ea14fe84164d
          - --keyring=/etc/ceph/keyring-store/keyring
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=h
          - --setuser=ceph
          - --setgroup=ceph
          - --public-addr=10.43.167.194
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 300m
              memory: 250Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-h
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-h/data
            type: ""
          name: ceph-daemon-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      kustomize.toolkit.fluxcd.io/prune: disabled
    creationTimestamp: "2025-03-08T13:17:48Z"
    generation: 1
    labels:
      app: rook-ceph-operator
      pod-template-hash: fc4766689
    name: rook-ceph-operator-fc4766689
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 3e5f5635-f606-4b2d-a2c3-f46259c81f7e
    resourceVersion: "9309699"
    uid: 0aec6bb7-b43c-44a0-9917-8bf0f8d363d8
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-operator
        pod-template-hash: fc4766689
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-operator
          pod-template-hash: fc4766689
      spec:
        containers:
        - args:
          - ceph
          - operator
          env:
          - name: ROOK_CURRENT_NAMESPACE_ONLY
            value: "false"
          - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
            value: "false"
          - name: DISCOVER_DAEMON_UDEV_BLACKLIST
            value: (?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+
          - name: ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS
            value: "5"
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: rook-ceph-operator
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-config
          - mountPath: /etc/ceph
            name: default-config-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-system
        serviceAccountName: rook-ceph-system
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - emptyDir: {}
          name: rook-config
        - emptyDir: {}
          name: default-config-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
      deployment.kubernetes.io/revision-history: "1"
    creationTimestamp: "2025-03-08T15:08:01Z"
    generation: 4
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      pod-template-hash: 549c69467
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0-549c69467
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: fed060e3-4b07-4c07-8720-734d979bdd12
    resourceVersion: "6028118"
    uid: 2f2cb6ae-f7ce-475d-8c4a-ba2bc223c052
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: 549c69467
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-m-1
          osd: "0"
          osd-store: bluestore
          pod-template-hash: 549c69467
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-m-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-m-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: e319b7b8-8843-4ff4-a053-f07520dbe330
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-09T20:49:44Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      pod-template-hash: 58b9bcdc46
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0-58b9bcdc46
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: fed060e3-4b07-4c07-8720-734d979bdd12
    resourceVersion: "6022638"
    uid: 90ee552c-c9b1-4dbc-a794-1320c4a70c48
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: 58b9bcdc46
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-m-1
          osd: "0"
          osd-store: bluestore
          pod-template-hash: 58b9bcdc46
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-m-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-m-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: e319b7b8-8843-4ff4-a053-f07520dbe330
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-03-09T22:10:15Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      pod-template-hash: 646776fd99
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0-646776fd99
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: fed060e3-4b07-4c07-8720-734d979bdd12
    resourceVersion: "6031804"
    uid: fc539bb0-994d-4426-b4c7-6e69d63582a9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: 646776fd99
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-m-1
          osd: "0"
          osd-store: bluestore
          pod-template-hash: 646776fd99
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-m-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-m-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: e319b7b8-8843-4ff4-a053-f07520dbe330
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: 500m
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: 500m
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: 500m
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-03-16T18:47:45Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      pod-template-hash: 6776b649fb
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0-6776b649fb
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: fed060e3-4b07-4c07-8720-734d979bdd12
    resourceVersion: "9908300"
    uid: 021cdc72-5b12-4531-abdf-8699a246bcd2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: 6776b649fb
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-m-1
          osd: "0"
          osd-store: bluestore
          pod-template-hash: 6776b649fb
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-m-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-m-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: e319b7b8-8843-4ff4-a053-f07520dbe330
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-03-09T22:35:20Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      pod-template-hash: 69fcd8b9f
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0-69fcd8b9f
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: fed060e3-4b07-4c07-8720-734d979bdd12
    resourceVersion: "6228639"
    uid: 85d17b38-d8f2-4162-84b9-5de66ed1c7ee
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: 69fcd8b9f
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-m-1
          osd: "0"
          osd-store: bluestore
          pod-template-hash: 69fcd8b9f
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-m-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-m-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: e319b7b8-8843-4ff4-a053-f07520dbe330
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsOmtvIrmWf8XjbolkluKRV6cZoRUT6B40SciFpFerVBaZqlPgSZVdY7ug2d7899Wxq6B4dWfuzOq2VvMFgX183k+bLzQBw0JmGG19oTGbQKzxG0tT2qJKymcvgHTmSR3SKq7WnrMJKAEGdI3LeiCTVAoQhrYoAgZxpg0oXcMfNTxf43L/SQXMQOhNlpuEUlDMSLX3DBfaMBEAbdHGXoCECTb9g0gFSyDn/qCYKVPGk1EZKa2ujng8zDmyK3NQmktBW7R5WTupnXjFzjhkkEgx3gAvFs0yRTYcByHMeQBeEDOtaYvOQlyMGI8zBV4oE8YR/fOp9hKvSav2lEOJ7GgjFeKaxBm471WaSmXYJMbliMUal6woa2bnzVrzrHaZb6DUhdo8VJFOmVV8WQH4fZybfGvLyFTGcrr0Yhkww6XwZlKbDaZ3QZSUCBJCxLLY0JcqzY2z4YhWn4dYkgsBaggRKBABaNp6RGfmn1Zilh2zPkc+JrEMngd4rgsxGAtmVAZVGkhhlIxjlM6tPHOBmr6CdHaVy73LJK3SzNr4IjgLgkn0zjtrsPfeWXRy7k0COPVCdnnSOH/fnJxdNOjL00uV6hQCjDsFacwDpmmrWaUaYgjQbVtfaMJMMLv+RnzuOuRBC70gUaOYgekSEebuNwQXl6h7A0ka4/fW31niL88S32Eu+I5DnpZChEURF9yg1764EGVcgMpDXU3xC/W8SCqYKpkJVJeHyq9apXhepO0POIH3ZxfswmtcwKV3dtm89C5P3597wJpnEVyeNS/O3FENJtM20HNB7RLiTstrgcr0bCVYGwVr52IRVER7rQbPyze8WE49Iz1tQlCqjTlmYxuU+tp2IoWX2+JbmHDb7Xmpgoh/bocwyaZkHzcRj6FdeMZXqW1BJtqLgSnhsTBUXqRk4qUAKod4QmslCcMM+riOpKcqBTG31stNPxwMfh3fDrq98W3npkerdM7iDEqO9FLdhL26fhjd94bjfrcE/LrkewDTFuFS3tw6cDfsf+rc98b9uwL8g5IJ+mnEIQ6HEK2+3zEzoy2qDTOZrqUy7N/Rl5cdhA8/X/ev/hJ8d4OuFWR017l6nTQ3g9tx77Z7N+jf3o+2GQikiPj0hqW/wjLn4xkwhdrCsCdw0WFAhKnkwug9ol4Nbj/0P467/WGJufqcqXrMJ3XEs2uf3t0vxbnBp95w2O+WJauDCezBumO2LuegFA+hhr/L2Hbc65tqTiGoCRnCLSLYI8zwYfTLTeduPBwM7kssrTPbAfhfBqP7b/u5FfzT4Prhpjfu9n5++FgCPwg4+rV/Nx72RveDYe9qcHvoSPdm3O2POj9f98YP3d6nQ2CW7cGoOxrf9Ybjbu9Tf8OrNrkY3N53+rcYkzedj2Ww3zO2tJUd0pn9aM1dm0z3eO5rjVO0JzWxYxxEddO7GQz/c3zdv+nfb6NUoGWmAvhQQh3yOdfYd7keKoegLRrzhBtdSyCRanmQzrD3j4fe6M9RUvA7FuzDtK7uHv4ZgZp7BQrSbD+Fv1SUHSrWVx9GvfGwc9sd3IxvB7cbLmWr2B4PtD7dGd6P+7f3veGnznXpzG5OxwMPDxu1AU6b7yfvJpfe5eXZqXcWRWcea5yfelHj3flJI5zA6el+PBtY9pQPFAiTKAb1ttI09tZmO3liv2f7pD0J1CWxPTrrDD+OSox4CXl7tMvA8Q5/P18Prn4d33XufyknzRDmdR2yPXX10/hmsJFgFVvs1YvLBuOr686ozBg2qzsFbtD9E9XNdQvu1GOpJuVHd1SIPTCIuVcUAmyLU2zQWOxmOouSJ2z69eSUw9xlcXwnYx6g6frRrTR3CjRONlUa8zkI0PpOyYkdmuCz61hLXQ92OlXqcVql2raNAa1SX8jMpG2fvj1CmsTzWJhw4blpgNRVJtYcoUS1Ro1p+UyccsiJnzUap4CfJxfNY5/6QgXtt//uCx6RR/JWBcQTQBrkiTgY90keyQ/Ei0jdJGndzgoxQEqefiJmBsIXvoFgJolvWzWSMzMDFpsZCWYQPBOcQiAkC25meIZEMo7lgospQYEy00JWVmje4qJPyf8QDSHxgFR0/b/qjndSn1Ys6GdukGFfRNwXFEfjfNS5nynQMxmHtHVepTgBcBZ3IWbLEQRShDgyN6o0BcVluFq6aFSpzoIAtC4haFap4QnIzKwATxvr0cPNXUUGs5Ouy5PWmGlGW/SUVmmemVv07CPHAaXIc2uoZhnqBKFwjoEgU9wsr6Qw8NkgdKr4nMcwhbC4Z1DAwoGIl0MpzQceg15qAwlt2U66SlUmOvoBh5JWww7yTJks/dvx/o8d791J4/Wuh0vbXnb+UqVzGWcJ3MgMm2LMYQl+zdPdZve7WRJsm43ZdOMAdrz5DLqR/GxiLKe9wqXytLeNp7D1Go81utO+9rQMnr2Qq10GLMdyupcJxBDL6YFDfJJ7l2J671G3sXM4hPka2l2P6F0oFCjbAFWZ8OzK19mROnQO31gfZYHhc2bAXUY8VelCqmcupl2udnRgC4rg5mrzaqIUjPUJF/UJKweiBkM8SUAp9MTV75SngH64WhAys1/f2BggPCJzpjibxKAJU0Ds7gr6s1EsAF/4wjUvGOqlXgajxHYMH0b9bvs1FyIOEXZT7df0UA7czh/jD9edj22fet7qKgrJ2+ahc9/BKbB9wA4+fVvi1/UkbcUWvnBtx0qqdXtj41+8yXOInf58SpJMGzJjc7BJw6eR5qFPyTSWExYTFzKZsvc3xEhSGJ2ACNQyNRCSwairES+LDKit1HMF6ayiyd2QcE0SUFMIawg7MybVrXp9ys0sm9QCmayTah29gpv6yfnF+Tmcs8vLk/fQaDRZGJ1dvDtvNtnFJITL5tnJpMHO350jvvsZ18QWJMfpgscxmQBRkMg5hCRmBlSNdHIWmamSBZCAiRyCmBm3QqD4hIvfILB4AhlCjWARYZN4SRiZShkSTGEo0GIGVik5jn9kXARLorM0lcocktOO4vYjzeK43mycnp6+CbkOMq25FGN1+f7ksvnu8t07XwQp8TwhvRQbKjWHdiJDIDa/GUhSb6Uza02ySnzrNV+kSzOT4pR4AUEP4Akylxs2ZUqDQrdwv0l7Y6N2ZX/c2R9HxwVUDdPmUWUPtcox4uIREbIgUZsxPdZOnUcV51aV45YvCCE5yGOx/ETa5MvLfhSuQ11hqFbQUgcRPbptRFh5uwpnrGrCFkiZgtgvQZVUFpVjwnSOM+IxbBCpLRQ3cLTeRZnz0HJX80TAgjzDUnEx9YXrIgQJYg7C1Gw3QVhmZmQKxpPKyw9hK1GKaoIFvsIwkEiqJBJCkApJpmrvutRhsf5jhXjPJW9wHUzOkbv+rq/4s6GLUe0yAKKxKrJpxhVm17s8YtuQpxqfknab+DSeJz4lT6U25f7mzuatt0fJM/oo8ULrFL55s04eUoe4UiJB4nmy3rdO7/q7kORKWWdMn5KyovLvmICdHZBUINOllcckaaRJyJV9NlraHEaQMamYWq433DFMAxjZAiCEkEwgYJmGPNBnQLCGkdX9OsG6pKslOlOJNWfBloSJkHBjvciAME6nWMVWZGyOUqCz2BDu0EOSmmXIFXI5yX864sz268iSEythXFhbrZixCsV8MQc1kTpXYgiqePgrFFWUFp/Wf8S13GI+rRfKy4TtBEpyaY4IFkBCKSrGqodwQ5hYJli0hG8yd2SHxoZBJix4tlgLrcioRIQLI50PKj7lgsVWwA0LfVPAtTA/HuZFgbHqW2AjMuMpsoGBpEnOgI3ZTIOq2ycNS3kmF2KDuMKhRWPcIHjLnvkKybzMwCHnU8kGTvtasyWTLyDW4BBWysGj2ILEXJsKOVrMeDArIhkLR4ot0G9ygvJoV61sBcCyTaQgQoagjx1S1EymhLaye5HiIMJ4SVw/SbDvw6KHBLB0BjMmpnlwuGDVRMFESlMj99JhfMYpZjDqklSGmkRKJsR2sNgcKEjtSyYWVluYWX7cFmfrY0bm0w23juJw2sfpgqkFuO6Fa6INRlQgFWq16iIwQjflGp12tbFG7vBpFIbFsdVayPWzVVPERWhXFJ/ODJECagiO5r3uj+7thOhS3HFh5igTrm3As2PH4NEx+YJ7SOi338lCZjHmFZJat8UmtUommSGJpVJqWSKpyIypEASEto8i9rZFF8iKuq5nFiV85trmEWZ1ULTZqJyiEXMn9/cDeqmr5DcthS+Q8LhKuIgk4sPFWixZeKSXuqZNyMVxjaOxj7D6+gZ1LCL5WJE6HLui2yZ5em7lNBUX5shBObVUnqoERNiu2JZhBVKJZCbC3LYtUiH/RrZP2fe1nBVQ6pi8IbGcrmyH1nVIjCQOxBHAI5ivjxrHhLzJE5l1z1RJTK52VMY0skLFtUPli9XhipBuz9GwWR3du98tRK4U/YB5cV7BI/ID2RetGNuuW8cq5kbvPIGgh/l0XVXR4C4ESD4z2Vknrz2WoYQtnftgmIbukB32K5W9yK3DcpvamNnZKvguebLDEhxmcb+MB0gfJOub9QRz9C3yx2U5Hh+J998bSn3aumT5QvLrD7SixepT8pMt46RJfiIvRbruR4SRZ1AC4nL2K1LejKUpCG1TDNvMR+ieDkde09E71u3HBAKJWTTh1uGqrhVAGGtP57RM5X0CxjEXmcx0vKy5ISdVchJDYhOwNbeW8dz1BbYlVYnr6myXAWTOYh5ysywqbYlLy71yf25B38dEGREBGAtMLVfZbj1BHhpGc8+vW0XkrvNIvGsXEuvzxGNoniOcH2Iunrf3j31KfmiT3H7kiZS9SyXb4CvLO11teZ8dlaSIVwOZJqspm2i51cwYSVKmNWHEAUQxs1rcxlpqUQu3LPnbRt9qf9jOprhA23nV/15eMP+fvdHc9O47rgVzD7Lb+//RuT60te9B6zt+X9m6ivuXX9Fv3sG/7lo3eN0F4z9347lxVfJn74O/OlDvC4QNuF38T0ih+HsWJqdIe5MQ5h58Tln+J60UaVcPa2vPP4hWac4zUsb0L/Y1x5vn2P3b4TZsuBoPrelWw1z+l7hW7oPbzwP7b/7Xrw9bJsZB9K82qkXqrcYG+65inzb+1fb9Q29+rzP/389I3/MzEpZGIUMYlf7ovfmXYuw68rPrP4Whm2LfvPLzTrxgS8xPGhSK0QkCZOt2T7Gmhec4p0H8jvsvNN3nOy8HnOfrTpIq+RsEBn37C12FVLmfwA07WNtl12blrVSVYjtLWyfvT6oFU0FxZ207lK8Rf3l5sq3YtySrw2dslO3/evP/W3eLyWWgrlb/gv+mF3+T0Er/defDh337D6AqfPpr3r4PnfPbfV6/D3rl/C/7vf8PMPya18Xxa94UX2evnfL2zX7Ftf7rIWAnejYPWD9z/70wGeb/l5f/DQAA//9QSwcI9mHNKtgPAABNNQAAUEsBAhQAFAAIAAgAAAAAAPZhzSrYDwAATTUAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAAA4QAAAAAA==
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2025-03-29T17:13:39Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      pod-template-hash: cddcdb46d
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0-cddcdb46d
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: fed060e3-4b07-4c07-8720-734d979bdd12
    resourceVersion: "10640776"
    uid: 95c1d2aa-d2cc-4002-ac1a-81e123f20e7a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: cddcdb46d
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-m-1
          osd: "0"
          osd-store: bluestore
          pod-template-hash: cddcdb46d
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-m-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-m-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: e319b7b8-8843-4ff4-a053-f07520dbe330
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsWntvGsmW/yrndiJhz9KAn3EYoRVjSAaNbXyxndXKeFHRfRpq3F3VU1UNYbP+7qtT1Q3NK/HcyUrRav5BUI9T5/E7ryq+eAkaFjLDvOYXL2ZjjDV9Y2nqNT0l5bMfYDr1pQ69Ko3WnrMxKoEGdY3LeiCTVAoUxmt6tDCIM21Q6Rr9qNH+Gpe7dypkBkN/vFg/KEXFjFQ793ChDRMBek2vsXNBwgSb/EmigiWYc79XzJQp48uoTNSrLrf4PMw5siMzVJpL4TW9o4vace3YL2ZGIcNEitHa8mLQLFJiw3EQ4owH6Acx09pretOQBiPG40yhH8qEcSL/fKL9xD/yqnaXI0nsaCMV0RrHGbrvVS+VyrBxTMMRizUNWVFWzM6OakentYt8gqQu1OaTinTKrOLLCqDvo9zkG1NGpjKWk4Ufy4AZLoU/ldqsMb29RElJS0KMWBYb76Xq5cZZA6LV5z6W5FygGmCECkWA2ms+Epj5p6WYZWDWZ8THOJbBc5/2dTBGY5cZlWHVC6QwSsYxSedGnrkgTV9iOr3M5d5m0qt6mbXxeXAaBOPonX/aYO/90+j4zB8HeOKH7OK4cfb+aHx63vBenl6qnk4xIL9TmMY8YNprHlU9jTEGBNvmFy9hJphefcM/twG510IvdKhRzOBkQQRz+A3Q+SXp3mCSxvS9+XeU+O5R4geMBT+wy3slF2FRxAU3hNoX56KMC1S5q6sJffF8P5IKJ0pmgtTlk/KrVim+H2n7A4/x/ek5O/cb53jhn14cXfgXJ+/PfGRHpxFenB6dn7qtGk2mraPngtohop2WxwKV6elSsBYJ1srFAlJEa6UG388n/FhOfCN9bUJUqkUxZm0alfradCKFn9viW5Ro2s35qcKIf26FOM4msIubiMfYKpDx1dM2Vibaj5Ep4bMwVH6kZOKniCpf8UTWShJGEfRx5UlPVQ/FzFovN/2g3/9tdNPvdEc37euuV/VmLM6wBKSX6vray6uHu/vuYNTrlBa/LvjuobRxcClubmy4HfQ+te+7o95tsfyDkgnhNOIYhwOMlt9vmZl6TU8bZjJdS2XYu/VeXrYIPvxy1bv8LvRu+x0ryN1t+/J10lz3b0bdm85tv3dzf7fJQCBFxCfXLP0NFzkfz0gh1CaGHY5LgEERppILo3eIetm/+dD7OOr0BiXm6jOm6jEf14nOtn26t78W+/qfuoNBr1OWrI4msBvrjtm6nKFSPMQa/S5T24LXN9WcYlATMsQbIrBDmMHD3a/X7dvRoN+/L7G0imx71v/av7v/Ns6t4J/6Vw/X3VGn+8vDx9LyvQvvfuvdjgbdu/v+oHvZv9m3pXM96vTu2r9cdUcPne6nfcss2/27zt3otjsYdbqfemuoWueif3Pf7t2QT163P5aX/ZGxhc3smE7tR3PmymRvB3Jfa5yiPKmJLeMQqevudX/wn6Or3nXvfpOkQi0zFeCHEumQz7imusvVUPkKr+nFPOFG1xJMpFrsPWfQ/edD9+6vnaTwD0rY+8+6vH34VwQ62ilQkGa7T/iuomydYrH6cNcdDdo3nf716KZ/swYpm8V2INBiuj24H/Vu7ruDT+2r0p7tmE4bHh7WcgOeHL0fvxtf+BcXpyf+aRSd+qxxduJHjXdnx41wjCcnu+msUdmRPkggCqLk1JtK01Rbm83gSfWerZN2BFAXxHborD34eFdixE/g7cE2A4db/P1y1b/8bXTbvv+1HDRDnNV1yHbk1U+j6/5agFVsvlMvLhqMLq/ad2XGqFjdSnD9zl/Ibq5acLseSzkp37qlQqqBUcz8IhFQWZxSgcZi19NZkjxhk68Hp3zNbRbHtzLmAZmuF91Ic6tQU2dT9WI+Q4Fa3yo5tk0TfnYVa6nqoUqn6vncq3ralo2BV/WGQmYmbQ29twd0Jvg+CxMufNcNQF1lYsURSVRr1JiWz+CUA8fDrNE4Qfo8Pj86HHpDoYLW238fCh7BI7xVAfgCoQFP4Na4T3iEf4AfQd0kad32CjFiCk8/g5miGIqhwWAqYWhLNciZmSKLzRSCKQbPQF0IhjDnZkp7IJJxLOdcTIAEykyTWFmSeUuDQw/+BzSG4CNUdP2/6o53qE8qdulnbojhoYj4UHjUGuetzv1UoZ7KOPSaZ1WPOgDO4g7GbHGHgRQhtcyNqpei4jJcDp03qp7OggC1LhE4qnqGJygzs1x40li1Hq7vKiKY7XRdnLTGTLM8jOaRuemdfuTUoBRxbrXqrNFIyguPaSG1MhhkipvFpRQGPxvakCo+4zFOMCyuGhSysC/ixUBK84HHqBfaYOI1bTFd9VQm2vqB+pJmw/byTJks/Rt7/8fYe3fceD36aGgTaGcvVW8m4yzBa5lRXUxhLKGvecRbL4DXs4KttCmgrm2gojdvQ9fin42N5chXQCqPfJt0Cluv6FijO+1rX8vg2Q+52mbAciwnO5kgCrGc7NnExzm6FNM7t7qJrc0hzlar3Q2J3l5FAmVrS1UmfDvydXakDh3gG6utLDB8xgy6+4inqjeX6pmLSYerLR3YnCK4uVy/nSg5Y33MRX3Myo6o0YAvAZUiJC5/pzxFwuFyQMjMfn1jfQB4BDOmOBvHqIEpBDu7XP3ZKBbgUAyFq1/I1UvlDHmJLRo+3PU6rdfciThCVFC1XlNGueW2BRl9uGp/bA0931/eRtHxtn5o37epEWztscPQe1vi15UlLcXmQ+Eqj6VUqwrH+r94k8cQ2wAOPUgybWDKZmiDxtCLNA+HHkxiOWYxOJfJlL3CASOhMDqgCNQiNRhC/66jiS6LDKqN0HOJ6bSi4XYAXEOCaoJhjdZOjUl1s16fcDPNxrVAJqugWidUcFM/Pjs/O8MzdnFx/B4bjSMWRqfn786Ojtj5OMSLo9PjcYOdvTsjevdTrsHmJMfpnMcxjBEUJnKGIcTMoKpBO2eRmSrMEQIm8hVgptwKQeIDF79jYOkEMsQaUBJh43gBDCZShkAhjASaT9EqJafxz4yLYAE6S1OpzD45bTduP9IsjutHjZOTkzch10GmNZdipC7eH18cvbt4924oghR8X0g/pZpKzbCVyBDBxjeDSeovdWatCcvAtxobinRhplKcgB8AIYAnxFxu2JQpjYpg4X5Da22idml/3NofB4fFqhqFzYPKjtMqh0SLRyBkcURtyvRIO3UeVBysKofNoQCAfMljMfwELfjyspuEK1KXFKoVstReQo9umghW3i7dmbKasAlSpih2S1CFyrxyCEznNCMe49ohtbniBg9WsyRz7lrudh4EzuEZF4qLyVC4KkJAEHMUpmarCWCZmcIEjS+Vn2+iUqLk1UAJvsLIkSBVkg6iJRVIJmrnuNRhMf5TBfznEhpcBZNz5G7A60v+rOuSV7sIQGSsimyYcYnZ1S6PVDbkoWboQasFQy+eJUMPnkplyv31rY1bbw+SZ8Io+KEFxdC8WQUPqUMaKR0B8SxZzVvQu/ouhFwpq4g59KCsqPw7BWBnBzoqkOnCymOSNNIQcmVfjhY2hgExJhVTi9WE20ZhgDxbIIYYwhgDlmnMHX2KQDkMllfsQHlJV0vnTCTlnDlbABMhcGNRZFAYp1PKYstjbIxSqLPYAHfkMUnNIuSKuBznP93hzJbsxJITK2FcWFstmbEKpXgxQzWWOldiiKp4+ysUVaSWoVf/icZyiw29eqG8TNhKoCSX5kRgjhBKUTFWPcANMLFIKGmJocnclq0z1gwyZsGzpVpoRUalQ7gw0mFQ8QkXLLYCrlnomwKuhPlpPy8KjVXfnAqRKU+JDXIkDTkD1mczjapuXzXsyVM5F2uHK2paNPkNLW/aPV85Mk8zuA98KlmjaR9sNmQaCow1OoKVsvMoNoeYa1OBg/mUB9PCkylxpFQC/S7HJI922cpmAErbIAUIGaI+dERJM5kS2sruR4qjCOMFuHoSqO6jpEcHUOoMpkxMcudwzqpB4VhKU4N76Sg+UxfTv+tAKkMNkZIJ2AqWigOFqX3MpMRqEzPLt9vkbDFmZN7dcAsUR9O+TxdMzdFVL1yDNuRRgVSk1arzwIhgyjWBdjmxIu7oaRKGxbHVWsj1s1VTxEVoRxSfTA1IgTVaTua96t3d2w7RhbjDwsxRJlzZQHtHjsGDQ/hCc3TQ73/AXGYxxRVILWypSK3CODOQ2FNKJUskFUyZClFgaOsosBcuuiBW5HU9tSTxM9c2jjCrg6LMJuUUhZjbubse0Atdhd+1FENBB4+qwEUkiR4N1mLJwgO90DVtQi4Oa5yMfUDZd2hIxyKSjxWpw5FLui3Iw3MzP1NxYQ7cKqeWylMVUIStii0ZlksqkcxEmNu2CRX4N9jcZZ/YclZQqUN4A7GcLG1H1nVEjAS3xB1AWyheHzQOAd7kgczCM1WSgqttlSmMLElx7UgNxXJzRUg3586wUZ3g3esUIleKesC8OFTwCP4Bu7yVfNtV65TFXOudBxBC2NBbZVUyuHMByHsm2+vkuccylLCFgw+5aeg22Wa/UtlJ3AKW29DGzNZUwXcJyY5KsJ/F3TLuOXrvsUOz6mAOvnX8YVmOx0fw/3tNqU8blyxfIL/+ICtaqkMPfrZpHI7gZ3gpwnUvAgbPqATG5ehXhLwpS1MU2oYYth6PCJ6ORp7TCR2r8mOMgaQomnALuKorBWiNtacDLVN5nUB+zEUmMx0vaq7JSZUcx5jYAGzNrWU8c3WBLUlV4qo6W2UgzFjMQ24WRaYtcWm5V+7/LYR9CpQRCCRfYGqxjHarDnJfM5ojv24VkUPnEfwr5xKr/eAzMs8B9Q8xF8+b84dDD/7Rgtx+8ARldKlkc/nS8k5XG+izrZIU8bIh07DsskHLjWLGSEiZ1sDALYhiZrW4SbVUohawLOFtrW61P2xlU1ygbT3s/yiPmP/Pnmmuu/dtV4K5N9nN+f9oX+2b2vWm9QM/sWxcxf0It/Tr1/Cvu9kNXnfH+K9deq7dlvzVK+Gv9tS7fGFt3Tb9Jzqh+JMWxadI++MQZz5+Tln+V62Uzq7u19aO/xEtI51vpIy97ww3x5vv2P0bc9pbN+OySbTWW7Z0+X/jmjkMNx8Jdt//r94gNqxM7ej3tqsl6i+bB/u6Yh84fgAT/6nHv9ch4O/3pB/5PYlypJAh3pX+9L3+92IqP/K9qz+IEVKpgF5CvR3P2YKilEZFYrSDgNi62ZG1vQI5DjRE33H/xUt3YedlD3i+DpJUyd8xMITtL97Sq8qFBU3YDtsOu3orr6mqHtW1XvP4/XG1YCooLq9tqfK1w19enmxN9i3J6viZKmb7H9/8v9edooXpq8vlP+K/ieJvHrTUf91heD+2/wSpAtNfQ/sucg63u1C/a/US/C+70f8nGH7NM+PoNY+Lr7PXVob7ZtXieoBVN7DlPesbLM7cnzBMRing5eV/AwAA//9QSwcIsyOkzNsPAABZNQAAUEsBAhQAFAAIAAgAAAAAALMjpMzbDwAAWTUAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAABEQAAAAAA==
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-03-10T18:51:40Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "0"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "0"
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-m-1
      osd: "0"
      osd-store: bluestore
      pod-template-hash: f9d5d9d8
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-m-1
      topology-location-root: default
    name: rook-ceph-osd-0-f9d5d9d8
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: fed060e3-4b07-4c07-8720-734d979bdd12
    resourceVersion: "7405556"
    uid: 23a6276d-e66b-47a7-a99a-1b9b4b158f07
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: f9d5d9d8
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "0"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "0"
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-m-1
          osd: "0"
          osd-store: bluestore
          pod-template-hash: f9d5d9d8
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-m-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-m-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: e319b7b8-8843-4ff4-a053-f07520dbe330
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=e319b7b8-8843-4ff4-a053-f07520dbe330\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda1
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "0"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-0
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_e319b7b8-8843-4ff4-a053-f07520dbe330
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-03-16T18:47:55Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 544c8b779
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1-544c8b779
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: 4c42924e-03e7-45bb-a6d2-679081f0ffbb
    resourceVersion: "9908496"
    uid: 7fbc3100-4aa1-4b56-b119-04ceca96f934
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 544c8b779
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-w-1
          osd: "1"
          osd-store: bluestore
          pod-template-hash: 544c8b779
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-03-09T22:35:21Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 57c89897f6
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1-57c89897f6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: 4c42924e-03e7-45bb-a6d2-679081f0ffbb
    resourceVersion: "6228824"
    uid: 2344cbb7-9fb5-4f8b-8041-7b130b798010
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 57c89897f6
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-w-1
          osd: "1"
          osd-store: bluestore
          pod-template-hash: 57c89897f6
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-09T20:49:46Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 57d677f45
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1-57d677f45
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: 4c42924e-03e7-45bb-a6d2-679081f0ffbb
    resourceVersion: "6022857"
    uid: 76bafc6d-6c2f-4f03-9d31-90c94b30f464
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 57d677f45
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-w-1
          osd: "1"
          osd-store: bluestore
          pod-template-hash: 57d677f45
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
      deployment.kubernetes.io/revision-history: "1"
    creationTimestamp: "2025-03-08T15:03:40Z"
    generation: 4
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 5db7955bfb
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1-5db7955bfb
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: 4c42924e-03e7-45bb-a6d2-679081f0ffbb
    resourceVersion: "6029541"
    uid: 5baeda3b-158f-41e9-b5a2-5d2baeb49f7e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 5db7955bfb
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-w-1
          osd: "1"
          osd-store: bluestore
          pod-template-hash: 5db7955bfb
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-03-09T22:20:21Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 6bdcc4ff64
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1-6bdcc4ff64
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: 4c42924e-03e7-45bb-a6d2-679081f0ffbb
    resourceVersion: "6031819"
    uid: 955589d5-750c-4a27-af8c-197a6deaaac2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 6bdcc4ff64
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-w-1
          osd: "1"
          osd-store: bluestore
          pod-template-hash: 6bdcc4ff64
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: 500m
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: 500m
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: 500m
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: 500m
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 400Mi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2025-03-29T17:14:27Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 756ccd44c
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1-756ccd44c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: 4c42924e-03e7-45bb-a6d2-679081f0ffbb
    resourceVersion: "11100320"
    uid: 87a37012-88a1-4f4b-98e5-29878ecda3b7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 756ccd44c
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-w-1
          osd: "1"
          osd-store: bluestore
          pod-template-hash: 756ccd44c
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-03-10T18:52:30Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "1"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "1"
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      device-class: hdd
      failure-domain: k3s-w-1
      osd: "1"
      osd-store: bluestore
      pod-template-hash: 78dcc4fd95
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-1
      topology-location-root: default
    name: rook-ceph-osd-1-78dcc4fd95
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: 4c42924e-03e7-45bb-a6d2-679081f0ffbb
    resourceVersion: "7405655"
    uid: ab2c4e1a-a812-4846-8350-f9ef8b770b85
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 78dcc4fd95
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "1"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "1"
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          device-class: hdd
          failure-domain: k3s-w-1
          osd: "1"
          osd-store: bluestore
          pod-template-hash: 78dcc4fd95
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-1
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-1
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: hdd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.1.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=a99210a3-f301-4ba2-8f87-85a3c5ae2b1e\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "1"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-1
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_a99210a3-f301-4ba2-8f87-85a3c5ae2b1e
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-03-10T18:54:31Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k3s-w-2
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 55d549c6c
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-2
      topology-location-root: default
    name: rook-ceph-osd-2-55d549c6c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-2
      uid: 25c07072-1e4b-44d0-bc38-26a56f686927
    resourceVersion: "7405795"
    uid: f937900a-e190-4d14-a20d-3c0ec99ebdf3
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        pod-template-hash: 55d549c6c
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: k3s-w-2
          osd: "2"
          osd-store: bluestore
          pod-template-hash: 55d549c6c
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-2
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: d0c27c6b-8b46-4935-b66b-1b22f85feda5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=d0c27c6b-8b46-4935-b66b-1b22f85feda5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_d0c27c6b-8b46-4935-b66b-1b22f85feda5
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-03-29T17:15:36Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k3s-w-2
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 56c7cdd86f
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-2
      topology-location-root: default
    name: rook-ceph-osd-2-56c7cdd86f
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-2
      uid: 25c07072-1e4b-44d0-bc38-26a56f686927
    resourceVersion: "11084690"
    uid: 70604cd3-722c-431e-a2dd-881516328eb8
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        pod-template-hash: 56c7cdd86f
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: k3s-w-2
          osd: "2"
          osd-store: bluestore
          pod-template-hash: 56c7cdd86f
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-2
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: d0c27c6b-8b46-4935-b66b-1b22f85feda5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=d0c27c6b-8b46-4935-b66b-1b22f85feda5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_d0c27c6b-8b46-4935-b66b-1b22f85feda5
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsemtzGrmX91c500kV9jw02MR2HKaopxhDMtTYxn9sZ2vLeCnRfRo07pZ6JDWEzfq7bx2pG5pb7PlPXqS25g0Fuhydy+/cJL56CRoWMsO85lcvZmOMNX1jaeo1PSXlkx9gOvWlDr0qjdaesjEqgQZ1jct6IJNUChTGa3q0MIgzbVDpGv2o0f4al7t3KmQGQ3+8WD8oRcWMVDv3cKENEwF6Ta+xc0HCBJv8RaKCJZhzv1fMlCnjy6hM1Ksut/g8zDmyIzNUmkvhNb3j81qj1vCP8plRyDCRYrS2vBg0i5TYcByEOOMB+kHMtPaanraDEeNxptAPZcI4kX96p/25T4RolyNJ7GgjFdEaxxm671UvlcqwcUzDEYs1DVlRVszOjmvHJ7XzfIKkLtTmk4p0yqziywqg76Pc5BtTRqYylpOFH8uAGS6FP5XarDG9vURJSUtCjFgWG++56uXGWQOi3byPJTkXqAYYoUIRoPaaDwRm/nkpZhmY9dmxV/XGsQye+rSvgzEau8yoDKteIIVRMo5JOjfyxAVp+gLT6UUu9zaTXtXLrI3PgpMgGEfv/ZMj9sE/iRqn/jjAd37IzhtHpx+OxydnR97z43PV0ykG5HcK05gHTHvN46qnMcaAYNv86iXMBNPLF/xzG5B7LfRMhxrFDE4WRDCH3wCdX5LuDSZpTN+b/0SJ7x4lfsBY8AO7vFdyERZFXHBDqH12Lsq4QJW7uprQF8/3I6lwomQmSF0+Kb9qleL7kbY/sIEfTs7YmX90huf+yfnxuX/+7sOpj+z4JMLzk+OzE7dVo8m0dfRcUDtEtNPyWKAyPV0K1iLBWrlYQIpordTg+/mEH8uJb6SvTYhKtSjGrE2jUt+aTqTwc1u8RImm3ZyfKoz4l1aI42wCu7iJeIytAhnfPG1jZaL9GJkSPgtD5UdKJn6KqPIVj2StJGEUQR9WnvRY9VDMrPVy0w/6/d9H1/1Od3Tdvup6VW/G4gxLQHqurq+9uLy/vesORr1OafHrgu8eShsHl+LmxoabQe9z+6476t0Uyz8qmRBOI45xOMBo+f2GmSk5sGEm07VUhr0b7/l5i+D9r5e9i+9C76bfsYLc3rQvXifNVf961L3u3PR713e3mwwEUkR8csXS33GR8/GEFEJtYtjhuAQYFGEquTB6h6gX/euPvU+jTm9QYq4+Y6oe83Gd6Gzbp3vzW7Gv/7k7GPQ6ZcnqaAK7se6YrcsZKsVDrNHvMrUteL2o5hSDmpAhXhOBHcIM7m9/u2rfjAb9/l2JpVVk27P+t/7t3cs4t4J/7l/eX3VHne6v959Ky4/3Lbz9vXczGnRv7/qD7kX/et+WztWo07tt/3rZHd13up/3LbNs9287t6Ob7mDU6X7uraFqnYv+9V27d00+edX+VF72Z8YWNrNjOrUfzZkrk70dyH2tcYrypCa2jEOkrrpX/cF/ji57V727TZIKtcxUgB9LpEM+45rqLo9K92KF1/RinnCjawkmUi32njPo/uu+e/v3TlL4JyXs/Wdd3Nz/OwId7xQoSLPdJ3xXUbZOsVi9v+2OBu3rTv9qdN2/XoOUzWI7EGgx3R7cjXrXd93B5/Zlac92TKcN9/druSE8Chrvg7Oxfz4+OfNPPrw79cdnZ2P/eNxoROenEYbsdCedNSo7EhEJREGUnHpTaZpqa7MZPKnes3XSjgDqgtgOnbUHn25LjPgJvD3YZuBwi79fL/sXv49u2ne/lYNmiLO6Dtm2NJ9HV/21+KrYfKdaXDAYXVy2b8t8aVu5beS3fudvJDdXLLhdD6WUlG/d0iCVwChmfpEHqCpOqT5jsWvpLEmesMm3Y1O+5iaL4xsZ84As14uupblRqKmxqXoxn6FArW+UHNueCb+4grVU9FChU/V87lU9bavGwKt6QyEzk7aG3tsDOhN8n4UJF75rBqCuMrHiiCSqNWpMyydwyoHGMDs6eof02Tg7Phx6Q6GC1tv/PxQ8ggd4qwLwBcIRPIJb4z7hAX4CP4K6SdK6bRVixBQefwEzRTEUQ4PBVMLQVmqQMzNFFpspBFMMnoCaEAxhzs2U9kAk41jOuZgACZSZJrGyJPOWBoce/A9oDMFHqOj6f9Ud71CfVOzSL9wQw0MR8aHwqDOmWp+zuIMxW9xiIEVIzfFR1TM8QZmZ5djpqndwjVMRgmyr6gKdNUea5XEwD61N7+QTpw6jCFSrVadHR0l5YYMWUi+CQaa4WVxIYfCLoQ2p4jMe4wTD4q5AIQv7Il4MpDQfeYx6oQ0mXtNWw1VPZaKt76mxaB7ZZpwpk6X/oOe7oifvk++mCvVUxqHXfN84+gaoUlRchi/ibCbjLMErmVFhS4Eooa95zFqvYNfDui2VKSSubaCqNe8j1yKYjW7l2FVAKo9dm3QKW6/oWKM77Wtfy+DJD7naZsByLCc7mSAKsZzs2cTHOboU0zu3uomtzSHOVqvdFYfeXkUCZWtLVSZ8O/JtdqQOHeAbq60sMHzGDLoLhceqN5fqiYtJh6stHdisILi5WL9eKDljfcxFfczKjqjRgC8BlSIkLn+nPEXC4XJAyMx+fWN9AHgEM6Y4G8eogSkEO7tc/cUoFuBQDIUrQMjVS/UIeYnN+h9ve53Way41HCGqiFqvqYPccttDjD5etj+1hp7vL6+T6HhbAbTv2tTJtfbYYei9LfHrCouWYvOhcLXDUqpViWL9X7zJY4jt4IYeJJk2MGUztEFj6EWah0MPJrEcsxicy2TK3sGAkVAYHVAEapEaDKF/29FEl0UG1UboucB0WtFwMwCuIUE1wbBGa6fGpLpZr0+4mWbjWiCTVVCtEyq4qTdOz05P8ZSdnzc+4NHRMQujk7P3p8fH7Gwc4vnxSWN8xE7fnxK9uynXYHOS43TO4xjGCAoTOcMQYmZQ1aCds8hMFeYIARP5CjBTboUg8YGLPzCwdAIZYg0oibBxvAAGEylDoBBGAs2naJWS0/hXxkWwAJ2lqVRmn5y2nbYfaRbH9eOjd+/evQm5DjKtuRQjdf6hcX78/vz9+6EIUvB9If2UqiI1w1YiQwQb3wwmqb/UmbUmLAPfamwo0oWZSvEO/AAIATwh5nLDpkxpVAQL9xtaaxO1C/vjxv44OCxW1ShsHlR2nFY5JFo8AiGLI2pTpkfaqfOg4mBVOWwOBQDkSx6K4Udowdfn3SRcmbmkUK2QpfYSenDTRLDydunOlNWETZAyRbFbgipU5pVDYDqnGfEY1w6pzRU3eLCaJZlz13LX6yBwDk+4UFxMhsJVEQKCmKMwNVtNAMvMFCZofKn8fBOVEiWvBkrwFUaOBKmSdBAtqUAyUTvHpQ6L8Z8r4D+V0OAqmJwjd4VdX/JnXZe82kUAImNVZMOMS8yudnmgsiEPNUMPWi0YevEsGXrwWCpT7q5ubNx6e5A8EUbBDy0ohubNKnhIHdJI6QiIZ8lq3oLe1Xch5EpZRcyhB2VF5d8pADs70FGBTBdWHpOkkYaQK/v0s7AxDIgxqZharCbcNgoD5NkCMcQQxhiwTGPu6FMEymGwvCMHyku6WjpnIinnzNkCmAiBG4sig8I4nVIWWx5jY5RCncUGuCOPSWoWIVfE5Tj/6Q5nQYCaSrWxEythXFhbLZmxCqV4MUM1ljpXYoiqeLwrFFWklqFX/5nGcosNvXqhvEzYSqAkl+ZEYI4QSlExVj3ADTCxSChpiaHJ3JatM9YMMmbBk6VaaEVGpUO4MNJhUPEJFyy2Aq5Z6EUBV8L8vJ8Xhcaqb06FyJSnxAY5koacAeuzmUZVt88S9uSpnIu1wxU1LZr8hpY37Z5vHJmnGdwHPpWs0bQvLhsyDQXGGh3BStl5FJtDzLWpwMF8yoNp4cmUOFIqgf6QY5JHu2xlMwClbZAChAxRHzqipJlMCW1l9yPFUYTxAlw9CVT3UdKjAyh1BlMmJrlzOGfVoHAspanBnXQUn6iL6d92IJWhhkjJBGwFS8WBwtS+RlJitYmZ5dttcrYYMzLvbrgFiqNpH5gLpuboqheuQRvyqEAq0mrVeWBEMOWaQLucWBF39DQJw+LYai3k+smqKeIitCOKT6YGpMAaLSfzXvZu72yH6ELcYWHmKBOubKC9I8fgwSF8pTk66I8/YS6zmOIKpBa2VKRWYZwZSOwppZIlkgqmTIUoMLR1FNgrE10QK/K6nlqS+IVrG0eY1UFRZpNyikLM7dxdD+iFrsIfWoqhoINHVeAikkSPBmuxZOGBXuiaNiEXhzVOxj6g7Ds0pGMRyYeK1OHIJd0W5OG5mZ+puDAHbpVTS+WxCijCVsWWDMsllUhmIsxt24QK/D/Y3GXfyHJWUKlDeAOxnCxtR9Z1RIwEt8QdQFsoXh8cHQK8yQOZhWeqJAVX2ypTGFmS4tqRGorl5oqQbs6dYaM6wbvXKUSuFPWAeXao4BH8BLu8lXzbVeuUxVzrnQcQQtjQW2VVMrhzAch7Jtvr5LnHMpSwhYMPuWnoNtlmv1LZSdwCltvQxszWVMF3CcmOSrCfxd0y7jl677FDs+pgDl46/rAsx8MD+P+9ptTHjUuWr5Bff5AVLdWhB7/YNA7H8As8F+G6FwGDJ1QC43L0K0LelKUpCm1DDFuPRwRPRyPP6YSOVfkxxkBSFE24BVzVlQK0xtrTgZapvE4gP+Yik5mOFzXX5KRKjmNMbAC25tYynrm6wJakKnFVna0yEGYs5iE3iyLTlri03Cv3BxXCPgXKCASSLzC1WEa7VQe5rxnNkV+3isih8wD+pXOJ1X7wGZnngPqHmIunzfnDoQc/tSC3HzxCGV0q2Vy+tLzT1Qb6bKskRbxsyDQsu2zQcqOYMRJSpjUwcAuimFktblItlagFLEt4W6tb7Q9b2RQXaFsv8z/KK+T/rXeWq+5d21Vg7k11c/4/2pf7pna9Sf3AbyQbN3E/wiX9+i386y52g9ddMf57d55rlyV/90b4my31LldYW7dN/5FOKP5kReEp0v44xJmPX1KW/9UqpbOr+7W1439Ay0DnGylj7zvDzfHmO3b/wZz21s247BGt9ZYdXf7ftmYOw803gt3X/6sniA0rUzf6ve1qifrL3sE+rtj3jR/AxH/p7e91CPjnOelHfk6iHClkiLelP22v/z2Yqo987+oPXoRUqp+XUG/Hc7agKKVRkRjtICC2rndkba9AjgMN0Xfcf/XSXdh53gOeb4MkVfIPDAxh+6u39KpyYUETtsG2w67cykuqqkdlrddsfGhUC6aC4u7alirfOvz5+dGWZC9JVscvVDDb/+jm/53uFB1MX10s/9H+IopfPGip/7rD8H5s/wVSBaa/hfZd5Bxud6F+1+ol+J93o/8vMPyaV8bRa94WX2evrQz3YtXiWoBVM7DlPesbLM7cfzBMRing+fl/AwAA//9QSwcIBvp48MkPAAAZNQAAUEsBAhQAFAAIAAgAAAAAAAb6ePDJDwAAGTUAAAgAAAAAAAAAAAAAAAAAAAAAAG9yaWdpbmFsUEsFBgAAAAABAAEANgAAAP8PAAAAAA==
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-03-09T22:35:44Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k3s-w-2
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 69d86df6b4
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-2
      topology-location-root: default
    name: rook-ceph-osd-2-69d86df6b4
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-2
      uid: 25c07072-1e4b-44d0-bc38-26a56f686927
    resourceVersion: "6229167"
    uid: 855653fd-b4c2-4d67-b32d-4c364149580a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        pod-template-hash: 69d86df6b4
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: k3s-w-2
          osd: "2"
          osd-store: bluestore
          pod-template-hash: 69d86df6b4
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-2
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: d0c27c6b-8b46-4935-b66b-1b22f85feda5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=d0c27c6b-8b46-4935-b66b-1b22f85feda5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_d0c27c6b-8b46-4935-b66b-1b22f85feda5
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-09T20:50:02Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k3s-w-2
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 6d895f7fb5
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-2
      topology-location-root: default
    name: rook-ceph-osd-2-6d895f7fb5
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-2
      uid: 25c07072-1e4b-44d0-bc38-26a56f686927
    resourceVersion: "6023016"
    uid: 99dc7bcc-0331-47ba-ac6f-66ae77342d58
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        pod-template-hash: 6d895f7fb5
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: k3s-w-2
          osd: "2"
          osd-store: bluestore
          pod-template-hash: 6d895f7fb5
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-2
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: d0c27c6b-8b46-4935-b66b-1b22f85feda5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=d0c27c6b-8b46-4935-b66b-1b22f85feda5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_d0c27c6b-8b46-4935-b66b-1b22f85feda5
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
      deployment.kubernetes.io/revision-history: "1"
    creationTimestamp: "2025-03-08T15:04:06Z"
    generation: 4
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k3s-w-2
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 75cdddb768
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-2
      topology-location-root: default
    name: rook-ceph-osd-2-75cdddb768
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-2
      uid: 25c07072-1e4b-44d0-bc38-26a56f686927
    resourceVersion: "6031836"
    uid: fb20ba60-8206-4570-97e8-abdfb4670a55
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        pod-template-hash: 75cdddb768
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: k3s-w-2
          osd: "2"
          osd-store: bluestore
          pod-template-hash: 75cdddb768
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-2
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: d0c27c6b-8b46-4935-b66b-1b22f85feda5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=d0c27c6b-8b46-4935-b66b-1b22f85feda5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_d0c27c6b-8b46-4935-b66b-1b22f85feda5
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWzsOmtvIrmWf+Xc6pZI7lIQ6CSdZoRWTKB70CQhlyS9WoUsMlWnwJMqu8Z2QbO9+e+rY1dB8erO3JnVHa3mCwL7+LyfNl+9BA0LmWFe66sXswnGmr6xNPVanpLy2Q8wnflSh16VVmvP2QSVQIO6xmU9kEkqBQrjtTwCDOJMG1S6Rj9qdL7G5f6TCpnB0J8sNwmlqJiRau8ZLrRhIkCv5TX3AiRMsOlvRCpYgjn3B8VMmTK+jMpIverqiM/DnCO7MkeluRRey2tc1Jq1pn+S74xDhokU4w3wYtEsU2LDcRDinAfoBzHT2mt52i5GjMeZQj+UCeOE/vmd9hc+IaJTDiWxo41UhGsSZ+i+V71UKsMmMS1HLNa0ZEVZMztv1BqntYt8g6Qu1OaTinTKrOLLCqDv49zkW1tGpjKW06Ufy4AZLoU/k9psML0LoqQkkBAjlsXGe6l6uXE2HNEePsSSXAhUQ4xQoQhQe61Hcmb+eSVm2THr84ZX9SaxDJ4HdK6LMRoLZlSGVS+QwigZxySdW3nmgjR9iensMpd7l0mv6mXWxufBaRBMovf+6Qn74J9GzTN/EuA7P2QXzZOzD43J6fmJ9/L0UvV0igHFncI05gHTXqtR9TTGGJDbtr56CTPB7Oo78bnrkAct9EJEjWIGp0tCmLvfEF1cku4NJmlM31t/ZYk/PEv8CXPBnzjkvVKIsCjighvy2hcXoowLVHmoqyl98Xw/kgqnSmaC1OWT8qtWKb4fafsDm/jh9Jyd+yfneOGfXjQu/It3H858ZI3TCC9OG+en7qhGk2kb6Lmgdolwp+W1QGV6thKsTYK1c7GAFNFeq8H38w0/llPfSF+bEJVqU47Z2EalvrWdSOHntvgeJtp2e36qMOJf2iFOsins4ybiMbYLz/gmtS3IRPsxMiV8FobKj5RM/BRR5RBPZK0kYZRBH9eR9FT1UMyt9XLTDweDn8c3g25vfNO57nlVb87iDEuO9FLdhL28eri77w3H/W4J+HXJ9wCmLcKlvLl14HbY/9y57437twX4RyUT8tOIYxwOMVp9v2VmRgFsmMl0LZVh/9Z7edlB+PDjVf/yD8F3O+haQe5uO5evk+Z6cDPu3XRvB/2b+7ttBgIpIj69ZunPuMz5eEZKobYw7AlcchgUYSq5MHqPqJeDm4/9T+Nuf1hirj5nqh7zSZ3w7Nqnd/tTcW7wuTcc9rtlyepoAnuw7pityzkqxUOs0e8yth33+q6aUwxqQoZ4Qwj2CDN8uPvpunM7Hg4G9yWW1pntAPxPg7v77/u5Ffzz4Orhujfu9n58+FQCbxwCvPu5fzse9u7uB8Pe5eDm0JHu9bjbv+v8eNUbP3R7nw+BWbYHd9278W1vOO72Pvc3vGqTi8HNfad/QzF53flUBvs1Y0tb2TGd2Y/W3LXJ3h7Pfa1xivakJnaMQ6iue9eD4X+Or/rX/fttlAq1zFSAH0uoQz7nmvouj1r3AsJreTFPuNG1BBOplgfpDHv/eOjd/T5KCn+lgn2Y1uXtwz8jUGOvQEGa7afwh4qyQ8X66sNdbzzs3HQH1+Obwc2GS9kqtscDrU93hvfj/s19b/i5c1U6s5vT6cDDw0ZtCE+C5vvgfOJfTE7P/dMP7878yfn5xG9Mms3o4izCkJ3txbOBZU8hIoEoiVJQbytNU29ttpMn9Xu2T9qTQF0S26OzzvDTXYkRP4G3R7sMHO/w9+PV4PLn8W3n/qdy0gxxXtch25Xm8/h6sJFfFVvsVYtLBuPLq85dmS9tO7et+jbo/o7i5poFd+qxVJLyozsapBYYxdwv6gB1xSn1Zyx2I51FyRM2/XZuymFuszi+lTEPyHL96EaaW4WaBpuqF/M5CtT6VsmJnZnwi2tYS00PNTpVz+de1dO2awy8qjcSMjNpe+S9PSKa4PssTLjw3TAAdZWJNUckUa1ZY1o+g1MONEfZyck7pM/meeN45I2ECtpv/30keASP8FYF4AuEE3gCB+M+4RH+Bn4EdZOkdTsqxIgpPP0AZoZiJEYGg5mEke3UIGdmhiw2MwhmGDwDDSEYwoKbGZ2BSMaxXHAxBRIoMy1iZYXmLS2OPPgf0BiCj1DR9f+qO96hPq1Y0C/cEMMjEfGR8Ggyzied+5lCPZNx6LXOqh4NAJzFXYzZ8g4DKUKamE+qXoqKy3C1dH5S9XQWBKh1CUGj6hmeoMzMCvDdyXrycGNXkcDsoOvSpDVmmuXTVZ6YW97pJ07zSZHm1lCNMlSToGiMwSBT3CwvpTD4xRB0qvicxzjFsLhmUMjCgYiXQynNRx6jXmqDideyjXTVU5no6AeaSVondo5nymTpX473f+x475snr3c9Wtr2srOXqjeXcZbgtcyoJ6YcltDXPN1tNr+bFcF22ZRNNw5Qw5uPoBvJzybGctorXCpPe9t4Cluv8VijO+1rX8vg2Q+52mXAciyne5kgDLGcHjjEJ7l3Kab3HnUbO4dDnK+h3e2I3oUigbINUJUJ3658mx2pQ+fwzfVRFhg+ZwbdXcRT1VtI9czFtMvVjg5sQRHcXG7eTJSCsT7hoj5h5UDUaMCXgEqRJ65+pzxF8sPVgpCZ/frGxgDwCOZMcTaJUQNTCHZ3Bf3FKBbgSIyE610o1EutDEWJbRg+3vW77dfchzhE1Ey1X9NCOXA7fow/XnU+tUee769uooi8bR469x0aAtsH7DDy3pb4dT1JW7HFSLi2YyXVurux8S/e5DnEDn8jD5JMG5ixOdqkMfIizcORB9NYTlgMLmQyZa9vwEgojA4oArVMDYYwuOtqwssig2or9VxiOqtouB0C15CgmmJYI9iZMalu1etTbmbZpBbIZJ1U6+QV3NSbZ+dnZ3jGLi6aH/DkpMHC6PT8/Vmjwc4nIV40TpuTE3b2/ozw3c+4BluQHKcLHscwQVCYyDmGEDODqgadnEVmqrBACJjIIcDMuBWCxAcufsHA4glkiDWgIsIm8RIYTKUMgVIYCbSYoVVKjuMfGRfBEnSWplKZQ3LaSdx+pFkc1xsn7969exNyHWRacynG6uJD86Lx/uL9+5EIUvB9If2UGio1x3YiQwSb3wwmqb/SmbUmrBLfem0k0qWZSfEO/ADIA3hCzOWGTZnSqMgt3G9ob2zULu2PW/vj6LiAqlHaPKrsoVY5Jlw8AiELErUZ02Pt1HlUcW5VOW6NBADkII/F8hO04evLfhSuQ11hqFbIUgcRPbptQlh5uwpnqmrCFkiZotgvQRUqi8oxMJ3jjHiMG0RqC8UNHq13SeY8tNzNPAhcwDMuFRfTkXBdhIAg5ihMzXYTwDIzgykaXyo/P0StRCmqgQp8hVEgQaokESKQCiRTtXdd6rBY/3sF/OeSN7gOJufI3X7XV/zZ0KWodhmA0FgV2TTjCrPrXR6pbchTzciDdhtGXjxPRh48ldqU++tbm7feHiXP5KPgh9YpRubNOnlIHdJKiQTE82S9b53e9Xch5EpZZ8yRB2VF5d8pATs7EKlApksrj0nSSEPIlX01WtocBsSYVEwt1xvuGKUBimyBGGIIEwxYpjEP9BkC1TBYXa8D1SVdLdGZSqo5C7YEJkLgxnqRQWGcTqmKrcjYHKVQZ7EB7tBjkpplyBVxOcl/OuLM9uvEkhMrYVxYW62YsQqlfDFHNZE6V2KIqnj3KxRVlJaRV/87reUWG3n1QnmZsJ1ASS7NCcECIZSiYqx6gBtgYplQ0RIjk7kjOzQ2DDJhwbPFWmhFRiUiXBjpfFDxKRcstgJuWOi7Aq6F+fthXhQaq74FNSIznhIbFEgacgZszGYaVd2+aFjKM7kQG8QVDS2a4obAW/bMN0jmZQYPOZ9KNnDax5otmUYCY40OYaUcPIotIObaVOBoMePBrIhkKhwptUC/yAnJo121shWAyjZIAUKGqI8dUtJMpoS2svuR4ijCeAmunwTq+6joEQEqncGMiWkeHC5YNSicSGlqcC8dxmeaYgZ3XUhlqCFSMgHbwVJzoDC1D5lUWG1hZvlxW5ytjxmZTzfcOorDad+mC6YW6LoXrkEbiqhAKtJq1UVgRG7KNTntamON3OHTJAyLY6u1kOtnq6aIi9CuKD6dGZACawRO5r3q393bCdGluOPCzFEmXNtAZ8eOwaNj+Ep7ROiXX2Ehs5jyCqTWbalJrcIkM5BYKqWWJZIKZkyFKDC0fRTY2xZdICvqup5ZlPiFa5tHmNVB0WaTcopGzJ3c3w/opa7CL1qKkSDC4ypwEUnCR4u1WLLwSC91TZuQi+MaJ2MfUfUdGdKxiORjRepw7IpuG/L03MppKi7MkYNyaqk8VQFF2K7YlmEFUolkJsLcti2owL/B9in7vJazgkodwxuI5XRlO7KuQ2IkOBBHgI5Qvj46OQZ4kycy656pkpRc7ahMaWSFimuHaiRWhytCuj1Hw2Z1cu9+txC5UvQD5sV5BY/gb7AvWim2XbdOVcyN3nkCIQ8beeuqSgZ3IQD5zGRnnbz2WIYStnTuQ2EaukN22K9U9iK3DsttamNmZ6vgu+TJDktwmMX9Mh4gfZDsyKwnmKPvkT8uy/H4CP5/byj1aeuS5Svk1x9kRYt15MEPtoxDA36AlyJd9yNg8IxKYFzOfkXKm7E0RaFtimGb+Yjc0+HIazp5x7r9mGAgKYsm3Dpc1bUCBGPt6ZyWqbxPoDjmIpOZjpc1N+SkSk5iTGwCtubWMp67vsC2pCpxXZ3tMhDmLOYhN8ui0pa4tNwr998W8n1KlBEIpFhgarnKdusJ8tAwmnt+3Soid51H8K9cSKzPg8/IPEc0P8RcPG/vH488+FsbcvvBE5S9SyXb4CvLO11teZ8dlaSIVwOZhtWUDVpuNTNGQsq0BgYOIIqZ1eI21lKLWrhlyd82+lb7w3Y2xQXazqP+n+UB8//XE811777jOjD3HLu9/x+dq0Nb+56z/sTPK1s3cf/yG/rNK/jX3eoGr7tf/OcuPDduSn7vdfA35+l9cbABt4v/iSgUf86i3BRpfxLi3McvKcv/opUS7ephbe35/9Aqy/lGytj7g33N8eY7dv9yuA0brqZDa7rVLJf/Ia6V++D268D+i//148OWiWkO/aONapH6q6nBPqvYl41/tX1/05Pf68z/1yvSn/kViUqjkCHelf7mvfmHYmo68rPrv4SRm1LbvPLzTrxgS8pPGhWJ0QkCYutmT7H2Cs9xTkP4HfdfvXSf77wccJ5vO0mq5C8YGPLtr94qpMr9BG3Yudouuy4r76SqHnWzXqv5oVktmAqKK2vboXyL+MvLk+3EvidZHb9Qn2z/1Zv/27pbDC4Ddbn6D/x3vfi7hFb6rzsfPuzbvwFV4dPf8vZ96Jzf7vP6fdAr53/Z7/2/geHXPC6OX/Ok+Dp77ZS37/YrrvNfzwA70bN5wPqZ++uFySj/v7z8bwAAAP//UEsHCNtMVkTZDwAASzUAAFBLAQIUABQACAAIAAAAAADbTFZE2Q8AAEs1AAAIAAAAAAAAAAAAAAAAAAAAAABvcmlnaW5hbFBLBQYAAAAAAQABADYAAAAPEAAAAAA=
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-03-16T18:48:47Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "2"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "2"
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      device-class: ssd
      failure-domain: k3s-w-2
      osd: "2"
      osd-store: bluestore
      pod-template-hash: 7fc99fdf85
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-2
      topology-location-root: default
    name: rook-ceph-osd-2-7fc99fdf85
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-2
      uid: 25c07072-1e4b-44d0-bc38-26a56f686927
    resourceVersion: "9908646"
    uid: 1808b3f1-006a-46f3-96c6-c07fea5fd5b4
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        pod-template-hash: 7fc99fdf85
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "2"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "2"
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          device-class: ssd
          failure-domain: k3s-w-2
          osd: "2"
          osd-store: bluestore
          pod-template-hash: 7fc99fdf85
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-2
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-2
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: d0c27c6b-8b46-4935-b66b-1b22f85feda5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: ssd
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.2.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=d0c27c6b-8b46-4935-b66b-1b22f85feda5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "2"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-2
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_d0c27c6b-8b46-4935-b66b-1b22f85feda5
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-17T20:18:49Z"
    generation: 2
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "3"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "3"
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      device-class: nvme
      failure-domain: k3s-w-3
      osd: "3"
      osd-store: bluestore
      pod-template-hash: 787ccc5f45
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-3
      topology-location-root: default
    name: rook-ceph-osd-3-787ccc5f45
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-3
      uid: 527b36fa-aab5-417c-857e-1d8e59d2b011
    resourceVersion: "9908837"
    uid: 5fee994a-4e8a-4f34-a0cb-4750a0c44c22
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "3"
        pod-template-hash: 787ccc5f45
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "3"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "3"
          ceph_daemon_id: "3"
          ceph_daemon_type: osd
          device-class: nvme
          failure-domain: k3s-w-3
          osd: "3"
          osd-store: bluestore
          pod-template-hash: 787ccc5f45
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-3
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "3"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-3
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-3
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-3
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 9c8eabc6-e7b1-4837-8d10-9d1b9e107222
          - name: ROOK_OSD_ID
            value: "3"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/nvme0n1p3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: nvme
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=9c8eabc6-e7b1-4837-8d10-9d1b9e107222\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/nvme0n1p3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "3"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-3
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-3
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_9c8eabc6-e7b1-4837-8d10-9d1b9e107222
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: 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
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-29T17:16:06Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      app.kubernetes.io/component: cephclusters.ceph.rook.io
      app.kubernetes.io/created-by: rook-ceph-operator
      app.kubernetes.io/instance: "3"
      app.kubernetes.io/managed-by: rook-ceph-operator
      app.kubernetes.io/name: ceph-osd
      app.kubernetes.io/part-of: rook-ceph
      ceph-osd-id: "3"
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      device-class: nvme
      failure-domain: k3s-w-3
      osd: "3"
      osd-store: bluestore
      pod-template-hash: c984f9545
      portable: "false"
      rook.io/operator-namespace: rook-ceph
      rook_cluster: rook-ceph
      topology-location-host: k3s-w-3
      topology-location-root: default
    name: rook-ceph-osd-3-c984f9545
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-3
      uid: 527b36fa-aab5-417c-857e-1d8e59d2b011
    resourceVersion: "11084869"
    uid: 1e9e668a-08ea-44a3-9e2a-ca9e32128b98
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "3"
        pod-template-hash: c984f9545
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          app.kubernetes.io/component: cephclusters.ceph.rook.io
          app.kubernetes.io/created-by: rook-ceph-operator
          app.kubernetes.io/instance: "3"
          app.kubernetes.io/managed-by: rook-ceph-operator
          app.kubernetes.io/name: ceph-osd
          app.kubernetes.io/part-of: rook-ceph
          ceph-osd-id: "3"
          ceph_daemon_id: "3"
          ceph_daemon_type: osd
          device-class: nvme
          failure-domain: k3s-w-3
          osd: "3"
          osd-store: bluestore
          pod-template-hash: c984f9545
          portable: "false"
          rook.io/operator-namespace: rook-ceph
          rook_cluster: rook-ceph
          topology-location-host: k3s-w-3
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "3"
          - --fsid
          - e2e946a6-06e8-4818-8395-ea14fe84164d
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=k3s-w-3
          - --default-log-to-stderr=true
          - --default-err-to-stderr=true
          - --default-mon-cluster-log-to-stderr=true
          - '--default-log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-3
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-3
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: CONTAINER_IMAGE
            value: quay.io/ceph/ceph:v18.2.2
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: CEPH_USE_RANDOM_NONCE
            value: "true"
          - name: ROOK_OSD_RESTART_INTERVAL
            value: "0"
          - name: ROOK_OSD_UUID
            value: 9c8eabc6-e7b1-4837-8d10-9d1b9e107222
          - name: ROOK_OSD_ID
            value: "3"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/nvme0n1p3
          - name: ROOK_CV_MODE
            value: raw
          - name: ROOK_OSD_DEVICE_CLASS
            value: nvme
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 5
            initialDelaySeconds: 10
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: osd
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          startupProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.3.asok status 2>&1)\"\nrc=$?\nif
                [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon
                health check failed with the following output:\"\n\techo \"$outp\"
                | sed -e 's/^/> /g'\n\texit $rc\nfi\n"
            failureThreshold: 720
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables
            are unset\nset -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=e2e946a6-06e8-4818-8395-ea14fe84164d\nOSD_UUID=9c8eabc6-e7b1-4837-8d10-9d1b9e107222\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
            \"ceph.conf\" must have the \"fsid\" global configuration to activate
            encrypted OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
            This limitation will be removed later. After that, we can remove this\n#
            fsid injection code. Probably a good time is when to remove Quincy support.\n#
            https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp --no-preserve=mode
            /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c \"\nimport configparser\n\nconfig
            = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
            not config.has_section('global'):\n    config['global'] = {}\n\nif not
            config.has_option('global','fsid'):\n    config['global']['fsid'] = '$CEPH_FSID'\n\nwith
            open('/etc/ceph/ceph.conf', 'w') as configfile:\n    config.write(configfile)\n\"\n\n#
            create new keyring\nceph -n client.admin auth get-or-create osd.\"$OSD_ID\"
            mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
            \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary
            directory\n\t# this is needed because when the init container exits, the
            tmpfs goes away and its content with it\n\t# this will result in the emptydir
            to be empty when accessed by the main osd container\n\tcp --verbose --no-dereference
            \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't
            need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back the content
            of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
            \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the
            ceph user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
            remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\t#
            'ceph-volume raw list' (which the osd-prepare job uses to report OSDs
            on nodes)\n\t#  returns user-friendly device names which can change when
            systems reboot. To\n\t# keep OSD pods from crashing repeatedly after a
            reboot, we need to check if the\n\t# block device we have is still correct,
            and if it isn't correct, we need to\n\t# scan all the disks to find the
            right one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t#
            jq would be preferable, but might be removed for hardened Ceph images\n\t\t#
            python3 should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport
            sys, json\nfor _, info in json.load(sys.stdin).items():\n\tif info['osd_id']
            == $OSD_ID:\n\t\tprint(info['device'], end='')\n\t\tprint('found device:
            ' + info['device'], file=sys.stderr) # log the disk we found to stderr\n\t\tsys.exit(0)
            \ # don't keep processing once the disk is found\nsys.exit('no disk found
            with OSD ID $OSD_ID')\n\"\n\t}\n\n\tif ! ceph-volume raw list \"$DEVICE\"
            > \"$OSD_LIST\"; then\n\t\t# if the command fails, the disk may be renamed\n\t\techo
            '' > \"$OSD_LIST\"\n\tfi\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device <
            \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat
            \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device < \"$OSD_LIST\")\"\n\tfi\n\t[[
            -z \"$DEVICE\" ]] && { echo \"no device\" ; exit 1 ; }\n\n\t# If a kernel
            device name change happens and a block device file\n\t# in the OSD directory
            becomes missing, this OSD fails to start\n\t# continuously. This problem
            can be resolved by confirming\n\t# the validity of the device file and
            recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
            [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ;
            then\n\t\trm $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports
            bluestore so we don't need to pass a store flag\n\tceph-volume raw activate
            --device \"$DEVICE\" --no-systemd --no-tmpfs\nfi\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/nvme0n1p3
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          - name: ROOK_OSD_ID
            value: "3"
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: activate
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/temp-ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/admin-keyring-store/
            name: rook-ceph-admin-keyring
            readOnly: true
        - args:
          - bluefs-bdev-expand
          - --path
          - /var/lib/ceph/osd/ceph-3
          command:
          - ceph-bluestore-tool
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: expand-bluefs
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /run/ceph
          command:
          - chown
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources:
            limits:
              cpu: "3"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: k3s-w-3
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/e2e946a6-06e8-4818-8395-ea14fe84164d_9c8eabc6-e7b1-4837-8d10-9d1b9e107222
            type: DirectoryOrCreate
          name: activate-osd
        - name: rook-ceph-admin-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-admin-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-01T20:51:38Z"
    generation: 2
    labels:
      app: rook-ceph-tools
      pod-template-hash: 5d4d797ff9
    name: rook-ceph-tools-5d4d797ff9
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "4566692"
    uid: b0dd0225-4ad6-4d40-83fb-2cd2185fd721
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 5d4d797ff9
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 5d4d797ff9
      spec:
        containers:
        - args:
          - -m
          - -c
          - while true; do sleep 5; done
          command:
          - /bin/bash
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            optional: false
            secretName: rook-ceph-mon
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
    creationTimestamp: "2025-03-08T13:54:40Z"
    generation: 2
    labels:
      app: rook-ceph-tools
      pod-template-hash: 5f5574f766
    name: rook-ceph-tools-5f5574f766
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "5744467"
    uid: a1826a3d-e266-4287-af9e-067e9efc6e0e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 5f5574f766
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 5f5574f766
      spec:
        containers:
        - args:
          - -c
          - "# Wait for the Ceph configuration to be available\nwhile [ ! -f /etc/ceph/ceph.conf
            ]; do\n  echo 'Waiting for Ceph configuration to be available...'\n  \n
            \ # Get monitor endpoints\n  MONS=$(grep -v '^#' /etc/rook/mon-endpoints
            | tr ',' ' ' | sed 's/=/ /g' | awk '{print $2}' | tr '\\n' ',' | sed 's/,$//')\n
            \ \n  # Create initial Ceph configuration\n  if [ ! -z \"$MONS\" ]; then\n
            \   echo '[global]' > /etc/ceph/ceph.conf\n    echo 'mon_host = '$MONS
            >> /etc/ceph/ceph.conf\n    echo 'auth_cluster_required = cephx' >> /etc/ceph/ceph.conf\n
            \   echo 'auth_service_required = cephx' >> /etc/ceph/ceph.conf\n    echo
            'auth_client_required = cephx' >> /etc/ceph/ceph.conf\n    \n    # Create
            keyring file\n    SECRET=$(cat /etc/ceph/secret)\n    echo '[client.admin]'
            > /etc/ceph/ceph.client.admin.keyring\n    echo '  key = '$SECRET >> /etc/ceph/ceph.client.admin.keyring\n
            \ fi\n  \n  sleep 5\ndone\n\necho 'Ceph configuration is available.'\n\n#
            Keep the container running\nwhile true; do\n  sleep 5\ndone"
          command:
          - /bin/bash
          - -c
          - |
            # Replicate the script from toolbox.sh inline so the ceph image
            # can be run directly, instead of requiring the rook toolbox
            CEPH_CONFIG="/etc/ceph/ceph.conf"
            MON_CONFIG="/etc/rook/mon-endpoints"
            KEYRING_FILE="/etc/ceph/keyring"

            # create a ceph config file in its default location so ceph/rados tools can be used
            # without specifying any arguments
            write_endpoints() {
              endpoints=$(cat ${MON_CONFIG})

              # filter out the mon names
              # external cluster can have numbers or hyphens in mon names, handling them in regex
              # shellcheck disable=SC2001
              mon_endpoints=$(echo "${endpoints}"| sed 's/[a-z0-9_-]\+=//g')

              DATE=$(date)
              echo "$DATE writing mon endpoints to ${CEPH_CONFIG}: ${endpoints}"
                cat <<EOF > ${CEPH_CONFIG}
            [global]
            mon_host = ${mon_endpoints}

            [client.admin]
            keyring = ${KEYRING_FILE}
            EOF
            }

            # watch the endpoints config file and update if the mon endpoints ever change
            watch_endpoints() {
              # get the timestamp for the target of the soft link
              real_path=$(realpath ${MON_CONFIG})
              initial_time=$(stat -c %Z "${real_path}")
              while true; do
                real_path=$(realpath ${MON_CONFIG})
                latest_time=$(stat -c %Z "${real_path}")

                if [[ "${latest_time}" != "${initial_time}" ]]; then
                  write_endpoints
                  initial_time=${latest_time}
                fi

                sleep 10
              done
            }

            # read the secret from an env var (for backward compatibility), or from the secret file
            ceph_secret=${ROOK_CEPH_SECRET}
            if [[ "$ceph_secret" == "" ]]; then
              ceph_secret=$(cat /var/lib/rook-ceph-mon/secret.keyring)
            fi

            # create the keyring file
            cat <<EOF > ${KEYRING_FILE}
            [${ROOK_CEPH_USERNAME}]
            key = ${ceph_secret}
            EOF

            # write the initial config file
            write_endpoints

            # continuously update the mon endpoints if they fail over
            watch_endpoints
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /etc/ceph/secret
            name: ceph-secret
            subPath: secret
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            optional: false
            secretName: rook-ceph-mon
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
        - name: ceph-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret
            secretName: rook-ceph-mon
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-03-01T21:42:57Z"
    generation: 3
    labels:
      app: rook-ceph-tools
      pod-template-hash: 6689667d48
    name: rook-ceph-tools-6689667d48
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "4572603"
    uid: b55642ae-3bd6-4181-aef6-5b15ec59520e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 6689667d48
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 6689667d48
      spec:
        containers:
        - args:
          - -m
          - -c
          - while true; do sleep 5; done
          command:
          - /bin/bash
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /etc/ceph/ceph.client.admin.keyring
            name: ceph-admin-creds
            subPath: keyring
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
        - name: ceph-admin-creds
          secret:
            defaultMode: 420
            items:
            - key: ceph-username
              path: username
            - key: ceph-secret
              path: keyring
            secretName: rook-ceph-mon
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-08T22:20:42Z"
    generation: 2
    labels:
      app: rook-ceph-tools
      pod-template-hash: 68c4dcc67f
    name: rook-ceph-tools-68c4dcc67f
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "4563635"
    uid: 7dab1e87-6d30-4f57-b10f-cbea240ed111
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 68c4dcc67f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 68c4dcc67f
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - |
            # Replicate the script from toolbox.sh inline so the ceph image
            # can be run directly, instead of requiring the rook toolbox
            CEPH_CONFIG="/etc/ceph/ceph.conf"
            MON_CONFIG="/etc/rook/mon-endpoints"
            KEYRING_FILE="/etc/ceph/keyring"

            # create a ceph config file in its default location so ceph/rados tools can be used
            # without specifying any arguments
            write_endpoints() {
              endpoints=$(cat ${MON_CONFIG})

              # filter out the mon names
              # external cluster can have numbers or hyphens in mon names, handling them in regex
              # shellcheck disable=SC2001
              mon_endpoints=$(echo "${endpoints}"| sed 's/[a-z0-9_-]\+=//g')

              DATE=$(date)
              echo "$DATE writing mon endpoints to ${CEPH_CONFIG}: ${endpoints}"
                cat <<EOF > ${CEPH_CONFIG}
            [global]
            mon_host = ${mon_endpoints}

            [client.admin]
            keyring = ${KEYRING_FILE}
            EOF
            }

            # watch the endpoints config file and update if the mon endpoints ever change
            watch_endpoints() {
              # get the timestamp for the target of the soft link
              real_path=$(realpath ${MON_CONFIG})
              initial_time=$(stat -c %Z "${real_path}")
              while true; do
                real_path=$(realpath ${MON_CONFIG})
                latest_time=$(stat -c %Z "${real_path}")

                if [[ "${latest_time}" != "${initial_time}" ]]; then
                  write_endpoints
                  initial_time=${latest_time}
                fi

                sleep 10
              done
            }

            # read the secret from an env var (for backward compatibility), or from the secret file
            ceph_secret=${ROOK_CEPH_SECRET}
            if [[ "$ceph_secret" == "" ]]; then
              ceph_secret=$(cat /var/lib/rook-ceph-mon/secret.keyring)
            fi

            # create the keyring file
            cat <<EOF > ${KEYRING_FILE}
            [${ROOK_CEPH_USERNAME}]
            key = ${ceph_secret}
            EOF

            # write the initial config file
            write_endpoints

            # continuously update the mon endpoints if they fail over
            watch_endpoints
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 2016
            runAsNonRoot: true
            runAsUser: 2016
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          tty: true
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-default
        serviceAccountName: rook-ceph-default
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            optional: false
            secretName: rook-ceph-mon
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-03-01T21:45:34Z"
    generation: 2
    labels:
      app: rook-ceph-tools
      pod-template-hash: 759769c46b
    name: rook-ceph-tools-759769c46b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "4572564"
    uid: a0a77215-9e40-405c-b1fe-49d49cf3c91a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 759769c46b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 759769c46b
      spec:
        containers:
        - args:
          - -c
          - " # Wait for the Ceph configuration to be available while [ ! -f /etc/ceph/ceph.conf
            ]; do echo 'Waiting for Ceph configuration to be available...'\n# Get
            monitor endpoints MONS=$(grep -v '^#' /etc/rook/mon-endpoints | sed 's/[,]/\\n/g'
            | grep ':')\n# Create initial Ceph configuration if [ ! -z \"$MONS\" ];
            then echo '[global]' > /etc/ceph/ceph.conf echo 'mon_host = '$MONS >>
            /etc/ceph/ceph.conf echo 'auth_cluster_required = cephx' >> /etc/ceph/ceph.conf
            echo 'auth_service_required = cephx' >> /etc/ceph/ceph.conf echo 'auth_client_required
            = cephx' >> /etc/ceph/ceph.conf\n# Create keyring file SECRET=$(cat /etc/ceph/secret)
            echo '[client.admin]' > /etc/ceph/ceph.client.admin.keyring echo '  key
            = '$SECRET >> /etc/ceph/ceph.client.admin.keyring fi\nsleep 5 done\necho
            'Ceph configuration is available.'\n# Keep the container running while
            true; do sleep 5; done "
          command:
          - /bin/bash
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /etc/ceph/secret
            name: ceph-secret
            subPath: secret
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
        - name: ceph-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret
            secretName: rook-ceph-mon
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-03-01T21:48:23Z"
    generation: 3
    labels:
      app: rook-ceph-tools
      pod-template-hash: 764c97bd99
    name: rook-ceph-tools-764c97bd99
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "4573584"
    uid: c2c296ef-4c57-4707-bdb9-74878e91f1a5
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 764c97bd99
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 764c97bd99
      spec:
        containers:
        - args:
          - -c
          - "# Wait for the Ceph configuration to be available\nwhile [ ! -f /etc/ceph/ceph.conf
            ]; do\n  echo 'Waiting for Ceph configuration to be available...'\n  \n
            \ # Get monitor endpoints\n  MONS=$(grep -v '^#' /etc/rook/mon-endpoints
            | sed 's/[,]/\\n/g' | grep ':')\n  \n  # Create initial Ceph configuration\n
            \ if [ ! -z \"$MONS\" ]; then\n    echo '[global]' > /etc/ceph/ceph.conf\n
            \   echo 'mon_host = '$MONS >> /etc/ceph/ceph.conf\n    echo 'auth_cluster_required
            = cephx' >> /etc/ceph/ceph.conf\n    echo 'auth_service_required = cephx'
            >> /etc/ceph/ceph.conf\n    echo 'auth_client_required = cephx' >> /etc/ceph/ceph.conf\n
            \   \n    # Create keyring file\n    SECRET=$(cat /etc/ceph/secret)\n
            \   echo '[client.admin]' > /etc/ceph/ceph.client.admin.keyring\n    echo
            '  key = '$SECRET >> /etc/ceph/ceph.client.admin.keyring\n  fi\n  \n  sleep
            5\ndone\n\necho 'Ceph configuration is available.'\n\n# Keep the container
            running\nwhile true; do\n  sleep 5\ndone"
          command:
          - /bin/bash
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /etc/ceph/secret
            name: ceph-secret
            subPath: secret
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
        - name: ceph-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret
            secretName: rook-ceph-mon
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-03-01T21:39:53Z"
    generation: 2
    labels:
      app: rook-ceph-tools
      pod-template-hash: 86bf8bd674
    name: rook-ceph-tools-86bf8bd674
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "4571609"
    uid: 63bf9df6-a504-444a-8909-735990ca0654
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 86bf8bd674
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 86bf8bd674
      spec:
        containers:
        - args:
          - -m
          - -c
          - while true; do sleep 5; done
          command:
          - /bin/bash
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /etc/ceph/keyring
            name: ceph-admin-keyring
            subPath: keyring
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
        - name: ceph-admin-keyring
          secret:
            defaultMode: 420
            items:
            - key: admin-keyring
              path: keyring
            secretName: rook-ceph-mon
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
      deployment.kubernetes.io/revision-history: "8"
    creationTimestamp: "2025-03-01T21:54:42Z"
    generation: 3
    labels:
      app: rook-ceph-tools
      pod-template-hash: 95c4b57b6
    name: rook-ceph-tools-95c4b57b6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "5852133"
    uid: 8505abe8-fee5-4c63-be11-2d28708e5956
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 95c4b57b6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 95c4b57b6
      spec:
        containers:
        - args:
          - -c
          - "# Wait for the Ceph configuration to be available\nwhile [ ! -f /etc/ceph/ceph.conf
            ]; do\n  echo 'Waiting for Ceph configuration to be available...'\n  \n
            \ # Get monitor endpoints\n  MONS=$(grep -v '^#' /etc/rook/mon-endpoints
            | tr ',' ' ' | sed 's/=/ /g' | awk '{print $2}' | tr '\\n' ',' | sed 's/,$//')\n
            \ \n  # Create initial Ceph configuration\n  if [ ! -z \"$MONS\" ]; then\n
            \   echo '[global]' > /etc/ceph/ceph.conf\n    echo 'mon_host = '$MONS
            >> /etc/ceph/ceph.conf\n    echo 'auth_cluster_required = cephx' >> /etc/ceph/ceph.conf\n
            \   echo 'auth_service_required = cephx' >> /etc/ceph/ceph.conf\n    echo
            'auth_client_required = cephx' >> /etc/ceph/ceph.conf\n    \n    # Create
            keyring file\n    SECRET=$(cat /etc/ceph/secret)\n    echo '[client.admin]'
            > /etc/ceph/ceph.client.admin.keyring\n    echo '  key = '$SECRET >> /etc/ceph/ceph.client.admin.keyring\n
            \ fi\n  \n  sleep 5\ndone\n\necho 'Ceph configuration is available.'\n\n#
            Keep the container running\nwhile true; do\n  sleep 5\ndone"
          command:
          - /bin/bash
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
          - mountPath: /etc/ceph/secret
            name: ceph-secret
            subPath: secret
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
        - name: ceph-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret
            secretName: rook-ceph-mon
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-03-01T21:11:41Z"
    generation: 2
    labels:
      app: rook-ceph-tools
      pod-template-hash: 9c4cfbfc
    name: rook-ceph-tools-9c4cfbfc
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 77a6b9e3-23d3-4406-8001-396f69e6c9ef
    resourceVersion: "4571645"
    uid: 2946acc0-610b-43a3-836f-28bb2296f04d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 9c4cfbfc
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 9c4cfbfc
      spec:
        containers:
        - args:
          - -m
          - -c
          - while true; do sleep 5; done
          command:
          - /bin/bash
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-03-30T20:28:26Z"
    generation: 2
    labels:
      app: uptime-kuma
      pod-template-hash: 57f469f7c
    name: uptime-kuma-57f469f7c
    namespace: uptime-kuma
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: uptime-kuma
      uid: b105299d-92dd-4ef7-a948-86bd5978b397
    resourceVersion: "10629377"
    uid: ed58522b-bd7c-43b3-aab5-441cb069fb6a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: uptime-kuma
        pod-template-hash: 57f469f7c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: uptime-kuma
          pod-template-hash: 57f469f7c
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        containers:
        - env:
          - name: TZ
            value: Europe/Stockholm
          image: louislam/uptime-kuma:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: uptime-kuma
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 750m
              memory: 384Mi
            requests:
              cpu: 200m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: uptime-kuma-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-03-30T12:30:13Z"
    generation: 2
    labels:
      app: uptime-kuma
      pod-template-hash: 5fbb5f9ff
    name: uptime-kuma-5fbb5f9ff
    namespace: uptime-kuma
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: uptime-kuma
      uid: b105299d-92dd-4ef7-a948-86bd5978b397
    resourceVersion: "10112909"
    uid: a5075509-9a7e-478e-a66f-ec847ca8059b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: uptime-kuma
        pod-template-hash: 5fbb5f9ff
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: uptime-kuma
          pod-template-hash: 5fbb5f9ff
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        containers:
        - env:
          - name: TZ
            value: Europe/Stockholm
          image: louislam/uptime-kuma:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: uptime-kuma
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 384Mi
            requests:
              cpu: 150m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: uptime-kuma-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-29T14:40:31Z"
    generation: 2
    labels:
      app: uptime-kuma
      pod-template-hash: 7f6df55f88
    name: uptime-kuma-7f6df55f88
    namespace: uptime-kuma
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: uptime-kuma
      uid: b105299d-92dd-4ef7-a948-86bd5978b397
    resourceVersion: "9888763"
    uid: 51a77e1f-94ca-4f0d-b3b7-573a820dc8a9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: uptime-kuma
        pod-template-hash: 7f6df55f88
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: uptime-kuma
          pod-template-hash: 7f6df55f88
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        containers:
        - env:
          - name: TZ
            value: Europe/Stockholm
          image: louislam/uptime-kuma:1.23.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: uptime-kuma
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 300m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: uptime-kuma-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-04-02T19:37:24Z"
    generation: 1
    labels:
      app: uptime-kuma
      pod-template-hash: 844847fb6b
    name: uptime-kuma-844847fb6b
    namespace: uptime-kuma
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: uptime-kuma
      uid: b105299d-92dd-4ef7-a948-86bd5978b397
    resourceVersion: "11100223"
    uid: 996acf53-0fa7-4f29-b0e4-cf1f27b9f337
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: uptime-kuma
        pod-template-hash: 844847fb6b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: uptime-kuma
          pod-template-hash: 844847fb6b
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        containers:
        - env:
          - name: TZ
            value: Europe/Stockholm
          image: louislam/uptime-kuma:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: uptime-kuma
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 384Mi
            requests:
              cpu: 400m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: uptime-kuma-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-03-29T14:42:28Z"
    generation: 2
    labels:
      app: uptime-kuma
      pod-template-hash: 8689b749bf
    name: uptime-kuma-8689b749bf
    namespace: uptime-kuma
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: uptime-kuma
      uid: b105299d-92dd-4ef7-a948-86bd5978b397
    resourceVersion: "10083576"
    uid: a27a4b1e-db2f-4ceb-867b-68ef499bc671
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: uptime-kuma
        pod-template-hash: 8689b749bf
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: uptime-kuma
          pod-template-hash: 8689b749bf
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        containers:
        - env:
          - name: TZ
            value: Europe/Stockholm
          image: louislam/uptime-kuma:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: uptime-kuma
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 300m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: uptime-kuma-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    creationTimestamp: "2025-02-15T13:41:52Z"
    generation: 25
    labels:
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
      name: bitwarden
    name: bitwarden
    namespace: bitwarden
    resourceVersion: "9322491"
    uid: f6300354-5f9b-4cae-8cfb-abc1f678f8db
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: bitwarden
    serviceName: bitwarden
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: bitwarden
      spec:
        containers:
        - env:
          - name: ROCKET_PORT
            value: "80"
          - name: ADMIN_TOKEN
            valueFrom:
              secretKeyRef:
                key: token
                name: bitwarden-admin-token
          - name: SMTP_USERNAME
            valueFrom:
              secretKeyRef:
                key: emailUser
                name: bitwarden-smtp
          - name: SMTP_PASSWORD
            valueFrom:
              secretKeyRef:
                key: emailPassword
                name: bitwarden-smtp
          envFrom:
          - configMapRef:
              name: bitwarden
          image: vaultwarden/server:1.33.0
          imagePullPolicy: IfNotPresent
          name: bitwarden
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 3012
            name: websocket
            protocol: TCP
          resources:
            limits:
              cpu: 300m
              memory: 1Gi
            requests:
              cpu: 50m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: bitwarden-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: bitwarden
        serviceAccountName: bitwarden
        terminationGracePeriodSeconds: 30
        volumes:
        - name: bitwarden-data
          persistentVolumeClaim:
            claimName: bitwarden-data-claim
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: bitwarden-7fd6fd9558
    observedGeneration: 25
    readyReplicas: 1
    replicas: 1
    updateRevision: bitwarden-7fd6fd9558
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: loki
    creationTimestamp: "2025-03-30T10:32:05Z"
    generation: 1
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      helm.toolkit.fluxcd.io/name: loki
      helm.toolkit.fluxcd.io/namespace: loki
      heritage: Helm
      release: loki
    name: loki
    namespace: loki
    resourceVersion: "11084826"
    uid: f291132c-427c-432b-ab51-c8b0a33ef2e4
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: loki
        release: loki
    serviceName: loki-headless
    template:
      metadata:
        annotations:
          checksum/config: f7c516d64b976f04d4c31a243b6c2091ced173f942f6315d3ee2ba37ca7c0bd9
          prometheus.io/port: http-metrics
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: loki
          name: loki
          release: loki
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        containers:
        - args:
          - -config.file=/etc/loki/loki.yaml
          image: grafana/loki:2.6.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: memberlist-port
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 1500m
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /etc/loki
            name: config
          - mountPath: /data
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 4800
        volumes:
        - emptyDir: {}
          name: tmp
        - name: config
          secret:
            defaultMode: 420
            secretName: loki
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: rook-ceph-block
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: loki-845bb44c7d
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: loki-845bb44c7d
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "3753659855849333265"
    creationTimestamp: "2025-03-10T19:52:00Z"
    generation: 2
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
      release: kube-prometheus-stack
    name: alertmanager-kube-prometheus-stack-alertmanager
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: kube-prometheus-stack-alertmanager
      uid: 8537e4f7-60d8-4ff9-a3a6-1c60a113c611
    resourceVersion: "11084801"
    uid: 30417bf4-383e-43b3-81e6-e4b604b15655
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: kube-prometheus-stack-alertmanager
        app.kubernetes.io/instance: kube-prometheus-stack-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
          kubectl.kubernetes.io/restartedAt: "2025-04-03T22:46:15+02:00"
        creationTimestamp: null
        labels:
          alertmanager: kube-prometheus-stack-alertmanager
          app.kubernetes.io/instance: kube-prometheus-stack-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.26.0
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
              weight: 100
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=120h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=http://alertmanager.local
          - --web.route-prefix=/
          - --cluster.label=monitoring/kube-prometheus-stack-alertmanager
          - --cluster.peer=alertmanager-kube-prometheus-stack-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-kube-prometheus-stack-alertmanager-db
            subPath: alertmanager-db
          - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
            name: secret-alertmanager-notification-secret
            readOnly: true
          - mountPath: /etc/alertmanager/configmaps/alertmanager-config
            name: configmap-alertmanager-config
            readOnly: true
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9093/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          - --watched-dir=/etc/alertmanager/secrets/alertmanager-notification-secret
          - --watched-dir=/etc/alertmanager/configmaps/alertmanager-config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
            name: secret-alertmanager-notification-secret
            readOnly: true
          - mountPath: /etc/alertmanager/configmaps/alertmanager-config
            name: configmap-alertmanager-config
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8080
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          - --watched-dir=/etc/alertmanager/secrets/alertmanager-notification-secret
          - --watched-dir=/etc/alertmanager/configmaps/alertmanager-config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/secrets/alertmanager-notification-secret
            name: secret-alertmanager-notification-secret
            readOnly: true
          - mountPath: /etc/alertmanager/configmaps/alertmanager-config
            name: configmap-alertmanager-config
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-alertmanager
        serviceAccountName: kube-prometheus-stack-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-kube-prometheus-stack-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-kube-prometheus-stack-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: secret-alertmanager-notification-secret
          secret:
            defaultMode: 420
            secretName: alertmanager-notification-secret
        - configMap:
            defaultMode: 420
            name: alertmanager-config
          name: configmap-alertmanager-config
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-kube-prometheus-stack-alertmanager-web-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: alertmanager-kube-prometheus-stack-alertmanager-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
        storageClassName: rook-ceph-block
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-kube-prometheus-stack-alertmanager-959d94b5b
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-kube-prometheus-stack-alertmanager-959d94b5b
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "10364796910751627918"
    creationTimestamp: "2025-03-10T19:52:05Z"
    generation: 9
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 55.5.0
      chart: kube-prometheus-stack-55.5.0
      helm.toolkit.fluxcd.io/name: kube-prometheus-stack
      helm.toolkit.fluxcd.io/namespace: monitoring
      heritage: Helm
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: kube-prometheus-stack-prometheus
      operator.prometheus.io/shard: "0"
      release: kube-prometheus-stack
    name: prometheus-kube-prometheus-stack-prometheus
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: kube-prometheus-stack-prometheus
      uid: b20ea06a-5b57-4c14-bf85-7c9d771bb2bf
    resourceVersion: "11117744"
    uid: e4ab85fa-5439-4bf7-818e-8ff5f02ba104
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prometheus-stack-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: kube-prometheus-stack-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: kube-prometheus-stack-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
          kubectl.kubernetes.io/restartedAt: "2025-04-05T16:21:53+02:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prometheus-stack-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 2.48.1
          operator.prometheus.io/name: kube-prometheus-stack-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: kube-prometheus-stack-prometheus
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/master
                  operator: NotIn
                  values:
                  - "true"
        automountServiceAccountToken: true
        containers:
        - args:
          - --web.console.templates=/etc/prometheus/consoles
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.external-url=http://prometheus.local/
          - --web.route-prefix=/
          - --storage.tsdb.retention.time=15d
          - --storage.tsdb.retention.size=15GB
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v2.48.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 400m
              memory: 768Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-kube-prometheus-stack-prometheus-db
            subPath: prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8080
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0
            name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prometheus-stack-prometheus
        serviceAccountName: kube-prometheus-stack-prometheus
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prometheus-stack-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-kube-prometheus-stack-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
          name: prometheus-kube-prometheus-stack-prometheus-rulefiles-0
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prometheus-stack-prometheus-web-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: prometheus-kube-prometheus-stack-prometheus-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
        storageClassName: rook-ceph-block
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentRevision: prometheus-kube-prometheus-stack-prometheus-656bf48fbf
    observedGeneration: 9
    replicas: 1
    updateRevision: prometheus-kube-prometheus-stack-prometheus-78c697b68c
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    creationTimestamp: "2025-02-15T12:57:34Z"
    generation: 2
    labels:
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
      name: mosquitto
    name: mosquitto
    namespace: mosquitto
    resourceVersion: "11099879"
    uid: 6170679b-cfcd-496c-8db5-9439f94955e1
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: mosquitto
    serviceName: mosquitto
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mosquitto
      spec:
        containers:
        - image: eclipse-mosquitto:latest
          imagePullPolicy: Always
          name: mosquitto-broker
          ports:
          - containerPort: 1883
            protocol: TCP
          resources:
            limits:
              cpu: 300m
              memory: 1Gi
            requests:
              cpu: 50m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mosquitto/config
            name: mosquitto-config
          - mountPath: /mosquitto/data
            name: mosquitto-data
          - mountPath: /mosquitto/log
            name: mosquitto-log
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: mosquitto-config
          name: mosquitto-config
        - name: mosquitto-data
          persistentVolumeClaim:
            claimName: mosquitto-data-claim
        - name: mosquitto-log
          persistentVolumeClaim:
            claimName: mosquitto-log-claim
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: mosquitto-5dbc9f7958
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updateRevision: mosquitto-5dbc9f7958
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    creationTimestamp: "2025-02-14T22:42:52Z"
    generation: 15
    labels:
      app: openhab-production
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: openhab-production
    namespace: openhab
    resourceVersion: "11099909"
    uid: 342c564c-1508-4982-8767-fccfeb62e1be
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: openhab-production
    serviceName: openhab-production
    template:
      metadata:
        annotations:
          kubernetes.io/change-cause: Update to OpenHAB 4.3.4 using official image
        creationTimestamp: null
        labels:
          app: openhab-production
      spec:
        containers:
        - env:
          - name: EXTRA_JAVA_OPTS
            value: -Xmx1500m -Duser.timezone=Europe/Stockholm
          image: openhab/openhab:4.3.4
          imagePullPolicy: Always
          name: openhab434
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 8101
            name: console
            protocol: TCP
          resources:
            limits:
              cpu: 2500m
              memory: 2000Mi
            requests:
              cpu: 800m
              memory: 1500Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/localtime
            name: etc-localtime
            readOnly: true
          - mountPath: /etc/timezone
            name: etc-timezone
            readOnly: true
          - mountPath: /openhab/conf
            name: openhab-conf
          - mountPath: /openhab/userdata
            name: openhab-userdata
          - mountPath: /openhab/addons
            name: openhab-addons
          - mountPath: /openhab/.karaf
            name: openhab-karaf
        - args:
          - -config.file=/etc/promtail/promtail.yaml
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: grafana/promtail:2.9.2
          imagePullPolicy: IfNotPresent
          name: promtail
          ports:
          - containerPort: 9080
            name: http-metrics
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/promtail
            name: promtail-config
          - mountPath: /logs
            name: openhab-userdata
            readOnly: true
            subPath: logs
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 300
        volumes:
        - hostPath:
            path: /usr/share/zoneinfo/Europe/Stockholm
            type: ""
          name: etc-localtime
        - hostPath:
            path: /etc/timezone
            type: ""
          name: etc-timezone
        - name: openhab-conf
          persistentVolumeClaim:
            claimName: openhab-production-conf-claim
        - name: openhab-userdata
          persistentVolumeClaim:
            claimName: openhab-production-userdata-claim
        - name: openhab-addons
          persistentVolumeClaim:
            claimName: openhab-production-addons-claim
        - name: openhab-karaf
          persistentVolumeClaim:
            claimName: openhab-production-karaf-claim
        - configMap:
            defaultMode: 420
            name: promtail-sidecar-config
          name: promtail-config
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: openhab-production-557cbffd6f
    observedGeneration: 15
    readyReplicas: 1
    replicas: 1
    updateRevision: openhab-production-557cbffd6f
    updatedReplicas: 1
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    creationTimestamp: "2025-02-16T19:51:27Z"
    generation: 4
    labels:
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: bitwarden-backup
    namespace: bitwarden
    resourceVersion: "11018654"
    uid: f41d1d65-833c-4efb-b990-b5bf237e5445
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        template:
          metadata:
            creationTimestamp: null
          spec:
            containers:
            - args:
              - |
                # Install required packages
                apk add --no-cache curl openssh-client sshpass && \
                # Install kubectl
                curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
                chmod +x ./kubectl && \
                mv ./kubectl /usr/local/bin/kubectl && \
                /backup-script.sh
              command:
              - /bin/sh
              - -c
              env:
              - name: NAS_PASSWORD
                valueFrom:
                  secretKeyRef:
                    key: password
                    name: nas-credentials
              image: alpine:3.18
              imagePullPolicy: IfNotPresent
              name: backup
              resources: {}
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /backup-script.sh
                name: backup-script
                subPath: backup-script.sh
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: bitwarden-backup
            serviceAccountName: bitwarden-backup
            terminationGracePeriodSeconds: 30
            volumes:
            - configMap:
                defaultMode: 511
                name: bitwarden-backup-script
              name: backup-script
    schedule: 0 3 * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2025-04-05T01:00:00Z"
    lastSuccessfulTime: "2025-04-05T01:00:08Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    creationTimestamp: "2025-02-12T21:38:20Z"
    generation: 34
    labels:
      kustomize.toolkit.fluxcd.io/name: apps
      kustomize.toolkit.fluxcd.io/namespace: flux-system
    name: openhab-backup
    namespace: openhab
    resourceVersion: "11011736"
    uid: 70a0817b-6b95-4ab7-a513-0167e8034ae3
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        template:
          metadata:
            creationTimestamp: null
          spec:
            containers:
            - args:
              - |
                # Install required packages
                apk add --no-cache curl openssh-client sshpass && \
                # Install kubectl
                curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
                chmod +x ./kubectl && \
                mv ./kubectl /usr/local/bin/kubectl && \
                /backup-script.sh
              command:
              - /bin/sh
              - -c
              env:
              - name: NAS_PASSWORD
                valueFrom:
                  secretKeyRef:
                    key: password
                    name: nas-credentials
              image: alpine:3.18
              imagePullPolicy: IfNotPresent
              name: backup
              resources: {}
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /backup-script.sh
                name: backup-script
                subPath: backup-script.sh
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: openhab-backup
            serviceAccountName: openhab-backup
            terminationGracePeriodSeconds: 30
            volumes:
            - configMap:
                defaultMode: 511
                name: openhab-backup-script-mtkth67t5h
              name: backup-script
    schedule: 0 2 * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2025-04-05T00:00:00Z"
    lastSuccessfulTime: "2025-04-05T00:03:10Z"
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-04-03T01:00:00Z"
    creationTimestamp: "2025-04-03T01:00:00Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 44873d7a-6514-4522-9008-5eceeaad1b57
      batch.kubernetes.io/job-name: bitwarden-backup-29060700
      controller-uid: 44873d7a-6514-4522-9008-5eceeaad1b57
      job-name: bitwarden-backup-29060700
    name: bitwarden-backup-29060700
    namespace: bitwarden
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: bitwarden-backup
      uid: f41d1d65-833c-4efb-b990-b5bf237e5445
    resourceVersion: "10669453"
    uid: 44873d7a-6514-4522-9008-5eceeaad1b57
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 44873d7a-6514-4522-9008-5eceeaad1b57
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 44873d7a-6514-4522-9008-5eceeaad1b57
          batch.kubernetes.io/job-name: bitwarden-backup-29060700
          controller-uid: 44873d7a-6514-4522-9008-5eceeaad1b57
          job-name: bitwarden-backup-29060700
      spec:
        containers:
        - args:
          - |
            # Install required packages
            apk add --no-cache curl openssh-client sshpass && \
            # Install kubectl
            curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
            chmod +x ./kubectl && \
            mv ./kubectl /usr/local/bin/kubectl && \
            /backup-script.sh
          command:
          - /bin/sh
          - -c
          env:
          - name: NAS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: nas-credentials
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          name: backup
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /backup-script.sh
            name: backup-script
            subPath: backup-script.sh
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: bitwarden-backup
        serviceAccountName: bitwarden-backup
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 511
            name: bitwarden-backup-script
          name: backup-script
  status:
    completionTime: "2025-04-03T01:00:12Z"
    conditions:
    - lastProbeTime: "2025-04-03T01:00:12Z"
      lastTransitionTime: "2025-04-03T01:00:12Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-04-03T01:00:12Z"
      lastTransitionTime: "2025-04-03T01:00:12Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-04-03T01:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-04-04T01:00:00Z"
    creationTimestamp: "2025-04-04T01:00:00Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
      batch.kubernetes.io/job-name: bitwarden-backup-29062140
      controller-uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
      job-name: bitwarden-backup-29062140
    name: bitwarden-backup-29062140
    namespace: bitwarden
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: bitwarden-backup
      uid: f41d1d65-833c-4efb-b990-b5bf237e5445
    resourceVersion: "10843921"
    uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
          batch.kubernetes.io/job-name: bitwarden-backup-29062140
          controller-uid: 3b804360-b7af-4cf1-957e-045a7a8a6c04
          job-name: bitwarden-backup-29062140
      spec:
        containers:
        - args:
          - |
            # Install required packages
            apk add --no-cache curl openssh-client sshpass && \
            # Install kubectl
            curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
            chmod +x ./kubectl && \
            mv ./kubectl /usr/local/bin/kubectl && \
            /backup-script.sh
          command:
          - /bin/sh
          - -c
          env:
          - name: NAS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: nas-credentials
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          name: backup
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /backup-script.sh
            name: backup-script
            subPath: backup-script.sh
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: bitwarden-backup
        serviceAccountName: bitwarden-backup
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 511
            name: bitwarden-backup-script
          name: backup-script
  status:
    completionTime: "2025-04-04T01:00:08Z"
    conditions:
    - lastProbeTime: "2025-04-04T01:00:08Z"
      lastTransitionTime: "2025-04-04T01:00:08Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-04-04T01:00:08Z"
      lastTransitionTime: "2025-04-04T01:00:08Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-04-04T01:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-04-05T01:00:00Z"
    creationTimestamp: "2025-04-05T01:00:00Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
      batch.kubernetes.io/job-name: bitwarden-backup-29063580
      controller-uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
      job-name: bitwarden-backup-29063580
    name: bitwarden-backup-29063580
    namespace: bitwarden
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: bitwarden-backup
      uid: f41d1d65-833c-4efb-b990-b5bf237e5445
    resourceVersion: "11018650"
    uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
          batch.kubernetes.io/job-name: bitwarden-backup-29063580
          controller-uid: 5825d457-5ade-49e7-8d2e-f4d7ae8362c0
          job-name: bitwarden-backup-29063580
      spec:
        containers:
        - args:
          - |
            # Install required packages
            apk add --no-cache curl openssh-client sshpass && \
            # Install kubectl
            curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
            chmod +x ./kubectl && \
            mv ./kubectl /usr/local/bin/kubectl && \
            /backup-script.sh
          command:
          - /bin/sh
          - -c
          env:
          - name: NAS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: nas-credentials
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          name: backup
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /backup-script.sh
            name: backup-script
            subPath: backup-script.sh
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: bitwarden-backup
        serviceAccountName: bitwarden-backup
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 511
            name: bitwarden-backup-script
          name: backup-script
  status:
    completionTime: "2025-04-05T01:00:08Z"
    conditions:
    - lastProbeTime: "2025-04-05T01:00:08Z"
      lastTransitionTime: "2025-04-05T01:00:08Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-04-05T01:00:08Z"
      lastTransitionTime: "2025-04-05T01:00:08Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-04-05T01:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xWUY/auhL+K1eW+nSTkBCyy0bqQwrhwi0LEdDqHB2tkGMm4INjR7ZDi1b89yM7aTdbttutdN5sM/NlZr5vZnhEuKKfQSoqOIpRjjU59E4BctCR8h2K0f9FjhxUgsY7rDGKHxHmXGisqeDKXEX+NxCtQHuSCo9grRl4VPSo8T4AK10iuJaCMZAuOWCpXQl7qrS0GMj5KYL4wkG6+9OxBer8dAqc/3ykfPd+CqwcGdBf4nBcAoqRlhgKenyTuaowMT7HOgdXnZWGEl0cRCTY0De0BKVxWaGY14w5iOEcmC2KCdfmqrwfIrevb4rjgNUBxYj4xd0t4FvcjyDcAbkN7qJbcovD/iDA4bAIISowRIWJrM3RVp1ypTFj7tOHfpaSg2zKKyhAAiegUPzXD7q4Kj9yUM4EOS6N5xgYWC7jAjMFDnpi/PtTK6cuX1eM1FYz0TAq+gNSuPmQFO4gwL6bR4HvDgc30TDHBc7DPro8XBykKiCm2jkmR1EUc1pSjeLA930HaSgrhjWY319R7ytECV7Q/bQhYT1N+tHN+5vJOBhGk2Tk30UfRsHdzSichHd+MAkHfjT2A384DAPfHw/DMLq5C4fhYDQeR8E4+ddlc+mkb6qNKQfZEif35oBaASAHua4C7SotKd8jB+2ZyDHzGvbHUOCa6VXTkuf36MFBwE8WqSVokdynyEEnzOouXxfnu8XndLWeLRfdp1WaLbv3aTq/345Xs8/pqoOlgEjQXbvRNFlttuaT6ywZdb/7vAufO3TMDlpXKu713j1+/PQhXS3STbreJtns8q6nDPGkqaXqtXm4/VvP9/p+8N+6MseroF9IbpOs/pf+VpTJp810myXr9Xa0SsfpYjNL5uuOm+2SrsNssU5Hn1bpdv1xlm0387WJYzb58zWfbJ7MFtvpZpO9ZrVYbrPV8o8ukqdOxPEIq5UG6TFBMHMC3xv0Pd/ze8GNvYTtpYs1SWZzE2S2nM9GXUQJ3+R3eXAQLfHevmJODiB7R0arCqRrRB6ffO/OC928pmzX9/uDwPeHqPXJasYywSg5m5IUC6EzCQp4Z3wYDOQgCUrU0g6vR9MbQGpJ9XkkuIav2nY+Y+JLJumJMthDqghm+PnUwhXOKaOaWhS0k6IyjZTM58iMGwl4t+TsvBJCTyiDluVYyxouDjoJVpdwL2qum0YszTHD2gyQ3kGU8CzvntdG3ubR/c2W+FfuBJMDXPs3z28CsBPuBYTm/QpCl1VnapfVtcWPiFYN6gW75xvADDDD6eXBiIWLHayBAdFCGhpMR0kOGpTdzgrFiFFef0WWEqWx1N8lsuQTTFktTV1ekICseaIWghsGG96sGRFllUlRUGbXhT5XdoLVXNMS2gHZTFyQJ0ogIcRks+hs26cl1sigUQCUlT6PqWyW0I7WJYrRPZRCnjvr+or533N7Ivw3/Z5ofpvjN8qfk+t8G+LxY3tq69L812us3M7esguroPt7XBkf3rVulfBk/pJGLBMa69r2+uWfAAAA//8WHsdAwwoAAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-12T21:17:45Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik
      objectset.rio.cattle.io/hash: c0f97ea7a25e3dec71957c7a3241a38f3e5fae5f
    name: helm-install-traefik
    namespace: kube-system
    ownerReferences:
    - apiVersion: helm.cattle.io/v1
      blockOwnerDeletion: false
      controller: false
      kind: HelmChart
      name: traefik
      uid: 585f24cf-b8cf-41a0-b510-84658bafab32
    resourceVersion: "472216"
    uid: 5f404927-83da-4805-ad51-fe2b1765df9f
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 5f404927-83da-4805-ad51-fe2b1765df9f
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=6FD185FAC095BC196C3F3901F3405D010883100D8335693834CDD51DA359DB5C
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 5f404927-83da-4805-ad51-fe2b1765df9f
          batch.kubernetes.io/job-name: helm-install-traefik
          controller-uid: 5f404927-83da-4805-ad51-fe2b1765df9f
          helmcharts.helm.cattle.io/chart: traefik
          job-name: helm-install-traefik
      spec:
        containers:
        - args:
          - install
          - --set-string
          - global.systemDefaultRegistry=
          env:
          - name: NAME
            value: traefik
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-27.0.201+up27.0.2.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: INSECURE_SKIP_TLS_VERIFY
            value: "false"
          - name: PLAIN_HTTP
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.9.3-build20241008
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/klipper-helm/.helm
            name: klipper-helm
          - mountPath: /home/klipper-helm/.cache
            name: klipper-cache
          - mountPath: /home/klipper-helm/.config
            name: klipper-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: helm-traefik
        serviceAccountName: helm-traefik
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: klipper-helm
        - emptyDir:
            medium: Memory
          name: klipper-cache
        - emptyDir:
            medium: Memory
          name: klipper-config
        - emptyDir:
            medium: Memory
          name: tmp
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik
        - configMap:
            defaultMode: 420
            name: chart-content-traefik
          name: content
  status:
    completionTime: "2025-02-12T21:18:28Z"
    conditions:
    - lastProbeTime: "2025-02-12T21:18:28Z"
      lastTransitionTime: "2025-02-12T21:18:28Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-02-12T21:18:28Z"
      lastTransitionTime: "2025-02-12T21:18:28Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-02-12T21:17:45Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7RWUY/aOBD+KydLfbokJCEsm0h9yLLh4MpCBLS602mFjJmAD8eObIcWrfjvJ5u0Dbt0u324N+LM92Vmvs8zPCFc0U8gFRUcJWiNNdl1DgFy0J7yDUrQn2KNHFSCxhusMUqeEOZcaKyp4Mo8ivW/QLQC7UkqPIK1ZuBR0aEGvQNWukRwLQVjIF2yw1K7ErZUaWk5kPNDBvGZg3S3h31D1Hp1CJzfPlC+eT8CVg4M6U95OC4BJUhLDAXdu0Ru3gRRFSYGt6/X4Kqj0lCik4OIBJv+kpagNC4rlPCaMQcxvAZmG2NStvUq71n29vTNueyw2qEERbdF0d30SDcI+utuPyzIhuCeH/fjXhEXYd8PceH34tBk19Rqu0+50pgx9/JjPyrNQbb0ORQggRNQKPnnmUdeSIEctGaC7GcGeQ8MrK5JgZkCB31X/9tRY622dlfVqa2Hoph0MURr1yd+142g23Mx8SOXFMS/CTZ9EsYBOj2eHKQqIKbza0z2oigmtKQaJYHv+w7SUFYMazDvX3HzK6IJXtDt6CzGYpSGvZv3WffOH0RRGN8OB8EgiOJ0eDeMBrdxfDO8i8Mo7KdZFGTRTRTfxd1okEZxL46Du/7/YqFTqwWm65hykI2Acmt+oMYM6NFBwA/2VdP5afqQIQcdMKufC3FyvkV9yuaL8WzaPppn+az9PMomD6v7+fhTNm/xKSASdDtuMErny5X57CJPB+1vX161S0ArbKd1pZJO593Th4932XyaLbPFKs3Hp3cdZRQl5yapTqsWN+x7vhf6we919SLpK8Ut0/kf2S9lmX5cjlZ5ulisBvPsPpsux+lk0YLZK9AGjKeLbPBxnq0WH8b5ajlZmDzGw79fw+STdDxdjZbL/LWo6WyVz2d/tZk8dSCOR1itNEiPCYKZE/heFHq+53eCG/vQbR7aXMN0PDFJ5rPJeNBmlPDVU6dHB9ESb+0p5mQHsrNntKpAusa9ycH3Yq/rrmvKNqEfRoHv36IGk9eM5YJRcjQtKaZC5xIU8NZsMBzIQRKUqKWdTE/G8EBqSfVxILiGL9peacbE51zSA2WwhUwRzPDlSMIVXlNGNbUsaCNFZW5HOpkgM0ck4M2Ms+NcCD2kDBqVEy1rODnoIFhdwoOouT7frtL8zLE2k6GzEyVc1N3xmsybOtrvbIt/BieY7OAl/nz8JgI7uq4wnM9fUOiyao3ksnoZ8ZzRukFdibsc72YqGU1Pj8YsXGxgAQyIFtLIYG6U5KBB2RWsUIIY5fUXZCVRGkv9zSIzPsSU1dL05YoFZM1TNRXcKHjWzYYRUVa5FAVldg/oY2UnWM01LeEeClwzfR6jIA+UQEqIqWbaWqeXG+pshbMLoKz08Z7K84bZ0LpECXqAUshjaye/UP/XYN9F/0Xcd6nfBvwq+6XAztdBnjw1v5renP/YnaPcZ0vJbqOCbh9wZXC8jWgccQm55herisa6tvf+9F8AAAD//8B9fYe4CgAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik-crd
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-08T22:04:24Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik-crd
      objectset.rio.cattle.io/hash: 48ff3d5c3117b372fcdca509795f9f2702af0592
    name: helm-install-traefik-crd
    namespace: kube-system
    ownerReferences:
    - apiVersion: helm.cattle.io/v1
      blockOwnerDeletion: false
      controller: false
      kind: HelmChart
      name: traefik-crd
      uid: 49c3ae4b-0c03-4e35-ac04-cfc061d7c291
    resourceVersion: "630"
    uid: 1d05ec10-0395-4b33-9e6c-a8a5886f731a
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 1d05ec10-0395-4b33-9e6c-a8a5886f731a
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 1d05ec10-0395-4b33-9e6c-a8a5886f731a
          batch.kubernetes.io/job-name: helm-install-traefik-crd
          controller-uid: 1d05ec10-0395-4b33-9e6c-a8a5886f731a
          helmcharts.helm.cattle.io/chart: traefik-crd
          job-name: helm-install-traefik-crd
      spec:
        containers:
        - args:
          - install
          env:
          - name: NAME
            value: traefik-crd
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-27.0.201+up27.0.2.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: INSECURE_SKIP_TLS_VERIFY
            value: "false"
          - name: PLAIN_HTTP
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.9.3-build20241008
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/klipper-helm/.helm
            name: klipper-helm
          - mountPath: /home/klipper-helm/.cache
            name: klipper-cache
          - mountPath: /home/klipper-helm/.config
            name: klipper-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: helm-traefik-crd
        serviceAccountName: helm-traefik-crd
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: klipper-helm
        - emptyDir:
            medium: Memory
          name: klipper-cache
        - emptyDir:
            medium: Memory
          name: klipper-config
        - emptyDir:
            medium: Memory
          name: tmp
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik-crd
        - configMap:
            defaultMode: 420
            name: chart-content-traefik-crd
          name: content
  status:
    completionTime: "2025-02-08T22:04:42Z"
    conditions:
    - lastProbeTime: "2025-02-08T22:04:42Z"
      lastTransitionTime: "2025-02-08T22:04:42Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-02-08T22:04:42Z"
      lastTransitionTime: "2025-02-08T22:04:42Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-02-08T22:04:27Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-04-03T00:00:00Z"
    creationTimestamp: "2025-04-03T00:00:00Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 7cafa485-f819-4c78-9bda-400a566fea9d
      batch.kubernetes.io/job-name: openhab-backup-29060640
      controller-uid: 7cafa485-f819-4c78-9bda-400a566fea9d
      job-name: openhab-backup-29060640
    name: openhab-backup-29060640
    namespace: openhab
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: openhab-backup
      uid: 70a0817b-6b95-4ab7-a513-0167e8034ae3
    resourceVersion: "10662648"
    uid: 7cafa485-f819-4c78-9bda-400a566fea9d
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 7cafa485-f819-4c78-9bda-400a566fea9d
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 7cafa485-f819-4c78-9bda-400a566fea9d
          batch.kubernetes.io/job-name: openhab-backup-29060640
          controller-uid: 7cafa485-f819-4c78-9bda-400a566fea9d
          job-name: openhab-backup-29060640
      spec:
        containers:
        - args:
          - |
            # Install required packages
            apk add --no-cache curl openssh-client sshpass && \
            # Install kubectl
            curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
            chmod +x ./kubectl && \
            mv ./kubectl /usr/local/bin/kubectl && \
            /backup-script.sh
          command:
          - /bin/sh
          - -c
          env:
          - name: NAS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: nas-credentials
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          name: backup
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /backup-script.sh
            name: backup-script
            subPath: backup-script.sh
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openhab-backup
        serviceAccountName: openhab-backup
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 511
            name: openhab-backup-script-mtkth67t5h
          name: backup-script
  status:
    completionTime: "2025-04-03T00:04:05Z"
    conditions:
    - lastProbeTime: "2025-04-03T00:04:05Z"
      lastTransitionTime: "2025-04-03T00:04:05Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-04-03T00:04:05Z"
      lastTransitionTime: "2025-04-03T00:04:05Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-04-03T00:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-04-04T00:00:00Z"
    creationTimestamp: "2025-04-04T00:00:00Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
      batch.kubernetes.io/job-name: openhab-backup-29062080
      controller-uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
      job-name: openhab-backup-29062080
    name: openhab-backup-29062080
    namespace: openhab
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: openhab-backup
      uid: 70a0817b-6b95-4ab7-a513-0167e8034ae3
    resourceVersion: "10837097"
    uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
          batch.kubernetes.io/job-name: openhab-backup-29062080
          controller-uid: 5fbcf3dd-9096-4e35-9239-0c9e6b8ef939
          job-name: openhab-backup-29062080
      spec:
        containers:
        - args:
          - |
            # Install required packages
            apk add --no-cache curl openssh-client sshpass && \
            # Install kubectl
            curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
            chmod +x ./kubectl && \
            mv ./kubectl /usr/local/bin/kubectl && \
            /backup-script.sh
          command:
          - /bin/sh
          - -c
          env:
          - name: NAS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: nas-credentials
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          name: backup
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /backup-script.sh
            name: backup-script
            subPath: backup-script.sh
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openhab-backup
        serviceAccountName: openhab-backup
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 511
            name: openhab-backup-script-mtkth67t5h
          name: backup-script
  status:
    completionTime: "2025-04-04T00:03:51Z"
    conditions:
    - lastProbeTime: "2025-04-04T00:03:51Z"
      lastTransitionTime: "2025-04-04T00:03:51Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-04-04T00:03:51Z"
      lastTransitionTime: "2025-04-04T00:03:51Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-04-04T00:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-04-05T00:00:00Z"
    creationTimestamp: "2025-04-05T00:00:00Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
      batch.kubernetes.io/job-name: openhab-backup-29063520
      controller-uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
      job-name: openhab-backup-29063520
    name: openhab-backup-29063520
    namespace: openhab
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: openhab-backup
      uid: 70a0817b-6b95-4ab7-a513-0167e8034ae3
    resourceVersion: "11011732"
    uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
          batch.kubernetes.io/job-name: openhab-backup-29063520
          controller-uid: d4d1d31d-c3ad-4de7-b6a3-ab0e083b89e0
          job-name: openhab-backup-29063520
      spec:
        containers:
        - args:
          - |
            # Install required packages
            apk add --no-cache curl openssh-client sshpass && \
            # Install kubectl
            curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/arm64/kubectl && \
            chmod +x ./kubectl && \
            mv ./kubectl /usr/local/bin/kubectl && \
            /backup-script.sh
          command:
          - /bin/sh
          - -c
          env:
          - name: NAS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: nas-credentials
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          name: backup
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /backup-script.sh
            name: backup-script
            subPath: backup-script.sh
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openhab-backup
        serviceAccountName: openhab-backup
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 511
            name: openhab-backup-script-mtkth67t5h
          name: backup-script
  status:
    completionTime: "2025-04-05T00:03:10Z"
    conditions:
    - lastProbeTime: "2025-04-05T00:03:10Z"
      lastTransitionTime: "2025-04-05T00:03:10Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-04-05T00:03:10Z"
      lastTransitionTime: "2025-04-05T00:03:10Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-04-05T00:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2025-03-30T08:48:08Z"
    generation: 1
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 18.2.2-0
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-k3s-m-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "10026718"
    uid: 7c35dbe4-c62c-4edb-bcdb-fd4c996e58d2
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 7c35dbe4-c62c-4edb-bcdb-fd4c996e58d2
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          batch.kubernetes.io/controller-uid: 7c35dbe4-c62c-4edb-bcdb-fd4c996e58d2
          batch.kubernetes.io/job-name: rook-ceph-osd-prepare-k3s-m-1
          ceph.rook.io/pvc: ""
          controller-uid: 7c35dbe4-c62c-4edb-bcdb-fd4c996e58d2
          job-name: rook-ceph-osd-prepare-k3s-m-1
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - ceph
          - osd
          - provision
          command:
          - /rook/rook
          env:
          - name: ROOK_NODE_NAME
            value: k3s-m-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: ROOK_OSD_STORE_TYPE
            value: bluestore
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-m-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: all
          - name: ROOK_CEPH_VERSION
            value: ceph version 18.2.2-0 reef
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: provision
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --archive
          - --force
          - --verbose
          - /usr/local/bin/rook
          - /rook
          command:
          - cp
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: k3s-m-1
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2025-03-30T08:48:30Z"
    conditions:
    - lastProbeTime: "2025-03-30T08:48:30Z"
      lastTransitionTime: "2025-03-30T08:48:30Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-03-30T08:48:30Z"
      lastTransitionTime: "2025-03-30T08:48:30Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-03-30T08:48:08Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2025-03-30T08:48:11Z"
    generation: 1
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 18.2.2-0
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-k3s-w-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "10026769"
    uid: 0fde204e-d36e-4b00-ad56-da70c7c7a2e0
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 0fde204e-d36e-4b00-ad56-da70c7c7a2e0
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          batch.kubernetes.io/controller-uid: 0fde204e-d36e-4b00-ad56-da70c7c7a2e0
          batch.kubernetes.io/job-name: rook-ceph-osd-prepare-k3s-w-1
          ceph.rook.io/pvc: ""
          controller-uid: 0fde204e-d36e-4b00-ad56-da70c7c7a2e0
          job-name: rook-ceph-osd-prepare-k3s-w-1
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - ceph
          - osd
          - provision
          command:
          - /rook/rook
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-1
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: ROOK_OSD_STORE_TYPE
            value: bluestore
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-1
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: all
          - name: ROOK_CEPH_VERSION
            value: ceph version 18.2.2-0 reef
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: provision
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --archive
          - --force
          - --verbose
          - /usr/local/bin/rook
          - /rook
          command:
          - cp
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: k3s-w-1
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2025-03-30T08:48:48Z"
    conditions:
    - lastProbeTime: "2025-03-30T08:48:48Z"
      lastTransitionTime: "2025-03-30T08:48:48Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-03-30T08:48:49Z"
      lastTransitionTime: "2025-03-30T08:48:49Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-03-30T08:48:11Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2025-03-30T08:46:09Z"
    generation: 1
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 18.2.2-0
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-k3s-w-2
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "10026677"
    uid: c4c19d1e-46a1-46bf-a3e4-bfd573e5a7f1
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: c4c19d1e-46a1-46bf-a3e4-bfd573e5a7f1
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          batch.kubernetes.io/controller-uid: c4c19d1e-46a1-46bf-a3e4-bfd573e5a7f1
          batch.kubernetes.io/job-name: rook-ceph-osd-prepare-k3s-w-2
          ceph.rook.io/pvc: ""
          controller-uid: c4c19d1e-46a1-46bf-a3e4-bfd573e5a7f1
          job-name: rook-ceph-osd-prepare-k3s-w-2
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - ceph
          - osd
          - provision
          command:
          - /rook/rook
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-2
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: ROOK_OSD_STORE_TYPE
            value: bluestore
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-2
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: all
          - name: ROOK_CEPH_VERSION
            value: ceph version 18.2.2-0 reef
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: provision
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --archive
          - --force
          - --verbose
          - /usr/local/bin/rook
          - /rook
          command:
          - cp
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: k3s-w-2
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2025-03-30T08:48:20Z"
    conditions:
    - lastProbeTime: "2025-03-30T08:48:20Z"
      lastTransitionTime: "2025-03-30T08:48:20Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-03-30T08:48:20Z"
      lastTransitionTime: "2025-03-30T08:48:20Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-03-30T08:46:09Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2025-03-30T08:48:14Z"
    generation: 1
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 18.2.2-0
      rook-version: v1.14.8
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-k3s-w-3
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 6c4ccbf7-40a9-4f25-bce3-da820591b460
    resourceVersion: "10026700"
    uid: 94908c75-685c-4f74-ad29-e0e9f24ec701
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 94908c75-685c-4f74-ad29-e0e9f24ec701
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          batch.kubernetes.io/controller-uid: 94908c75-685c-4f74-ad29-e0e9f24ec701
          batch.kubernetes.io/job-name: rook-ceph-osd-prepare-k3s-w-3
          ceph.rook.io/pvc: ""
          controller-uid: 94908c75-685c-4f74-ad29-e0e9f24ec701
          job-name: rook-ceph-osd-prepare-k3s-w-3
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - ceph
          - osd
          - provision
          command:
          - /rook/rook
          env:
          - name: ROOK_NODE_NAME
            value: k3s-w-3
          - name: ROOK_CLUSTER_ID
            value: 6c4ccbf7-40a9-4f25-bce3-da820591b460
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: ROOK_OSD_STORE_TYPE
            value: bluestore
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: k3s-w-3
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: all
          - name: ROOK_CEPH_VERSION
            value: ceph version 18.2.2-0 reef
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          envFrom:
          - configMapRef:
              name: rook-ceph-osd-env-override
              optional: true
          image: quay.io/ceph/ceph:v18.2.2
          imagePullPolicy: IfNotPresent
          name: provision
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /run/ceph
            name: ceph-daemons-sock-dir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /var/lib/rook-ceph-mon
            name: ceph-admin-secret
            readOnly: true
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --archive
          - --force
          - --verbose
          - /usr/local/bin/rook
          - /rook
          command:
          - cp
          image: rook/ceph:v1.14.8
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: k3s-w-3
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/exporter
            type: DirectoryOrCreate
          name: ceph-daemons-sock-dir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - name: ceph-admin-secret
          secret:
            defaultMode: 420
            items:
            - key: ceph-secret
              path: secret.keyring
            secretName: rook-ceph-mon
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2025-03-30T08:48:27Z"
    conditions:
    - lastProbeTime: "2025-03-30T08:48:27Z"
      lastTransitionTime: "2025-03-30T08:48:27Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-03-30T08:48:27Z"
      lastTransitionTime: "2025-03-30T08:48:27Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-03-30T08:48:14Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
