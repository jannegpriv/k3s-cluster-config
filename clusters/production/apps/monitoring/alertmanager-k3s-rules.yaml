apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alertmanager-k3s-rules
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  groups:
  - name: k3s.rules
    rules:
    # Silence K3s component alerts (they're expected to be "down" in K3s architecture)
    - alert: KubeControllerManagerDown
      expr: absent(up{job="kube-controller-manager"} == 1)
      for: 15m
      labels:
        severity: none
        inhibited_by_k3s: "true"
      annotations:
        summary: "Kube Controller Manager metrics not available (normal in K3s)"
        description: "K3s runs control plane components embedded, so they don't expose metrics on standard ports. This is normal for K3s."
    
    - alert: KubeSchedulerDown
      expr: absent(up{job="kube-scheduler"} == 1)
      for: 15m
      labels:
        severity: none
        inhibited_by_k3s: "true"
      annotations:
        summary: "Kube Scheduler metrics not available (normal in K3s)"
        description: "K3s runs control plane components embedded, so they don't expose metrics on standard ports. This is normal for K3s."
    
    - alert: KubeProxyDown
      expr: absent(up{job="kube-proxy"} == 1)
      for: 15m
      labels:
        severity: none
        inhibited_by_k3s: "true"
      annotations:
        summary: "Kube Proxy metrics not available (normal in K3s)"
        description: "K3s runs control plane components embedded, so they don't expose metrics on standard ports. This is normal for K3s."
    
    # Create an inhibitor alert for k3s components
    - alert: K3sComponentsInhibitor
      expr: vector(1)
      for: 5m
      labels:
        severity: none
        inhibit_k3s_components: "true"
      annotations:
        summary: "Inhibitor for k3s embedded components"
        description: "This alert exists to inhibit alerts for components that are embedded in k3s"
    
    # Tune CPU throttling alerts to be more specific and increase threshold
    - alert: NodeCPUThrottlingHigh
      expr: sum by(container, pod, namespace) (increase(container_cpu_cfs_throttled_periods_total[5m])) / sum by(container, pod, namespace) (increase(container_cpu_cfs_periods_total[5m])) > 0.50
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: "Container CPU throttling high ({{ $value | humanizePercentage }})"
        description: "{{ $labels.container }} in pod {{ $labels.pod }} / {{ $labels.namespace }} is being throttled. This indicates resource constraints."
    
    # Silence NodeCPUThrottling for monitoring namespace
    - alert: NodeCPUThrottling
      expr: sum by(container, pod, namespace) (increase(container_cpu_cfs_throttled_periods_total[5m])) > 10 and namespace="monitoring"
      for: 15m
      labels:
        severity: info
      annotations:
        summary: "CPU throttling detected in monitoring namespace"
        description: "Container {{ $labels.container }} in pod {{ $labels.pod }} ({{ $labels.namespace }}) is being throttled. This is expected in resource-constrained environments."
    
    # Adjust API server latency threshold
    - alert: ApiServerCriticalLatency
      expr: histogram_quantile(0.99, sum by(le, verb) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|WATCH"}[5m]))) > 30
      for: 15m
      labels:
        severity: critical
      annotations:
        summary: "API server critical latency"
        description: "99th percentile of request latency is above 30 seconds for {{ $labels.verb }} requests. Consider restarting k3s.service."
    
    # Add K3s specific health check
    - alert: K3sServiceUnhealthy
      expr: node_systemd_unit_state{name="k3s.service", state="active"} != 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "K3s service not active on node {{ $labels.instance }}"
        description: "The k3s.service is not in active state on node {{ $labels.instance }}. This indicates a problem with the K3s service."
