apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.2
  continueUpgradeAfterChecksEvenIfNotHealthy: true
  crashCollector:
    disable: true
  dashboard:
    enabled: true
    ssl: false
  dataDirHostPath: /var/lib/rook
  # Configuration settings
  cephConfig:
    global:
      auth_allow_insecure_global_id_reclaim: "true"
      ms_client_mode: "crc secure"
      mon_allow_pool_delete: "true"
      mon_max_pg_per_osd: "300"
      # Note: The following settings are managed directly via Ceph tools
      # mon_osd_full_ratio: "0.95"
      # mon_osd_backfillfull_ratio: "0.90"
      # mon_osd_nearfull_ratio: "0.85"
    mon:
      debug_mon: "1/5"
      debug_paxos: "1/5"
      debug_auth: "1/5"
    mgr:
      debug_mgr: "1/5"
      mgr/prometheus/rbd_stats_pools: "replicapool"
    osd:
      debug_osd: "1/5"
      osd_max_backfills: "1"
      osd_recovery_max_active: "1"
      osd_recovery_op_priority: "1"
  
  mgr:
    modules:
    - enabled: true
      name: prometheus
  
  mon:
    allowMultiplePerNode: true
    count: 3  
    volumeClaimTemplate:
      spec:
        storageClassName: local-path  # Changed from rook-ceph-block to local-path
        resources:
          requests:
            storage: 25Gi
  
  skipUpgradeChecks: true
  storage:
    useAllDevices: true
    useAllNodes: true
    # Note: Manual CRUSH weight adjustments have been made:
    # - OSD.2 CRUSH weight and REWEIGHT both reduced to 0.25 (2025-04-13) to address storage imbalance and nearfull warnings
    nodes:
    - name: k3s-w-5
      devices:
      - name: /dev/nvme0n1p3
  
  placement:
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - k3s-m-1
    mon:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - rook-ceph-mon
            topologyKey: kubernetes.io/hostname
  
  resources:
    mgr:
      limits:
        memory: "1Gi"    # Kept memory limit, removed CPU limit
      requests:
        cpu: "500m"      # Kept CPU request
        memory: "512Mi"
    mon:
      limits:
        memory: "768Mi"  # Kept memory limit, removed CPU limit
      requests:
        cpu: "300m"      # Kept CPU request
        memory: "512Mi"
    osd:
      limits:
        memory: "4Gi"    # Kept memory limit, removed CPU limit
      requests:
        cpu: "1"         # Kept CPU request
        memory: "2Gi"
  
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        probe:
          initialDelaySeconds: 10
          timeoutSeconds: 30
          periodSeconds: 60
          successThreshold: 1
          failureThreshold: 5
      mgr:
        probe:
          initialDelaySeconds: 10
          timeoutSeconds: 45
          periodSeconds: 90
          successThreshold: 1
          failureThreshold: 6
      osd:
        probe:
          initialDelaySeconds: 10
          timeoutSeconds: 30
          periodSeconds: 60
          successThreshold: 1
          failureThreshold: 5